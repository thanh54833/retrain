{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Q&A with Qwen 7B\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n load v√† s·ª≠ d·ª•ng Qwen 7B model ƒë·ªÉ th·ª±c hi·ªán Q&A ƒë∆°n gi·∫£n.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **Task**: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Python version: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
            "‚ö† torch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m  \u001b[33m0:00:11\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22/22\u001b[0m [torch]m21/22\u001b[0m [torch]-cusolver-cu12]2]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.1 triton-3.5.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† transformers>=4.35.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting transformers>=4.35.0\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.35.0)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers>=4.35.0)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (6.0.3)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.35.0)\n",
            "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers>=4.35.0)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.35.0)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers>=4.35.0)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (1.2.0)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers>=4.35.0)\n",
            "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers>=4.35.0) (3.11)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers>=4.35.0)\n",
            "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers>=4.35.0) (2025.11.12)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Installing collected packages: urllib3, safetensors, regex, numpy, charset_normalizer, requests, huggingface-hub, tokenizers, transformers\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K    Found existing installation: huggingface_hub 1.2.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K    Uninstalling huggingface_hub-1.2.2:m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9/9\u001b[0m [transformers][0m [transformers]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed charset_normalizer-3.4.4 huggingface-hub-0.36.0 numpy-2.2.6 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3 urllib3-2.6.2\n",
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† accelerate>=0.24.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate>=0.24.0\n",
            "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (25.0)\n",
            "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (2.9.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.12.0)\n",
            "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.24.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.24.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.11.12)\n",
            "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-1.12.0\n",
            "‚úì accelerate>=0.24.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† bitsandbytes>=0.41.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes>=0.41.0\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.9 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (25.0)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes>=0.41.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes>=0.41.0) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.0\n",
            "‚úì bitsandbytes>=0.41.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì BitsAndBytesConfig s·∫µn s√†ng cho quantization\n",
            "\n",
            "==================================================\n",
            "‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (ch·∫°y cell n√†y tr∆∞·ªõc n·∫øu ch∆∞a c√†i ƒë·∫∑t)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def check_and_install(package_name):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "    \n",
        "    # ƒê·∫∑c bi·ªát x·ª≠ l√Ω torch - c√≥ th·ªÉ c√≥ l·ªói compatibility v·ªõi Python 3.12\n",
        "    if package_import == \"torch\":\n",
        "        try:\n",
        "            import torch\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "            return True\n",
        "        except (ImportError, ValueError) as e:\n",
        "            if isinstance(e, ValueError) and (\"METH_CLASS\" in str(e) or \"METH_STATIC\" in str(e)):\n",
        "                print(f\"‚ö† {package_name} c√≥ v·∫•n ƒë·ªÅ compatibility v·ªõi Python {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "                print(\"   ‚ö†Ô∏è L·ªñI: PyTorch kh√¥ng t∆∞∆°ng th√≠ch!\")\n",
        "                print(\"   üí° Gi·∫£i ph√°p: Ch·∫°y cell ti·∫øp theo (cell fix PyTorch) ƒë·ªÉ c√†i ƒë·∫∑t l·∫°i PyTorch\")\n",
        "                print(\"   Ho·∫∑c ch·∫°y l·ªánh sau trong terminal:\")\n",
        "                print(\"   !pip uninstall -y torch torchvision torchaudio\")\n",
        "                print(\"   !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
        "                raise ValueError(\"PyTorch kh√¥ng t∆∞∆°ng th√≠ch. Vui l√≤ng ch·∫°y cell fix PyTorch tr∆∞·ªõc.\")\n",
        "            else:\n",
        "                print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "                # C√†i ƒë·∫∑t PyTorch t∆∞∆°ng th√≠ch v·ªõi Python 3.12\n",
        "                if sys.version_info >= (3, 12):\n",
        "                    subprocess.check_call([\n",
        "                        sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                        \"--upgrade\", \"torch\", \"torchvision\", \"torchaudio\", \n",
        "                        \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
        "                    ])\n",
        "                else:\n",
        "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name])\n",
        "                print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "                return False\n",
        "    elif package_import == \"bitsandbytes\":\n",
        "        # ƒê·∫∑c bi·ªát x·ª≠ l√Ω bitsandbytes - c·∫ßn upgrade ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch\n",
        "        try:\n",
        "            import bitsandbytes as bnb\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {bnb.__version__})\")\n",
        "            # Ki·ªÉm tra xem c√≥ th·ªÉ import ƒë∆∞·ª£c kh√¥ng\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            print(\"   ‚úì BitsAndBytesConfig c√≥ th·ªÉ s·ª≠ d·ª•ng\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name])\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return False\n",
        "    else:\n",
        "        # X·ª≠ l√Ω c√°c package kh√°c\n",
        "        try:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",  # C·∫ßn cho quantization\n",
        "]\n",
        "\n",
        "try:\n",
        "    for package in packages:\n",
        "        check_and_install(package)\n",
        "    \n",
        "    # Verify bitsandbytes after installation\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        print(\"\\n‚úì BitsAndBytesConfig s·∫µn s√†ng cho quantization\")\n",
        "    except ImportError:\n",
        "        print(\"\\n‚ö† BitsAndBytesConfig ch∆∞a s·∫µn s√†ng - c√≥ th·ªÉ c·∫ßn restart kernel\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "    print(\"=\"*50)\n",
        "except ValueError as e:\n",
        "    if \"PyTorch\" in str(e):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"‚ùå PyTorch kh√¥ng t∆∞∆°ng th√≠ch!\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"\\nVui l√≤ng:\")\n",
        "        print(\"1. Ch·∫°y cell ti·∫øp theo (cell 'Fix PyTorch compatibility')\")\n",
        "        print(\"2. Sau ƒë√≥ restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "    else:\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gi·∫£i th√≠ch: T·∫°i sao c·∫ßn import torch v√† transformers?\n",
        "\n",
        "### 1. **`torch` (PyTorch)** - Framework Deep Learning\n",
        "- **M·ª•c ƒë√≠ch**: PyTorch l√† th∆∞ vi·ªán c·ªët l√µi ƒë·ªÉ l√†m vi·ªác v·ªõi m√¥ h√¨nh deep learning\n",
        "- **Ch·ª©c nƒÉng trong code n√†y**:\n",
        "  - Ki·ªÉm tra GPU/CUDA: `torch.cuda.is_available()` - xem c√≥ GPU kh√¥ng\n",
        "  - Qu·∫£n l√Ω b·ªô nh·ªõ GPU: `torch.cuda.empty_cache()` - gi·∫£i ph√≥ng b·ªô nh·ªõ GPU\n",
        "  - X·ª≠ l√Ω tensor: T·∫•t c·∫£ t√≠nh to√°n c·ªßa model ƒë·ªÅu d√πng tensor c·ªßa PyTorch\n",
        "  - ƒê·ªãnh d·∫°ng d·ªØ li·ªáu: `torch.float16` - d√πng ƒë·ªÉ gi·∫£m b·ªô nh·ªõ\n",
        "\n",
        "### 2. **`transformers`** - Th∆∞ vi·ªán Hugging Face\n",
        "- **`AutoModelForCausalLM`**: Load m√¥ h√¨nh ng√¥n ng·ªØ (nh∆∞ Qwen 7B)\n",
        "  - T·ª± ƒë·ªông nh·∫≠n di·ªán ki·ªÉu model v√† load ƒë√∫ng c√°ch\n",
        "  - H·ªó tr·ª£ quantization, device mapping\n",
        "  \n",
        "- **`AutoTokenizer`**: X·ª≠ l√Ω text (tokenize/encode/decode)\n",
        "  - Chuy·ªÉn text th√†nh s·ªë (tokens) ƒë·ªÉ model hi·ªÉu\n",
        "  - Chuy·ªÉn tokens th√†nh text khi model tr·∫£ l·ªùi\n",
        "  \n",
        "- **`BitsAndBytesConfig`**: C·∫•u h√¨nh quantization (4-bit, 8-bit)\n",
        "  - Gi·∫£m b·ªô nh·ªõ t·ª´ ~14GB xu·ªëng ~4-5GB\n",
        "  - Cho ph√©p ch·∫°y model l·ªõn tr√™n GPU nh·ªè h∆°n\n",
        "\n",
        "### 3. **T·∫°i sao c·∫ßn ki·ªÉm tra GPU?**\n",
        "- Model Qwen 7B r·∫•t l·ªõn (~14GB), c·∫ßn GPU ƒë·ªÉ ch·∫°y nhanh\n",
        "- N·∫øu kh√¥ng c√≥ GPU, model s·∫Ω ch·∫°y tr√™n CPU (r·∫•t ch·∫≠m)\n",
        "- Code n√†y ki·ªÉm tra GPU ƒë·ªÉ:\n",
        "  - Quy·∫øt ƒë·ªãnh c√≥ d√πng quantization kh√¥ng\n",
        "  - Qu·∫£n l√Ω b·ªô nh·ªõ GPU hi·ªáu qu·∫£\n",
        "  - Tr√°nh l·ªói \"Out of Memory\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 1: Import th∆∞ vi·ªán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 3060\n",
            "CUDA memory: 12.63 GB\n",
            "CUDA memory free: 2.15 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA memory free: {torch.cuda.get_device_properties(0).total_memory / 1e9 - torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"S·ª≠ d·ª•ng CPU\")\n",
        "\n",
        "# H√†m ƒë·ªÉ clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"X√≥a cache GPU ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "        print(\"‚úì ƒê√£ x√≥a GPU cache\")\n",
        "\n",
        "# clear_gpu_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
            "Loading tokenizer...\n",
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a6d498c4e162427ab375a933a2868802",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error loading model: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 11.76 GiB of which 570.00 MiB is free. Process 1212929 has 11.18 GiB memory in use. Of the allocated memory 9.76 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 11.76 GiB of which 570.00 MiB is free. Process 1212929 has 11.18 GiB memory in use. Of the allocated memory 9.76 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 61\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# No CUDA available\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     device_map_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43muse_quantization\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     67\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# Move to device if not using device_map\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m device_map_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:5048\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5039\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5041\u001b[0m     (\n\u001b[1;32m   5042\u001b[0m         model,\n\u001b[1;32m   5043\u001b[0m         missing_keys,\n\u001b[1;32m   5044\u001b[0m         unexpected_keys,\n\u001b[1;32m   5045\u001b[0m         mismatched_keys,\n\u001b[1;32m   5046\u001b[0m         offload_index,\n\u001b[1;32m   5047\u001b[0m         error_msgs,\n\u001b[0;32m-> 5048\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5065\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:5468\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5465\u001b[0m         args_list \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mtqdm(args_list, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading checkpoint shards\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[0;32m-> 5468\u001b[0m         _error_msgs, disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5469\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m _error_msgs\n\u001b[1;32m   5471\u001b[0m \u001b[38;5;66;03m# Save offloaded index if needed\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:843\u001b[0m, in \u001b[0;36mload_shard_file\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[1;32m    842\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[0;32m--> 843\u001b[0m     disk_offload_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_utils.py:748\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, hf_quantizer, keep_in_fp32_regex, device_mesh)\u001b[0m\n\u001b[1;32m    740\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(\n\u001b[1;32m    741\u001b[0m             model,\n\u001b[1;32m    742\u001b[0m             param,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    745\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msharding_kwargs,\n\u001b[1;32m    746\u001b[0m         )\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 748\u001b[0m     param \u001b[38;5;241m=\u001b[39m \u001b[43mparam\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    749\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m         param \u001b[38;5;241m=\u001b[39m param\u001b[38;5;241m.\u001b[39mto(casting_dtype)\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 11.76 GiB of which 570.00 MiB is free. Process 1212929 has 11.18 GiB memory in use. Of the allocated memory 9.76 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
        "# S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ gi·∫£m memory usage t·ª´ ~14GB xu·ªëng ~4-5GB\n",
        "use_quantization = True  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM (>16GB)\n",
        "\n",
        "# Ki·ªÉm tra bitsandbytes tr∆∞·ªõc khi s·ª≠ d·ª•ng quantization\n",
        "bitsandbytes_available = False\n",
        "if use_quantization and torch.cuda.is_available():\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        import bitsandbytes as bnb\n",
        "        bitsandbytes_available = True\n",
        "        print(\"‚ö†Ô∏è S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    except (ImportError, ModuleNotFoundError) as e:\n",
        "        print(\"‚ö†Ô∏è bitsandbytes kh√¥ng kh·∫£ d·ª•ng - t·∫Øt quantization\")\n",
        "        print(f\"   L·ªói: {e}\")\n",
        "        print(\"   üí° Ch·∫°y cell 'Fix bitsandbytes installation' ƒë·ªÉ c√†i ƒë·∫∑t l·∫°i\")\n",
        "        print(\"   Ho·∫∑c ƒë·∫∑t use_quantization = False ƒë·ªÉ ti·∫øp t·ª•c kh√¥ng d√πng quantization\")\n",
        "        use_quantization = False\n",
        "        quantization_config = None\n",
        "        bitsandbytes_available = False\n",
        "else:\n",
        "    quantization_config = None\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng quantization (kh√¥ng c√≥ CUDA)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng quantization (c·∫ßn nhi·ªÅu VRAM)\")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Loading model...\")\n",
        "try:\n",
        "    # Configure device_map based on quantization\n",
        "    if use_quantization and torch.cuda.is_available():\n",
        "        # For quantization, use a specific GPU device to avoid CPU offloading\n",
        "        # This ensures all quantized modules stay on GPU\n",
        "        device_map_config = 0  # Use first GPU\n",
        "    elif torch.cuda.is_available():\n",
        "        # For non-quantized models, auto device_map is fine\n",
        "        device_map_config = \"auto\"\n",
        "    else:\n",
        "        # No CUDA available\n",
        "        device_map_config = None\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if (torch.cuda.is_available() and not use_quantization) else None,\n",
        "        device_map=device_map_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Move to device if not using device_map\n",
        "    if not torch.cuda.is_available() and device_map_config is None:\n",
        "        model = model.to(\"cpu\")\n",
        "    \n",
        "    print(f\"‚úì Model loaded: {model_name}\")\n",
        "    \n",
        "    # Device detection\n",
        "    if hasattr(model, 'hf_device_map') and model.hf_device_map:\n",
        "        devices = set(model.hf_device_map.values())\n",
        "        # Convert device IDs to strings (they can be int or str)\n",
        "        device_strs = [str(d) for d in devices]\n",
        "        print(f\"‚úì Model devices: {', '.join(sorted(device_strs))}\")\n",
        "    else:\n",
        "        print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
        "    \n",
        "    # Ki·ªÉm tra memory usage\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "        print(f\"‚úì GPU memory allocated: {allocated:.2f} GB\")\n",
        "        print(f\"‚úì GPU memory reserved: {reserved:.2f} GB\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    if \"bitsandbytes\" in str(e).lower():\n",
        "        print(\"‚ùå Error: bitsandbytes ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c c√≥ v·∫•n ƒë·ªÅ\")\n",
        "        print(\"\\nüí° Gi·∫£i ph√°p:\")\n",
        "        print(\"1. Ch·∫°y cell 'Fix bitsandbytes installation' (cell tr∆∞·ªõc ƒë√≥)\")\n",
        "        print(\"2. Restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "        print(\"\\nHo·∫∑c t·∫Øt quantization b·∫±ng c√°ch:\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "except ValueError as e:\n",
        "    error_msg = str(e)\n",
        "    if \"CPU or the disk\" in error_msg or \"dispatched on the CPU\" in error_msg:\n",
        "        print(f\"‚ùå Error: {error_msg}\")\n",
        "        print(\"\\nüí° V·∫•n ƒë·ªÅ: Model kh√¥ng ƒë·ªß GPU RAM ƒë·ªÉ load v·ªõi quantization.\")\n",
        "        print(\"\\nüîß Gi·∫£i ph√°p:\")\n",
        "        print(\"1. T·∫Øt quantization (khuy·∫øn ngh·ªã):\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell (c·∫ßn ~14GB VRAM)\")\n",
        "        print(\"\\n2. Ho·∫∑c s·ª≠ d·ª•ng model nh·ªè h∆°n:\")\n",
        "        print(\"   - Thay ƒë·ªïi model_name th√†nh 'Qwen/Qwen2.5-3B-Instruct'\")\n",
        "        print(\"   - Ho·∫∑c 'Qwen/Qwen2.5-1.5B-Instruct'\")\n",
        "        print(\"\\n3. Ho·∫∑c gi·∫£i ph√≥ng GPU memory:\")\n",
        "        print(\"   - Restart kernel\")\n",
        "        print(\"   - ƒê√≥ng c√°c ·ª©ng d·ª•ng kh√°c ƒëang d√πng GPU\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    if use_quantization and \"bitsandbytes\" in str(e).lower():\n",
        "        print(\"\\nüí° Gi·∫£i ph√°p:\")\n",
        "        print(\"1. Ch·∫°y cell 'Fix bitsandbytes installation'\")\n",
        "        print(\"2. Restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "        print(\"\\nHo·∫∑c t·∫Øt quantization:\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell\")\n",
        "    raise\n",
        "\n",
        "# Ki·ªÉm tra xem tokenizer v√† model ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KI·ªÇM TRA LOAD MODEL\")\n",
        "print(\"=\"*60)\n",
        "try:\n",
        "    if 'tokenizer' not in globals() or tokenizer is None:\n",
        "        raise NameError(\"tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    if 'model' not in globals() or model is None:\n",
        "        raise NameError(\"model ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    print(\"‚úì Tokenizer v√† Model ƒë√£ s·∫µn s√†ng!\")\n",
        "    print(f\"  - Tokenizer: {type(tokenizer).__name__}\")\n",
        "    print(f\"  - Model: {type(model).__name__}\")\n",
        "    print(\"=\"*60)\n",
        "except NameError as e:\n",
        "    print(\"‚ùå L·ªói:\", str(e))\n",
        "    print(\"\\nüí° Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh load model ·ªü tr√™n!\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 3: H√†m Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì H√†m ask_question ƒë√£ s·∫µn s√†ng!\n"
          ]
        }
      ],
      "source": [
        "def ask_question(question: str, max_new_tokens: int = 512, temperature: float = 0.7):\n",
        "    \"\"\"\n",
        "    H√†m ƒë·ªÉ h·ªèi ƒë√°p v·ªõi model (t·ªëi ∆∞u memory)\n",
        "    \n",
        "    Args:\n",
        "        question: C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi\n",
        "        max_new_tokens: S·ªë token t·ªëi ƒëa ƒë·ªÉ generate (gi·∫£m n·∫øu h·∫øt memory)\n",
        "        temperature: Nhi·ªát ƒë·ªô cho sampling (0.0-1.0)\n",
        "    \"\"\"\n",
        "    # Clear cache tr∆∞·ªõc khi generate\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Format prompt cho Qwen Instruct model\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate v·ªõi c√°c t√πy ch·ªçn t·ªëi ∆∞u memory\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                # T·ªëi ∆∞u memory\n",
        "                use_cache=True,  # S·ª≠ d·ª•ng KV cache ƒë·ªÉ tƒÉng t·ªëc\n",
        "            )\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"‚ö†Ô∏è Out of Memory! ƒêang th·ª≠ v·ªõi max_new_tokens nh·ªè h∆°n...\")\n",
        "        # Clear cache v√† th·ª≠ l·∫°i v·ªõi max_new_tokens nh·ªè h∆°n\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Th·ª≠ l·∫°i v·ªõi max_new_tokens gi·∫£m ƒëi m·ªôt n·ª≠a\n",
        "        new_max_tokens = max_new_tokens // 2\n",
        "        if new_max_tokens < 50:\n",
        "            raise RuntimeError(\"Kh√¥ng ƒë·ªß memory ngay c·∫£ v·ªõi max_new_tokens nh·ªè nh·∫•t. H√£y s·ª≠ d·ª•ng quantization ho·∫∑c gi·∫£m k√≠ch th∆∞·ªõc model.\")\n",
        "        \n",
        "        print(f\"Th·ª≠ l·∫°i v·ªõi max_new_tokens={new_max_tokens}\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=new_max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Clear cache sau khi generate\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"‚úì H√†m ask_question ƒë√£ s·∫µn s√†ng!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 4: Test Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TESTING Q&A\n",
            "============================================================\n",
            "\n",
            "[1] C√¢u h·ªèi: Xin ch√†o! B·∫°n l√† ai?\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] C√¢u h·ªèi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTr·∫£ l·ªùi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
            "Cell \u001b[0;32mIn[4], line 20\u001b[0m, in \u001b[0;36mask_question\u001b[0;34m(question, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     15\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: question}\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Apply chat template\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m     21\u001b[0m     messages,\n\u001b[1;32m     22\u001b[0m     tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m     add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Tokenize\u001b[39;00m\n\u001b[1;32m     27\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([text], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Test v·ªõi m·ªôt s·ªë c√¢u h·ªèi ƒë∆°n gi·∫£n\n",
        "test_questions = [\n",
        "    \"Xin ch√†o! B·∫°n l√† ai?\",\n",
        "    \"Python l√† g√¨?\",\n",
        "    \"Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ machine learning\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TESTING Q&A\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n[{i}] C√¢u h·ªèi: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    answer = ask_question(question)\n",
        "    print(f\"Tr·∫£ l·ªùi: {answer}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## S·ª≠ d·ª•ng t√πy ch·ªânh\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng h√†m `ask_question()` v·ªõi b·∫•t k·ª≥ c√¢u h·ªèi n√†o:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå L·ªói: tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
            "\n",
            "üí° Vui l√≤ng ch·∫°y Cell 6 (B∆∞·ªõc 2: Load Model v√† Tokenizer) tr∆∞·ªõc!\n",
            "   Cell 6 s·∫Ω load tokenizer v√† model t·ª´ Qwen/Qwen2.5-7B-Instruct\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m() \u001b[38;5;129;01mor\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNameError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a"
          ]
        }
      ],
      "source": [
        "# Ki·ªÉm tra xem tokenizer v√† model ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "try:\n",
        "    if 'tokenizer' not in globals() or tokenizer is None:\n",
        "        raise NameError(\"tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    if 'model' not in globals() or model is None:\n",
        "        raise NameError(\"model ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    print(\"‚úì Tokenizer v√† Model ƒë√£ s·∫µn s√†ng!\")\n",
        "    print(f\"  - Tokenizer: {type(tokenizer).__name__}\")\n",
        "    print(f\"  - Model: {type(model).__name__}\")\n",
        "except NameError as e:\n",
        "    print(\"‚ùå L·ªói:\", str(e))\n",
        "    print(\"\\nüí° Vui l√≤ng ch·∫°y Cell 6 (B∆∞·ªõc 2: Load Model v√† Tokenizer) tr∆∞·ªõc!\")\n",
        "    print(\"   Cell 6 s·∫Ω load tokenizer v√† model t·ª´ Qwen/Qwen2.5-7B-Instruct\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# Th·ª≠ v·ªõi c√¢u h·ªèi c·ªßa b·∫°n\n",
        "my_question = \"\"\"\n",
        "# H∆Ø·ªöNG D·∫™N H·ªÜ TH·ªêNG\n",
        "B·∫°n l√† chuy√™n gia t·ªëi ∆∞u t·ª´ kh√≥a t√¨m ki·∫øm s·∫£n ph·∫©m cho Concung(m·ªôt chu·ªói si√™u th·ªã m·∫π v√† b√© l·ªõn nh·∫•t t·∫°i Vi·ªát Nam). Nhi·ªám v·ª• c·ªßa b·∫°n l√† bi·∫øn ƒë·ªïi c√¢u h·ªèi c·ªßa kh√°ch h√†ng th√†nh t·ª´ kh√≥a t√¨m ki·∫øm hi·ªáu qu·∫£ nh·∫•t v√† t·∫°o message banner th√¢n thi·ªán.\n",
        "\n",
        "# NHI·ªÜM V·ª§ CH√çNH\n",
        "1.  **T·ªëi ∆∞u h√≥a T·ª´ kh√≥a t√¨m ki·∫øm (`keyword`):** Chuy·ªÉn ƒë·ªïi `USER_QUERY` th√†nh t·ª´ kh√≥a t√¨m ki·∫øm ng·∫Øn g·ªçn, ch√≠nh x√°c v√† hi·ªáu qu·∫£ nh·∫•t cho h·ªá th·ªëng c·ªßa Concung.\n",
        "2.  **T·∫°o Tin nh·∫Øn Banner (`message_banner`):** So·∫°n m·ªôt tin nh·∫Øn banner th√¢n thi·ªán ƒë·ªÉ x√°c nh·∫≠n ƒë√£ hi·ªÉu y√™u c·∫ßu c·ªßa kh√°ch h√†ng.\n",
        "3.  **T·∫°o Tin nh·∫Øn D·ª± ph√≤ng (`message_no_result`):** So·∫°n m·ªôt tin nh·∫Øn d·ª± ph√≤ng ƒë∆∞·ª£c c√° nh√¢n h√≥a trong tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m, gi√∫p gi·ªØ ch√¢n kh√°ch h√†ng v√† g·ª£i √Ω c·∫£i thi·ªán t√¨m ki·∫øm.\n",
        "\n",
        "# QUY T·∫ÆC PH·∫†M VI X·ª¨ L√ù (QUAN TR·ªåNG)\n",
        "## is_in_scope = true khi QUERY th·ªèa m√£n T·∫§T C·∫¢ ƒëi·ªÅu ki·ªán sau:\n",
        "1.  **Li√™n quan ƒë·∫øn h·ªá sinh th√°i Con C∆∞ng**: QUERY ƒë·ªÅ c·∫≠p ƒë·∫øn m·ªôt trong c√°c ch·ªß ƒë·ªÅ sau:\n",
        "    *   **T√¨m ki·∫øm s·∫£n ph·∫©m**: T√¨m ki·∫øm, mua s·∫Øm, ho·∫∑c so s√°nh s·∫£n ph·∫©m (v√≠ d·ª•: \"s·ªØa cho b√©\", \"t√£ Huggies\", \"kem ch·ªëng hƒÉm\").\n",
        "    *   **Th√¥ng tin & H∆∞·ªõng d·∫´n**: C√°c b√†i vi·∫øt, c·∫©m nang, ho·∫∑c m·∫πo li√™n quan ƒë·∫øn m·∫π, b√©, v√† gia ƒë√¨nh (v√≠ d·ª•: \"c√°ch chƒÉm s√≥c tr·∫ª s∆° sinh\", \"th·ª±c ƒë∆°n ƒÉn d·∫∑m\", \"b√≠ quy·∫øt gi√∫p m·∫π b·∫ßu ƒë√≥n T·∫øt an to√†n\", \"b·ªánh vi·ªán ph·ª• s·∫£n uy t√≠n\").\n",
        "    *   **Th√¥ng tin v·ªÅ Con C∆∞ng**: C√°c ch√≠nh s√°ch, ch∆∞∆°ng tr√¨nh khuy·∫øn m√£i, ho·∫∑c th√¥ng tin v·ªÅ c√¥ng ty (v√≠ d·ª•: \"ch√≠nh s√°ch ƒë·ªïi tr·∫£\", \"khuy·∫øn m√£i th√°ng 10\", \"giao h√†ng\", \"giao nh·∫≠n\").\n",
        "2.  **H·ª£p ph√°p**: N·ªôi dung kh√¥ng vi ph·∫°m ph√°p lu·∫≠t Vi·ªát Nam.\n",
        "3.  **Kh√¥ng ƒë·ªôc h·∫°i**: Kh√¥ng ch·ª©a n·ªôi dung ph·∫£n c·∫£m, th√¥ t·ª•c, c√≥ h·∫°i.\n",
        "\n",
        "## is_in_scope = false khi QUERY thu·ªôc m·ªôt trong c√°c tr∆∞·ªùng h·ª£p sau:\n",
        "- **Kh√¥ng li√™n quan ƒë·∫øn h·ªá sinh th√°i Con C∆∞ng**: C√°c c√¢u h·ªèi ki·∫øn th·ª©c t·ªïng qu√°t kh√¥ng li√™n quan ƒë·∫øn m·∫π, b√© v√† gia ƒë√¨nh (v√≠ d·ª•: \"th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨\", \"k·∫øt qu·∫£ x·ªï s·ªë\"), tin t·ª©c kh√¥ng li√™n quan.\n",
        "- **N·ªôi dung ƒë·ªôc h·∫°i**: Th√¥ t·ª•c, ph·∫£n c·∫£m, k√≠ch ƒë·ªông b·∫°o l·ª±c, ph√¢n bi·ªát ch·ªßng t·ªôc.\n",
        "- **Ch√≠nh tr·ªã**: Quan ƒëi·ªÉm ch√≠nh tr·ªã, ·ª©ng vi√™n, ch√≠nh s√°ch, b·∫ßu c·ª≠.\n",
        "- **B·∫•t h·ª£p ph√°p**: Vi ph·∫°m ph√°p lu·∫≠t, ma t√∫y, v≈© kh√≠, h√†ng c·∫•m.\n",
        "- **So s√°nh ƒë·ªëi th·ªß**: So s√°nh tr·ª±c ti·∫øp Con C∆∞ng v·ªõi c√°c n·ªÅn t·∫£ng kh√°c (v√≠ d·ª•: \"gi√° ·ªü Shopee r·∫ª h∆°n ph·∫£i kh√¥ng\").\n",
        "- **T·∫•n c√¥ng danh ti·∫øng**: Ph·ªâ b√°ng c√° nh√¢n, doanh nghi·ªáp.\n",
        "\n",
        "# QUY T·∫ÆC T·ªêI ∆ØU T·ª™ KH√ìA (CH·ªà KHI is_in_scope = true)\n",
        "## 1. Ph√¢n T√≠ch √ù ƒê·ªãnh\n",
        "- X√°c ƒë·ªãnh nhu c·∫ßu s·∫£n ph·∫©m c·ªët l√µi t·ª´ c√¢u h·ªèi kh√°ch h√†ng\n",
        "- ∆Øu ti√™n c√°c nh√≥m s·∫£n ph·∫©m ch·ªß l·ª±c: m·∫π v√† b√©, th·ªùi trang, ƒëi·ªán t·ª≠, gia d·ª•ng, th·ª±c ph·∫©m\n",
        "\n",
        "## 2. X√¢y D·ª±ng T·ª´ Kh√≥a\n",
        "- **X·ª≠ L√Ω Truy V·∫•n V·ªÅ Th∆∞∆°ng Hi·ªáu Con C∆∞ng**: N·∫øu `USER_QUERY` ch·ªâ l√† \"Con C∆∞ng\" (ho·∫∑c c√°c bi·∫øn th·ªÉ vi·∫øt kh√¥ng d·∫•u/sai ch√≠nh t·∫£ nh∆∞ \"concung\", \"con cung\"), `keyword` ph·∫£i l√† \"Gi·ªõi thi·ªáu Con C∆∞ng\".\n",
        "- S·ª≠ d·ª•ng t√™n s·∫£n ph·∫©m c·ª• th·ªÉ, kh√¥ng d√πng m√¥ t·∫£ chung chung\n",
        "- Bao g·ªìm thu·ªôc t√≠nh li√™n quan: ƒë·ªô tu·ªïi, th∆∞∆°ng hi·ªáu, lo·∫°i, ch·ª©c nƒÉng, k√≠ch th∆∞·ªõc\n",
        "- ∆Øu ti√™n t√™n s·∫£n ph·∫©m h∆°n m√¥ t·∫£ v·∫•n ƒë·ªÅ\n",
        "- T·ªëi ƒëa 20 t·ª´\n",
        "- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ ti·∫øng Vi·ªát ph√π h·ª£p v·ªõi th·ªã tr∆∞·ªùng Vi·ªát Nam\n",
        "  - **S·ª¨ D·ª§NG CONCUNG_SUGGESTIONS (CH·ªà 2 TR∆Ø·ªú·ªúNG H·ª¢P)**:\n",
        "  - **QUY T·∫ÆC NGHI√äM NG·∫∂T (√ÅP D·ª§NG CHO C·∫¢ 2 TR∆Ø·ªú·ªúNG H·ª¢P)**:\n",
        "      - **QUAN TR·ªåNG**: N·∫æU `USER_QUERY` C√ì T·ª™ 2 T·ª™ TR·ªû L√äN V√Ä ƒê√É R√ï NGHƒ®A, B·ªé QUA `CONCUNG_SUGGESTIONS` (tr·ª´ tr∆∞·ªùng h·ª£p 2: s·ª≠a l·ªói ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu).\n",
        "      - KH√îNG copy tr·ª±c ti·∫øp c√°c c·ª•m t·ª´ t·ª´ CONCUNG_SUGGESTIONS v√†o keyword\n",
        "      - CH·ªà d√πng CONCUNG_SUGGESTIONS ƒë·ªÉ hi·ªÉu ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m, sau ƒë√≥ t·∫°o keyword\n",
        "      - GI·ªÆ NGUY√äN t·ª´ kh√≥a g·ªëc t·ª´ QUERY (bao g·ªìm t√™n th∆∞∆°ng hi·ªáu n·∫øu c√≥)\n",
        "      - B·ªî SUNG th√¥ng tin lo·∫°i s·∫£n ph·∫©m chung chung v√†o keyword (v√≠ d·ª•: \"s·ªØa\", \"t√£\", \"qu·∫ßn √°o\"), kh√¥ng ph·∫£i c√°c bi·∫øn th·ªÉ chi ti·∫øt.\n",
        "      - KH√îNG s·ª≠ d·ª•ng CONCUNG_SUGGESTIONS cho message_banner\n",
        "  - **Tr∆∞·ªùng h·ª£p 1 ‚Äî Keyword ng·∫Øn/kh√¥ng r√µ**: Khi QUERY ch·ªâ c√≥ 1 t·ª´ v√† ho√†n to√†n m∆° h·ªì, kh√¥ng ƒëo√°n ƒë∆∞·ª£c lo·∫°i s·∫£n ph·∫©m.\n",
        "    - **ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG**: CH·ªà khi QUERY = 1 t·ª´ V√Ä kh√¥ng th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m\n",
        "    - **M·ª§C ƒê√çCH DUY NH·∫§T**: D√πng CONCUNG_SUGGESTIONS ƒë·ªÉ x√°c ƒë·ªãnh ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m trong c·ª≠a h√†ng Concung\n",
        "    - **V√≠ d·ª•**:\n",
        "      - QUERY \"nan\" + CONCUNG_SUGGESTIONS \"s·ªØa nan, nan 2, nan 3\" ‚Üí Ph√¢n t√≠ch: ng√†nh h√†ng = s·ªØa ‚Üí Keyword \"s·ªØa nan cho b√©\"\n",
        "      - QUERY \"milo\" + CONCUNG_SUGGESTIONS \"milo 3in1, milo √∫c\" ‚Üí Ph√¢n t√≠ch: ng√†nh h√†ng = s·ªØa ‚Üí Keyword \"s·ªØa milo\"\n",
        "      - QUERY \"animo\" ‚Üí Ph√¢n t√≠ch: animo l√† th∆∞∆°ng hi·ªáu ƒë·ªôc quy·ªÅn c·ªßa Concung ‚Üí Keyword \"s·∫£n ph·∫©m c·ªßa Animo\"\n",
        "  - **Tr∆∞·ªùng h·ª£p 2 ‚Äî S·ª≠a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu (ƒë·ªô tin c·∫≠y cao)**: Khi QUERY c√≥ d·∫•u hi·ªáu sai ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu V√Ä t·∫•t c·∫£ m·ª•c trong CONCUNG_SUGGESTIONS c√πng m·ªôt th∆∞∆°ng hi·ªáu (v√≠ d·ª• ƒë·ªÅu l√† Ensure).\n",
        "    - **M·ª§C ƒê√çCH DUY NH·∫§T**: Chu·∫©n h√≥a t√™n th∆∞∆°ng hi·ªáu trong keyword theo th∆∞∆°ng hi·ªáu chung t·ª´ CONCUNG_SUGGESTIONS.\n",
        "    - **V√≠ d·ª•**:\n",
        "      - QUERY \"en sua\" + CONCUNG_SUGGESTIONS \"s·ªØa Ensure, s·ªØa Ensure Gold, S·ªØa Ensure 237ml\" ‚Üí Keyword \"s·ªØa Ensure dinh d∆∞·ª°ng cho ng∆∞·ªùi l·ªõn\"\n",
        "  - **L∆ØU √ù**: N·∫øu QUERY ‚â• 2 t·ª´ ho·∫∑c ƒë√£ r√µ r√†ng v·ªÅ s·∫£n ph·∫©m ‚Üí B·ªé QUA CONCUNG_SUGGESTIONS, ngo·∫°i tr·ª´ Tr∆∞·ªùng h·ª£p 2 (s·ª≠a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu ‚Äì ƒë·ªô tin c·∫≠y cao)\n",
        "\n",
        "## 3. X·ª≠ L√Ω ƒê·∫∑c Bi·ªát Cho S·∫£n Ph·∫©m M·∫π v√† B√©\n",
        "- Chuy·ªÉn ƒë·ªïi v·∫•n ƒë·ªÅ s·ª©c kh·ªèe th√†nh nhu c·∫ßu s·∫£n ph·∫©m c·ª• th·ªÉ\n",
        "- X√°c ƒë·ªãnh ƒë·ªô tu·ªïi/c√¢n n·∫∑ng ƒë·ªÉ g·ª£i √Ω size ph√π h·ª£p\n",
        "- T·∫≠p trung v√†o gi·∫£i ph√°p s·∫£n ph·∫©m thay v√¨ m√¥ t·∫£ tri·ªáu ch·ª©ng\n",
        "\n",
        "# QUY T·∫ÆC T·∫†O MESSAGE BANNER (CH·ªà KHI is_in_scope = true)\n",
        "## C·∫•u tr√∫c message banner\n",
        "1. **Th√¥ng ƒëi·ªáp quan t√¢m**: C√¢u n√≥i th·∫•u hi·ªÉu, h·ªó tr·ª£ b·∫±ng ti·∫øng Vi·ªát (t·ªëi ƒëa 100 k√Ω t·ª±)\n",
        "2. **G·ª£i √Ω s·∫£n ph·∫©m**: K·∫øt n·ªëi t√¨m ki·∫øm v·ªõi nhu c·∫ßu/c·∫£m x√∫c c·ªßa kh√°ch h√†ng. Vi·ªác c√≥ nh·∫Øc t√™n th∆∞∆°ng hi·ªáu hay kh√¥ng tu√¢n theo c√°c quy t·∫Øc ∆∞u ti√™n d∆∞·ªõi ƒë√¢y.\n",
        "\n",
        "   - **QUY T·∫ÆC ∆ØU TI√äN (KI·ªÇM TRA ƒê·∫¶U TI√äN)**:\n",
        "       - **M·ª•c ƒë√≠ch**: X√°c nh·∫≠n ch√≠nh x√°c th∆∞∆°ng hi·ªáu khi h·ªá th·ªëng c√≥ ƒë·ªô tin c·∫≠y cao.\n",
        "       - **ƒêI·ªÄU KI·ªÜN**: Ph·∫£i th·ªèa m√£n **C·∫¢ HAI** ƒëi·ªÅu ki·ªán sau: 1) `USER_QUERY` c√≥ ch·ª©a t√™n th∆∞∆°ng hi·ªáu (ho·∫∑c bi·∫øn th·ªÉ sai ch√≠nh t·∫£) V√Ä 2) T·∫•t c·∫£ `CONCUNG_SUGGESTIONS` c√πng tr·ªè v·ªÅ m·ªôt th∆∞∆°ng hi·ªáu duy nh·∫•t ƒë·ªÉ x√°c nh·∫≠n.\n",
        "\n",
        "   - **QUY T·∫ÆC CHUNG (√ÅP D·ª§NG KHI QUY T·∫ÆC ∆ØU TI√äN KH√îNG TH·ªéA M√ÉN)**:\n",
        "       - **M·ª•c ƒë√≠ch**: M√¥ t·∫£ s·∫£n ph·∫©m chung khi kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th∆∞∆°ng hi·ªáu.\n",
        "\n",
        "   - **S·ª≠ d·ª•ng th·∫ª <b>**: D√πng ƒë·ªÉ nh·∫•n m·∫°nh t·ª´ kh√≥a c√≥ gi√° tr·ªã t√¨m ki·∫øm cao v√† li√™n quan tr·ª±c ti·∫øp ƒë·∫øn 'keyword' ƒë∆∞·ª£c t·∫°o ra.\n",
        "3. **H√†nh ƒë·ªông b·∫Øt bu·ªôc (S√ÅNG T·∫†O BANNER M·ªöI)**:\n",
        "   - **N·∫øu `QUY T·∫ÆC ∆ØU TI√äN` ƒë∆∞·ª£c th·ªèa m√£n**: `message_banner` **B·∫ÆT BU·ªòC** ph·∫£i nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a. D·ª±a v√†o `USER_QUERY`, h√£y **s√°ng t·∫°o m·ªôt th√¥ng ƒëi·ªáp t·ª± nhi√™n v√† th√¢n thi·ªán c√≥ l·ªìng gh√©p t√™n th∆∞∆°ng hi·ªáu** thay v√¨ d√πng m·∫´u c√≥ s·∫µn. T√™n th∆∞∆°ng hi·ªáu ph·∫£i ƒë∆∞·ª£c ƒë·∫∑t trong th·∫ª `<b>`.\n",
        "   - **N·∫øu `QUY T·∫ÆC ∆ØU TI√äN` kh√¥ng ƒë∆∞·ª£c th·ªèa m√£n**: √Åp d·ª•ng `QUY T·∫ÆC CHUNG`. **Ph√¢n t√≠ch s√¢u `USER_QUERY` ƒë·ªÉ n·∫Øm b·∫Øt √Ω ƒë·ªãnh v√† c·∫£m x√∫c c·ªßa kh√°ch h√†ng. T·ª´ ƒë√≥, h√£y t·ª± s√°ng t·∫°o m·ªôt `message_banner` ho√†n to√†n m·ªõi, ph√π h·ª£p v√† ƒë·ªôc ƒë√°o.** Banner c·∫ßn t√≠ch h·ª£p **m√¥ t·∫£ lo·∫°i s·∫£n ph·∫©m ho·∫∑c t√≠nh nƒÉng chung** ƒë∆∞·ª£c r√∫t ra t·ª´ `keyword` m·ªôt c√°ch t·ª± nhi√™n. TUY·ªÜT ƒê·ªêI KH√îNG nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu c·ª• th·ªÉ v√† kh√¥ng l·∫∑p l·∫°i c√°c m·∫´u c√¢u nh√†m ch√°n.\n",
        "4. **Emoji**: M·ªôt emoji ph√π h·ª£p (ch·ªß ƒë·ªÅ gia ƒë√¨nh/s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng)\n",
        "5. **Gi·ªçng ƒëi·ªáu**: Quan t√¢m, h·ªó tr·ª£, ƒë√°ng tin c·∫≠y, r·∫•t Vi·ªát Nam\n",
        "6. **C√°ch x∆∞ng h√¥**: S·ª≠ d·ª•ng \"Con C∆∞ng\" ƒë·ªÉ x∆∞ng v√† \"Ba m·∫π\" ƒë·ªÉ g·ªçi kh√°ch h√†ng\n",
        "\n",
        "## H∆∞·ªõng d·∫´n t·∫°o banner\n",
        "- S·ª≠ d·ª•ng ng√¥n ng·ªØ ti·∫øng Vi·ªát nh·∫π nh√†ng, quan t√¢m v·ªõi c√°ch x∆∞ng h√¥ \"Con C∆∞ng\" v√† \"Ba m·∫π\"\n",
        "- Th·ªÉ hi·ªán s·ª± hi·ªÉu bi·∫øt v·ªÅ nhu c·∫ßu kh√°ch h√†ng\n",
        "- X√¢y d·ª±ng ni·ªÅm tin v√† s·ª± t·ª± tin\n",
        "- Tr√°nh ƒë∆∞a ra l·ªùi khuy√™n y t·∫ø ho·∫∑c g√¢y √°p l·ª±c\n",
        "- K·∫øt n·ªëi v·ªõi t√¨nh c·∫£m v√† s·ª± chƒÉm s√≥c\n",
        "- Ph√π h·ª£p v·ªõi vƒÉn h√≥a Vi·ªát Nam\n",
        "- **B·∫ÆT BU·ªòC**: H·∫°n ch·∫ø t·ªëi ƒëa vi·ªác nh·∫Øc t√™n th∆∞∆°ng hi·ªáu c·ª• th·ªÉ, tr·ª´ tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá ƒë√£ n√™u tr√™n.\n",
        "\n",
        "# QUY T·∫ÆC T·∫†O MESSAGE KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M (message_no_result)\n",
        "\n",
        "## C·∫•u Tr√∫c B·∫Øt Bu·ªôc (2 Ph·∫ßn)\n",
        "1.  **B√†y T·ªè ƒê·ªìng C·∫£m**: Lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng m·ªôt c√¢u th·ªÉ hi·ªán s·ª± ti·∫øc nu·ªëi nh·∫π nh√†ng. H√£y ƒëa d·∫°ng c√°ch m·ªü ƒë·∫ßu.\n",
        "    - *V√≠ d·ª•*: \"√îi, Con C∆∞ng ch∆∞a t√¨m th·∫•y...\", \"Con C∆∞ng t√¨m k·ªπ m√† ch∆∞a ra...\", \"Hmm, Con C∆∞ng t√¨m ch∆∞a ra r·ªìi...\", \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m ƒë∆∞·ª£c s·∫£n ph·∫©m...\"\n",
        "2.  **H∆∞·ªõng D·∫´n Gi·∫£i Ph√°p Th√¥ng Minh**: Ph√¢n t√≠ch `USER_QUERY` ƒë·ªÉ ƒë∆∞a ra **ph∆∞∆°ng ph√°p** c·∫£i thi·ªán t√¨m ki·∫øm, **kh√¥ng g·ª£i √Ω t·ª´ kh√≥a c·ª• th·ªÉ**.\n",
        "    - **Query c√≥ th·ªÉ sai ch√≠nh t·∫£**: G·ª£i √Ω \"Ba m·∫π ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m nh√©!\".\n",
        "    - **Query qu√° d√†i/chi ti·∫øt**: G·ª£i √Ω \"Ba m·∫π th·ª≠ d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n xem sao ·∫°!\".\n",
        "    - **Query qu√° ng·∫Øn/chung chung**: G·ª£i √Ω \"Ba m·∫π th·ª≠ b·ªï sung th√™m th√¥ng tin chi ti·∫øt (v√≠ d·ª•: th∆∞∆°ng hi·ªáu, ƒë·ªô tu·ªïi) nh√©!\".\n",
        "    - **C√°c tr∆∞·ªùng h·ª£p kh√°c**: G·ª£i √Ω \"Ba m·∫π th·ª≠ t√¨m v·ªõi m·ªôt t·ª´ kh√≥a kh√°c nh√©!\".\n",
        "\n",
        "## Nguy√™n T·∫Øc V√†ng\n",
        "- **Gi·ªçng ƒëi·ªáu & X∆∞ng H√¥**: Lu√¥n nh·∫π nh√†ng, ƒë·ªìng c·∫£m, h·ªó tr·ª£. X∆∞ng h√¥ \"Con C∆∞ng\" v√† \"Ba m·∫π\".\n",
        "- **ƒê·ªô d√†i**: Ng·∫Øn g·ªçn (15-25 t·ª´, t·ªëi ƒëa 150 k√Ω t·ª±), vƒÉn phong ƒëa d·∫°ng, kh√¥ng l·∫∑p l·∫°i.\n",
        "- **Th√°i ƒë·ªô**: Tr√°nh kh·∫≥ng ƒë·ªãnh \"kh√¥ng c√≥\", thay b·∫±ng \"ch∆∞a t√¨m th·∫•y\". Tuy·ªát ƒë·ªëi kh√¥ng c√≥ h√†m √Ω ng∆∞·ªùi d√πng t√¨m sai.\n",
        "- **T√™n Th∆∞∆°ng Hi·ªáu**: **TUY·ªÜT ƒê·ªêI KH√îNG** nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu, tr·ª´ khi l·∫∑p l·∫°i ch√≠nh x√°c `USER_QUERY` ƒë·ªÉ x√°c nh·∫≠n.\n",
        "- **Nh·∫•n m·∫°nh**: Khi l·∫∑p l·∫°i `USER_QUERY`, ƒë·∫∑t trong th·∫ª `<b>`. V√≠ d·ª•: \"Con C∆∞ng t√¨m m√£i ch∆∞a ra <b>t√£ gi·∫•y Huggies</b>.\"\n",
        "- **K·∫øt th√∫c**: Lu√¥n c√≥ m·ªôt emoji th√¢n thi·ªán ·ªü cu·ªëi.\n",
        "\n",
        "# V√ç D·ª§ MINH H·ªåA\n",
        "\n",
        "## C√°c tr∆∞·ªùng h·ª£p is_in_scope = true\n",
        "**ƒê·∫ßu v√†o**: \"b√© 6 th√°ng b·ªã t√°o b√≥n n√™n u·ªëng s·ªØa g√¨\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"s·ªØa c√¥ng th·ª©c d·ªÖ ti√™u h√≥a cho b√© 6 th√°ng ch·ªëng t√°o b√≥n\", \"message_banner\": \"Con kh√≥c ƒë√™m nhi·ªÅu ph·∫£i kh√¥ng ba m·∫π? Con C∆∞ng t√¨m ƒë∆∞·ª£c nh·ªØng <b>s·ªØa c√¥ng th·ª©c</b> t·ªët nh·∫•t cho b√©! üçº\", \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y s·∫£n ph·∫©m cho b√©. Ba m·∫π th·ª≠ t√¨m b·∫±ng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§î\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"t√£ gi·∫•y huggies\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"t√£ gi·∫•y huggies cho b√©\", \"message_banner\": \"Ba m·∫π ƒëang t√¨m <b>t√£ Huggies</b> ph·∫£i kh√¥ng? Con C∆∞ng c√≥ nhi·ªÅu l·ª±a ch·ªçn ti·ªán l·ª£i v√† an to√†n cho b√© y√™u nh√©! üë∂\", \"message_no_result\": \"Con C∆∞ng t√¨m m√£i ch∆∞a ra <b>t√£ gi·∫•y Huggies</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c t√¨m v·ªõi t·ª´ kh√≥a chung h∆°n nh√©! ü•∫\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"bao cao su\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"bao cao su an to√†n\", \"message_banner\": \"S·∫£n ph·∫©m ch√≠nh h√£ng, <b>ch·∫•t l∆∞·ª£ng ƒë·∫£m b·∫£o</b> - Con C∆∞ng tin ba m·∫π s·∫Ω an t√¢m cho s·ª©c kh·ªèe gia ƒë√¨nh üíï\", \"message_no_result\": \"Hmm, Con C∆∞ng ch∆∞a t√¨m ra <b>bao cao su</b>. Ba m·∫π th·ª≠ b·ªï sung th√™m th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m nh∆∞ th∆∞∆°ng hi·ªáu mong mu·ªën nh√©! ü§ó\"}`\n",
        "\n",
        "## C√°c tr∆∞·ªùng h·ª£p is_in_scope = false\n",
        "**ƒê·∫ßu v√†o**: \"th·ªùi ti·∫øt h√¥m nay nh∆∞ th·∫ø n√†o\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"R·∫•t ti·∫øc, Con C∆∞ng kh√¥ng c√≥ th√¥ng tin v·ªÅ th·ªùi ti·∫øt. Ba m·∫π th·ª≠ t√¨m ki·∫øm m·ªôt s·∫£n ph·∫©m c·ª• th·ªÉ nh√©! ü§î\", \"reasoning\": \"Truy v·∫•n h·ªèi v·ªÅ th·ªùi ti·∫øt, kh√¥ng li√™n quan ƒë·∫øn s·∫£n ph·∫©m.\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"Shopee c√≥ t·ªët h∆°n Concung kh√¥ng\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"Con C∆∞ng r·∫•t ti·∫øc kh√¥ng th·ªÉ so s√°nh v·ªõi n·ªÅn t·∫£ng kh√°c. Ba m·∫π c·∫ßn t√¨m s·∫£n ph·∫©m g√¨ ·∫°? ü§ó\", \"reasoning\": \"Truy v·∫•n so s√°nh v·ªõi ƒë·ªëi th·ªß c·∫°nh tranh.\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"c√°ch n·∫•u ch√°o cho b√©\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"Con C∆∞ng ch∆∞a c√≥ c√¥ng th·ª©c n·∫•u ƒÉn ·∫°. Ba m·∫π c√≥ mu·ªën t√¨m c√°c lo·∫°i ch√°o ƒÉn li·ªÅn dinh d∆∞·ª°ng cho b√© kh√¥ng? üç≤\", \"reasoning\": \"Truy v·∫•n h·ªèi v·ªÅ ki·∫øn th·ª©c chung, kh√¥ng ph·∫£i t√¨m ki·∫øm s·∫£n ph·∫©m.\"}`\n",
        "\n",
        "\n",
        "# Y√äU C·∫¶U ƒê·∫¶U RA\n",
        "Ch·ªâ tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá:\n",
        "```json\n",
        "{\n",
        "  \"is_in_scope\": boolean,\n",
        "  \"keyword\": \"t·ª´ kh√≥a t√¨m ki·∫øm ƒë∆∞·ª£c t·ªëi ∆∞u (ch·ªâ khi is_in_scope = true)\",\n",
        "  \"message_banner\": \"banner th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát (lu√¥n t·∫°o khi is_in_scope = true)\",\n",
        "  \"message_no_result\": \"th√¥ng b√°o khi kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m (lu√¥n ƒë∆∞·ª£c t·∫°o)\",\n",
        "  \"reasoning\":\"l√Ω do is_in_scope = false 10 - 20 t·ª´\"\n",
        "}\n",
        "```\n",
        "\n",
        "# C√ÅC B∆Ø·ªöC X·ª¨ L√ù\n",
        "1. **Ki·ªÉm tra ph·∫°m vi**: ƒê√°nh gi√° QUERY theo quy t·∫Øc `is_in_scope`.\n",
        "2. **T·∫°o message_no_result**: Lu√¥n t·∫°o `message_no_result` theo c√°c quy t·∫Øc ·ªü tr√™n, b·∫•t k·ªÉ `is_in_scope` l√† g√¨.\n",
        "3. **N·∫øu is_in_scope = false**: Tr·∫£ v·ªÅ JSON v·ªõi `is_in_scope=false`, `keyword=\"\"`, `message_banner=\"\"`, `message_no_result` ƒë√£ t·∫°o, v√† `reasoning` gi·∫£i th√≠ch ng·∫Øn g·ªçn.\n",
        "4. **N·∫øu is_in_scope = true**:\n",
        "   - **Ph√¢n t√≠ch QUERY**: X√°c ƒë·ªãnh ƒë·ªô chi ti·∫øt v√† r√µ r√†ng c·ªßa query.\n",
        "   - **X·ª≠ l√Ω CONCUNG_SUGGESTIONS** (khi c·∫ßn thi·∫øt):\n",
        "     - Ph√¢n t√≠ch suggestions ƒë·ªÉ t√¨m th∆∞∆°ng hi·ªáu ch√≠nh x√°c v√† th√¥ng tin s·∫£n ph·∫©m.\n",
        "     - ∆Øu ti√™n s·ª≠ d·ª•ng th∆∞∆°ng hi·ªáu t·ª´ suggestions n·∫øu c√≥.\n",
        "     - Gi·ªØ nguy√™n t·ª´ kh√≥a c·ªët l√µi t·ª´ QUERY.\n",
        "     - Ch·ªâ b·ªï sung th√¥ng tin lo·∫°i s·∫£n ph·∫©m chung t·ª´ suggestions, kh√¥ng th√™m c√°c chi ti·∫øt c·ª• th·ªÉ.\n",
        "     - KH√îNG s·ª≠ d·ª•ng suggestions cho `message_banner`.\n",
        "     - Chu·∫©n h√≥a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu (ƒë·ªô tin c·∫≠y cao): N·∫øu t·∫•t c·∫£ CONCUNG_SUGGESTIONS c√πng m·ªôt th∆∞∆°ng hi·ªáu ‚Üí s·ª≠a th∆∞∆°ng hi·ªáu trong `keyword` theo t√™n ƒë√∫ng; kh√¥ng th√™m thu·ªôc t√≠nh m·ªõi t·ª´ CONCUNG_SUGGESTIONS.\n",
        "   - Tr√≠ch xu·∫•t nhu c·∫ßu s·∫£n ph·∫©m c·ªët l√µi.\n",
        "   - X√°c ƒë·ªãnh thu·ªôc t√≠nh s·∫£n ph·∫©m (ƒë·ªô tu·ªïi, size, th∆∞∆°ng hi·ªáu...).\n",
        "   - Chuy·ªÉn ƒë·ªïi v·∫•n ƒë·ªÅ th√†nh gi·∫£i ph√°p s·∫£n ph·∫©m.\n",
        "   - T·ªëi ∆∞u th√†nh t·ª´ kh√≥a t√¨m ki·∫øm hi·ªáu qu·∫£ (C√ì TH·ªÇ bao g·ªìm t√™n th∆∞∆°ng hi·ªáu t·ª´ query g·ªëc).\n",
        "   - **T·∫°o `message_banner` theo quy tr√¨nh nghi√™m ng·∫∑t sau**:\n",
        "     - **B1: Ki·ªÉm tra \"QUY T·∫ÆC ∆ØU TI√äN\"**. N·∫øu ƒë·ªß ƒëi·ªÅu ki·ªán, t·∫°o banner C√ì ch·ª©a t√™n th∆∞∆°ng hi·ªáu v√† chuy·ªÉn sang b∆∞·ªõc ti·∫øp theo.\n",
        "     - **B2: N·∫øu kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán ∆∞u ti√™n**, t·∫°o banner theo \"QUY T·∫ÆC CHUNG\" (tuy·ªát ƒë·ªëi kh√¥ng ch·ª©a t√™n th∆∞∆°ng hi·ªáu).\n",
        "   - **CRITICAL**: ƒê·∫£m b·∫£o `message_banner` tu√¢n th·ªß ƒë√∫ng quy tr√¨nh B1-B2 ·ªü tr√™n. `message_no_result` ch·ªâ ƒë∆∞·ª£c ph√©p ch·ª©a t√™n th∆∞∆°ng hi·ªáu khi tr√≠ch d·∫´n l·∫°i `USER_QUERY` v√† ƒë·∫∑t trong th·∫ª `<b>`.\n",
        "   - Tr·∫£ v·ªÅ JSON v·ªõi `keyword`, `message_banner`, v√† `message_no_result` ƒë√£ ƒë∆∞·ª£c t·∫°o.\n",
        "-----\n",
        "                # D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO\n",
        "                QUERY: ƒë·ªìng h·ªì chim nh·∫°i gi·ªçng \n",
        "                CONCUNG_SUGGESTIONS: \"xe √¥ t√¥ ƒë·ªìng h·ªì, v√≤ng tay b√© g√°i ƒë·ªìng h·ªì, v·ªõ ƒë·ªìng Natri b√© trai\" \n",
        "\"\"\"\n",
        "\n",
        "answer = ask_question(my_question,  temperature=0.7)\n",
        "print(f\"C√¢u h·ªèi: {my_question}\")\n",
        "print(f\"\\nTr·∫£ l·ªùi:\\n{answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# X√≥a GPU cache n·∫øu g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ memory\n",
        "clear_gpu_memory()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    print(f\"GPU memory allocated: {allocated:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {reserved:.2f} GB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
