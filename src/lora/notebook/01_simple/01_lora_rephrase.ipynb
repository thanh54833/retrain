{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune Qwen 2.5 7B v·ªõi Dataset Rephrase s·ª≠ d·ª•ng LoRA\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n fine-tune Qwen 2.5 7B Instruct v·ªõi dataset Rephrase ƒë·ªÉ t·∫°o JSON output t·ª´ query ti·∫øng Vi·ªát.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Base Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **PEFT Method**: LoRA (r=8 ho·∫∑c r=16)\n",
        "- **Dataset**: Rephrase (1,000 samples)\n",
        "- **Task**: Text-to-JSON Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Ph√°t hi·ªán macOS - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\n",
            "‚ö† torch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.24.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.9 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.9.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (6.9 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.20.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from torch) (4.15.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec>=0.8.5 (from torch)\n",
            "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting numpy (from torchvision)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision)\n",
            "  Downloading pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
            "Downloading torch-2.9.1-cp310-none-macosx_11_0_arm64.whl (74.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.24.1-cp310-cp310-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.9.1-cp310-cp310-macosx_11_0_arm64.whl (805 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m805.9/805.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.0.0-cp310-cp310-macosx_11_0_arm64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.20.1-py3-none-any.whl (16 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp310-cp310-macosx_11_0_arm64.whl (12 kB)\n",
            "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mpmath, sympy, pillow, numpy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch, torchvision, torchaudio\n",
            "Successfully installed MarkupSafe-3.0.3 filelock-3.20.1 fsspec-2025.12.0 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 numpy-2.2.6 pillow-12.0.0 sympy-1.14.0 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1\n",
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† transformers>=4.35.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers>=4.35.0\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.35.0) (3.20.1)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.35.0)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers>=4.35.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from transformers>=4.35.0) (25.0)\n",
            "Collecting pyyaml>=5.1 (from transformers>=4.35.0)\n",
            "  Downloading pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.35.0)\n",
            "  Downloading regex-2025.11.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers>=4.35.0)\n",
            "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.35.0)\n",
            "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers>=4.35.0)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (4.1 kB)\n",
            "Collecting tqdm>=4.27 (from transformers>=4.35.0)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (4.15.0)\n",
            "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0)\n",
            "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers>=4.35.0)\n",
            "  Downloading charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->transformers>=4.35.0)\n",
            "  Using cached idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers>=4.35.0)\n",
            "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->transformers>=4.35.0)\n",
            "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyyaml-6.0.3-cp310-cp310-macosx_11_0_arm64.whl (174 kB)\n",
            "Downloading regex-2025.11.3-cp310-cp310-macosx_11_0_arm64.whl (288 kB)\n",
            "Downloading safetensors-0.7.0-cp38-abi3-macosx_11_0_arm64.whl (447 kB)\n",
            "Using cached tokenizers-0.22.1-cp39-abi3-macosx_11_0_arm64.whl (2.9 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp310-cp310-macosx_10_9_universal2.whl (209 kB)\n",
            "Downloading hf_xet-1.2.0-cp37-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, hf-xet, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed certifi-2025.11.12 charset_normalizer-3.4.4 hf-xet-1.2.0 huggingface-hub-0.36.0 idna-3.11 pyyaml-6.0.3 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 urllib3-2.6.2\n",
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† peft>=0.6.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting peft>=0.6.0\n",
            "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from peft>=0.6.0) (25.0)\n",
            "Requirement already satisfied: psutil in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from peft>=0.6.0) (7.2.0)\n",
            "Requirement already satisfied: pyyaml in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.13.0 in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (2.9.1)\n",
            "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (4.67.1)\n",
            "Collecting accelerate>=0.21.0 (from peft>=0.6.0)\n",
            "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: safetensors in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (0.7.0)\n",
            "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/homebrew/lib/python3.10/site-packages (from peft>=0.6.0) (0.36.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (3.20.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2025.12.0)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (2.32.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft>=0.6.0) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.6.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch>=1.13.0->peft>=0.6.0) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.10/site-packages (from transformers->peft>=0.6.0) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/lib/python3.10/site-packages (from transformers->peft>=0.6.0) (0.22.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft>=0.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft>=0.6.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft>=0.6.0) (2025.11.12)\n",
            "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-1.12.0 peft-0.18.0\n",
            "‚úì peft>=0.6.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† datasets>=2.14.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets>=2.14.0\n",
            "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (3.20.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (2.2.6)\n",
            "Collecting pyarrow>=21.0.0 (from datasets>=2.14.0)\n",
            "  Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.1 kB)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets>=2.14.0)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting pandas (from datasets>=2.14.0)\n",
            "  Downloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (2.32.5)\n",
            "Collecting httpx<1.0.0 (from datasets>=2.14.0)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (4.67.1)\n",
            "Collecting xxhash (from datasets>=2.14.0)\n",
            "  Downloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.19 (from datasets>=2.14.0)\n",
            "  Downloading multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
            "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (0.36.0)\n",
            "Requirement already satisfied: packaging in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from datasets>=2.14.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.10/site-packages (from datasets>=2.14.0) (6.0.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading aiohttp-3.13.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
            "Collecting anyio (from httpx<1.0.0->datasets>=2.14.0)\n",
            "  Using cached anyio-4.12.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: certifi in /opt/homebrew/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.14.0) (2025.11.12)\n",
            "Collecting httpcore==1.* (from httpx<1.0.0->datasets>=2.14.0)\n",
            "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: idna in /opt/homebrew/lib/python3.10/site-packages (from httpx<1.0.0->datasets>=2.14.0) (3.11)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0->datasets>=2.14.0)\n",
            "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/lib/python3.10/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets>=2.14.0) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=2.14.0) (2.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from pandas->datasets>=2.14.0) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas->datasets>=2.14.0)\n",
            "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas->datasets>=2.14.0)\n",
            "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Using cached attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (20 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading multidict-6.7.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.14.0)\n",
            "  Downloading yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (75 kB)\n",
            "Requirement already satisfied: six>=1.5 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.14.0) (1.17.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/phamthanh/Library/Python/3.10/lib/python/site-packages (from anyio->httpx<1.0.0->datasets>=2.14.0) (1.3.1)\n",
            "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
            "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "Downloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Downloading multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
            "Downloading pyarrow-22.0.0-cp310-cp310-macosx_12_0_arm64.whl (34.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m34.2/34.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.6.0-cp310-cp310-macosx_11_0_arm64.whl (30 kB)\n",
            "Downloading aiohttp-3.13.2-cp310-cp310-macosx_11_0_arm64.whl (489 kB)\n",
            "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "Using cached anyio-4.12.0-py3-none-any.whl (113 kB)\n",
            "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Using cached attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading frozenlist-1.8.0-cp310-cp310-macosx_11_0_arm64.whl (49 kB)\n",
            "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Downloading multidict-6.7.0-cp310-cp310-macosx_11_0_arm64.whl (44 kB)\n",
            "Downloading propcache-0.4.1-cp310-cp310-macosx_11_0_arm64.whl (47 kB)\n",
            "Downloading yarl-1.22.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\n",
            "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, h11, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, httpcore, anyio, aiosignal, httpx, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.12.0\n",
            "    Uninstalling fsspec-2025.12.0:\n",
            "      Successfully uninstalled fsspec-2025.12.0\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 anyio-4.12.0 async-timeout-5.0.1 attrs-25.4.0 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 fsspec-2025.10.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 tzdata-2025.3 xxhash-3.6.0 yarl-1.22.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì datasets>=2.14.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (CH·∫†Y CELL N√ÄY TR∆Ø·ªöC - ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "def check_and_install(package_name, install_cmd_list=None):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "    \n",
        "    try:\n",
        "        # Th·ª≠ import ƒë·ªÉ ki·ªÉm tra\n",
        "        if package_import == \"torch\":\n",
        "            import torch\n",
        "            print(f\"‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "        elif package_import == \"sklearn\":\n",
        "            import sklearn\n",
        "            print(f\"‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {sklearn.__version__})\")\n",
        "        else:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "        if install_cmd_list:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + install_cmd_list)\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "\n",
        "# Ph√°t hi·ªán h·ªá ƒëi·ªÅu h√†nh v√† c√†i ƒë·∫∑t PyTorch ph√π h·ª£p\n",
        "system = platform.system()\n",
        "is_macos = system == \"Darwin\"\n",
        "is_linux = system == \"Linux\"\n",
        "\n",
        "if is_macos:\n",
        "    # macOS: C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\n",
        "    print(\"Ph√°t hi·ªán macOS - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "elif is_linux:\n",
        "    # Linux: Th·ª≠ c√†i ƒë·∫∑t v·ªõi CUDA support\n",
        "    print(\"Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\")\n",
        "    try:\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "    except:\n",
        "        # N·∫øu th·∫•t b·∫°i, c√†i ƒë·∫∑t t·ª´ PyPI\n",
        "        print(\"‚ö† Kh√¥ng th·ªÉ c√†i ƒë·∫∑t PyTorch v·ªõi CUDA, ƒëang c√†i ƒë·∫∑t t·ª´ PyPI...\")\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "else:\n",
        "    # Windows ho·∫∑c h·ªá th·ªëng kh√°c: C√†i ƒë·∫∑t t·ª´ PyPI\n",
        "    print(f\"Ph√°t hi·ªán {system} - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c packages kh√°c\n",
        "packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"scikit-learn\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "print(\"B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 1: Load v√† X·ª≠ L√Ω Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 1000\n",
            "Sample keys: dict_keys(['status', 'count', 'output', 'prompt_version', 'key', 'vector_distance', 'query'])\n",
            "\n",
            "Example sample:\n",
            "{\n",
            "  \"status\": 0,\n",
            "  \"count\": 12,\n",
            "  \"output\": {\n",
            "    \"keyword\": \"combo d·∫ßu tr√†m Cung ƒê√¨nh\",\n",
            "    \"reasoning\": \"\",\n",
            "    \"is_in_scope\": true,\n",
            "    \"message_banner\": \"Ba m·∫π ƒëang c·∫ßn <b>d·∫ßu tr√†m</b> an to√†n cho b√© v√† gia ƒë√¨nh? Con C∆∞ng lu√¥n s·∫µn s√†ng gi√∫p ba m·∫π chƒÉm s√≥c y√™u th∆∞∆°ng! üåø\",\n",
            "    \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y <b>combo d·∫ßu tr√†m ho√†ng cung</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§ó\"\n",
            "  },\n",
            "  \"prompt_version\": \"14\",\n",
            "  \"key\": \"d7c8973dd60986d1383be0676cb7eb1a63b7b632892456cd0fa656cae9efe7c3\",\n",
            "  \"vector_distance\": 0.37749016284942627,\n",
            "  \"query\": \"combo d·∫ßu tr√†m ho√†ng cung\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset_path = \"../../dataset/01_simple/01_dataset_rephrase.json\"\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(raw_data)}\")\n",
        "print(f\"Sample keys: {raw_data[0].keys()}\")\n",
        "print(f\"\\nExample sample:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'Dict' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format input prompt cho model\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput JSON:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_output\u001b[39m(data: \u001b[43mDict\u001b[49m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format output JSON t·ª´ data\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     output \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeyword\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_in_scope\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_in_scope\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_no_result\u001b[39m\u001b[38;5;124m\"\u001b[39m: data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage_no_result\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     }\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
          ]
        }
      ],
      "source": [
        "def format_prompt(query: str) -> str:\n",
        "    \"\"\"Format input prompt cho model\"\"\"\n",
        "    return f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "def format_output(data: Dict) -> str:\n",
        "    \"\"\"Format output JSON t·ª´ data\"\"\"\n",
        "    output = {\n",
        "        \"keyword\": data.get(\"keyword\", \"\"),\n",
        "        \"is_in_scope\": data.get(\"is_in_scope\", False),\n",
        "        \"reasoning\": data.get(\"reasoning\", \"\"),\n",
        "        \"message_banner\": data.get(\"message_banner\", \"\"),\n",
        "        \"message_no_result\": data.get(\"message_no_result\", \"\")\n",
        "    }\n",
        "    return json.dumps(output, ensure_ascii=False, indent=None)\n",
        "\n",
        "def prepare_dataset(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Chu·∫©n b·ªã dataset cho training\"\"\"\n",
        "    formatted_data = []\n",
        "    \n",
        "    for item in raw_data:\n",
        "        query = item.get(\"query\", \"\")\n",
        "        prompt = format_prompt(query)\n",
        "        output = format_output(item)\n",
        "        \n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output,\n",
        "            \"query\": query,\n",
        "            \"is_in_scope\": item.get(\"is_in_scope\", False)\n",
        "        })\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "# Format dataset\n",
        "formatted_data = prepare_dataset(raw_data)\n",
        "print(f\"Formatted samples: {len(formatted_data)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Prompt: {formatted_data[0]['prompt']}\")\n",
        "print(f\"Output: {formatted_data[0]['output'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia dataset: 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(\n",
        "    formatted_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in formatted_data]  # Stratified split\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in temp_data]\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "print(f\"Test: {len(test_data)} samples\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization (t√πy ch·ªçn, ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n",
        "# B·ªè comment n·∫øu c·∫ßn quantization\n",
        "use_quantization = True  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM\n",
        "\n",
        "if use_quantization:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Tokenizer loaded\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not use_quantization else None\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Ki·ªÉm tra s·ªë tham s·ªë\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 3: C·∫•u H√¨nh LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C·∫•u h√¨nh LoRA - Option 1: Conservative (r=8) - Khuy·∫øn ngh·ªã b·∫Øt ƒë·∫ßu\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                          # Rank\n",
        "    lora_alpha=16,               # Alpha = 2 * r\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.1,            # Dropout ƒë·ªÉ tr√°nh overfitting\n",
        "    bias=\"none\",                  # Kh√¥ng train bias\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Ho·∫∑c Option 2: Balanced (r=16) - Uncomment ƒë·ªÉ d√πng\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False\n",
        "# )\n",
        "\n",
        "print(\"LoRA Config:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# √Åp d·ª•ng LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Ki·ªÉm tra tham s·ªë c√≥ th·ªÉ train\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize prompt v√† output\"\"\"\n",
        "    # Combine prompt v√† output\n",
        "    texts = []\n",
        "    for prompt, output in zip(examples[\"prompt\"], examples[\"output\"]):\n",
        "        text = f\"{prompt} {output}\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n ƒë·ªô d√†i output\n",
        "        padding=False,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # Labels cho training (same as input_ids)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tokenized)} samples\")\n",
        "print(f\"Example tokenized length: {len(train_tokenized[0]['input_ids'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, kh√¥ng ph·∫£i masked LM\n",
        "    pad_to_multiple_of=8  # T·ªëi ∆∞u cho GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 5: C·∫•u H√¨nh Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = \"./results/qwen2.5-7b-rephrase-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=True,  # Ti·∫øt ki·ªám memory\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,  # Ch·ªâ gi·ªØ 3 checkpoints g·∫ßn nh·∫•t\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kh·ªüi t·∫°o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "print(f\"Training samples: {len(train_tokenized)}\")\n",
        "print(f\"Validation samples: {len(val_tokenized)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 6: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# B·∫Øt ƒë·∫ßu training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# L∆∞u model cu·ªëi c√πng\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nTraining completed! Model saved to {output_dir}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 7: Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate tr√™n test set\n",
        "print(\"Tokenizing test dataset...\")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validity_rate\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Ch·∫°y evaluation\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m validity_rate \u001b[38;5;241m=\u001b[39m evaluate_json_validity(\u001b[43mmodel\u001b[49m, tokenizer, test_data, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# ƒê√°nh gi√° JSON Validity\n",
        "def evaluate_json_validity(model, tokenizer, test_data, max_samples=50):\n",
        "    \"\"\"ƒê√°nh gi√° t·ª∑ l·ªá JSON h·ª£p l·ªá\"\"\"\n",
        "    model.eval()\n",
        "    valid_count = 0\n",
        "    total_count = 0\n",
        "    \n",
        "    for i, item in enumerate(tqdm(test_data[:max_samples], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        \n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract JSON (sau \"Output JSON:\")\n",
        "        if \"Output JSON:\" in generated_text:\n",
        "            json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "        else:\n",
        "            json_text = generated_text\n",
        "        \n",
        "        # Ki·ªÉm tra JSON validity\n",
        "        try:\n",
        "            parsed = json.loads(json_text)\n",
        "            valid_count += 1\n",
        "        except:\n",
        "            if total_count < 3:  # Print first few errors\n",
        "                print(f\"\\nError parsing JSON {i}:\")\n",
        "                print(f\"Generated: {json_text[:200]}\")\n",
        "        \n",
        "        total_count += 1\n",
        "    \n",
        "    validity_rate = valid_count / total_count if total_count > 0 else 0\n",
        "    print(f\"\\nJSON Validity Rate: {validity_rate:.2%} ({valid_count}/{total_count})\")\n",
        "    return validity_rate\n",
        "\n",
        "# Ch·∫°y evaluation\n",
        "validity_rate = evaluate_json_validity(model, tokenizer, test_data, max_samples=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 8: Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_json_response(model, tokenizer, query: str, max_length=512):\n",
        "    \"\"\"Generate JSON response t·ª´ query\"\"\"\n",
        "    prompt = f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract JSON\n",
        "    if \"Output JSON:\" in generated_text:\n",
        "        json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "    else:\n",
        "        json_text = generated_text\n",
        "    \n",
        "    # Parse JSON\n",
        "    try:\n",
        "        result = json.loads(json_text)\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        print(f\"Generated text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Test inference\n",
        "test_queries = [\n",
        "    \"s·ªØa cho b√© 6 th√°ng\",\n",
        "    \"t√£ b·ªâm size M\",\n",
        "    \"ƒë·ªì ch∆°i cho tr·∫ª s∆° sinh\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference:\\n\")\n",
        "for query in test_queries:\n",
        "    result = generate_json_response(model, tokenizer, query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Result: {json.dumps(result, ensure_ascii=False, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L∆∞u adapter (ch·ªâ LoRA weights, nh·ªè)\n",
        "adapter_path = f\"{output_dir}/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "print(\"\\nƒê·ªÉ load l·∫°i model sau n√†y:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{model_name}\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## L∆∞u √ù\n",
        "\n",
        "- ƒêi·ªÅu ch·ªânh `per_device_train_batch_size` v√† `gradient_accumulation_steps` d·ª±a tr√™n GPU memory\n",
        "- C√≥ th·ªÉ th·ª≠ `r=16` n·∫øu `r=8` kh√¥ng ƒë·ªß hi·ªáu su·∫•t\n",
        "- Monitor training loss v√† validation loss ƒë·ªÉ tr√°nh overfitting\n",
        "- S·ª≠ d·ª•ng TensorBoard ƒë·ªÉ theo d√µi: `tensorboard --logdir ./results/qwen2.5-7b-rephrase-lora/logs`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
