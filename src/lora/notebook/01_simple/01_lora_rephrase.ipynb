{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune Qwen 2.5 7B với Dataset Rephrase sử dụng LoRA\n",
        "\n",
        "Notebook này hướng dẫn fine-tune Qwen 2.5 7B Instruct với dataset Rephrase để tạo JSON output từ query tiếng Việt.\n",
        "\n",
        "## Thông tin\n",
        "- **Base Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **PEFT Method**: LoRA (r=8 hoặc r=16)\n",
        "- **Dataset**: Rephrase (1,000 samples)\n",
        "- **Task**: Text-to-JSON Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (chạy lần đầu)\n",
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install transformers>=4.35.0\n",
        "# !pip install peft>=0.6.0\n",
        "# !pip install datasets>=2.14.0\n",
        "# !pip install accelerate>=0.24.0\n",
        "# !pip install bitsandbytes>=0.41.0\n",
        "# !pip install scikit-learn\n",
        "# !pip install tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Kiểm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 1: Load và Xử Lý Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset_path = \"../../dataset/01_simple/01_dataset_rephrase.json\"\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(raw_data)}\")\n",
        "print(f\"Sample keys: {raw_data[0].keys()}\")\n",
        "print(f\"\\nExample sample:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_prompt(query: str) -> str:\n",
        "    \"\"\"Format input prompt cho model\"\"\"\n",
        "    return f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "def format_output(data: Dict) -> str:\n",
        "    \"\"\"Format output JSON từ data\"\"\"\n",
        "    output = {\n",
        "        \"keyword\": data.get(\"keyword\", \"\"),\n",
        "        \"is_in_scope\": data.get(\"is_in_scope\", False),\n",
        "        \"reasoning\": data.get(\"reasoning\", \"\"),\n",
        "        \"message_banner\": data.get(\"message_banner\", \"\"),\n",
        "        \"message_no_result\": data.get(\"message_no_result\", \"\")\n",
        "    }\n",
        "    return json.dumps(output, ensure_ascii=False, indent=None)\n",
        "\n",
        "def prepare_dataset(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Chuẩn bị dataset cho training\"\"\"\n",
        "    formatted_data = []\n",
        "    \n",
        "    for item in raw_data:\n",
        "        query = item.get(\"query\", \"\")\n",
        "        prompt = format_prompt(query)\n",
        "        output = format_output(item)\n",
        "        \n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output,\n",
        "            \"query\": query,\n",
        "            \"is_in_scope\": item.get(\"is_in_scope\", False)\n",
        "        })\n",
        "    \n",
        "    return formatted_data\n",
        "\n",
        "# Format dataset\n",
        "formatted_data = prepare_dataset(raw_data)\n",
        "print(f\"Formatted samples: {len(formatted_data)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Prompt: {formatted_data[0]['prompt']}\")\n",
        "print(f\"Output: {formatted_data[0]['output'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Chia dataset: 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(\n",
        "    formatted_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in formatted_data]  # Stratified split\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in temp_data]\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "print(f\"Test: {len(test_data)} samples\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 2: Load Model và Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# Cấu hình quantization (tùy chọn, để tiết kiệm bộ nhớ)\n",
        "# Bỏ comment nếu cần quantization\n",
        "use_quantization = True  # Đặt False nếu có đủ VRAM\n",
        "\n",
        "if use_quantization:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# Đảm bảo có pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Tokenizer loaded\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not use_quantization else None\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Kiểm tra số tham số\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 3: Cấu Hình LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cấu hình LoRA - Option 1: Conservative (r=8) - Khuyến nghị bắt đầu\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                          # Rank\n",
        "    lora_alpha=16,               # Alpha = 2 * r\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.1,            # Dropout để tránh overfitting\n",
        "    bias=\"none\",                  # Không train bias\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Hoặc Option 2: Balanced (r=16) - Uncomment để dùng\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False\n",
        "# )\n",
        "\n",
        "print(\"LoRA Config:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Áp dụng LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Kiểm tra tham số có thể train\n",
        "model.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize prompt và output\"\"\"\n",
        "    # Combine prompt và output\n",
        "    texts = []\n",
        "    for prompt, output in zip(examples[\"prompt\"], examples[\"output\"]):\n",
        "        text = f\"{prompt} {output}\"\n",
        "        texts.append(text)\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,  # Điều chỉnh dựa trên độ dài output\n",
        "        padding=False,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    \n",
        "    # Labels cho training (same as input_ids)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tokenized)} samples\")\n",
        "print(f\"Example tokenized length: {len(train_tokenized[0]['input_ids'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, không phải masked LM\n",
        "    pad_to_multiple_of=8  # Tối ưu cho GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 5: Cấu Hình Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_dir = \"./results/qwen2.5-7b-rephrase-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # Điều chỉnh dựa trên GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=True,  # Tiết kiệm memory\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,  # Chỉ giữ 3 checkpoints gần nhất\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Khởi tạo Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "print(f\"Training samples: {len(train_tokenized)}\")\n",
        "print(f\"Validation samples: {len(val_tokenized)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 6: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bắt đầu training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# Lưu model cuối cùng\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nTraining completed! Model saved to {output_dir}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 7: Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate trên test set\n",
        "print(\"Tokenizing test dataset...\")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 50\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m validity_rate\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Chạy evaluation\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m validity_rate \u001b[38;5;241m=\u001b[39m evaluate_json_validity(\u001b[43mmodel\u001b[49m, tokenizer, test_data, max_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "# Đánh giá JSON Validity\n",
        "def evaluate_json_validity(model, tokenizer, test_data, max_samples=50):\n",
        "    \"\"\"Đánh giá tỷ lệ JSON hợp lệ\"\"\"\n",
        "    model.eval()\n",
        "    valid_count = 0\n",
        "    total_count = 0\n",
        "    \n",
        "    for i, item in enumerate(tqdm(test_data[:max_samples], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "        \n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        \n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract JSON (sau \"Output JSON:\")\n",
        "        if \"Output JSON:\" in generated_text:\n",
        "            json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "        else:\n",
        "            json_text = generated_text\n",
        "        \n",
        "        # Kiểm tra JSON validity\n",
        "        try:\n",
        "            parsed = json.loads(json_text)\n",
        "            valid_count += 1\n",
        "        except:\n",
        "            if total_count < 3:  # Print first few errors\n",
        "                print(f\"\\nError parsing JSON {i}:\")\n",
        "                print(f\"Generated: {json_text[:200]}\")\n",
        "        \n",
        "        total_count += 1\n",
        "    \n",
        "    validity_rate = valid_count / total_count if total_count > 0 else 0\n",
        "    print(f\"\\nJSON Validity Rate: {validity_rate:.2%} ({valid_count}/{total_count})\")\n",
        "    return validity_rate\n",
        "\n",
        "# Chạy evaluation\n",
        "validity_rate = evaluate_json_validity(model, tokenizer, test_data, max_samples=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 8: Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_json_response(model, tokenizer, query: str, max_length=512):\n",
        "    \"\"\"Generate JSON response từ query\"\"\"\n",
        "    prompt = f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract JSON\n",
        "    if \"Output JSON:\" in generated_text:\n",
        "        json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "    else:\n",
        "        json_text = generated_text\n",
        "    \n",
        "    # Parse JSON\n",
        "    try:\n",
        "        result = json.loads(json_text)\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        print(f\"Generated text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Test inference\n",
        "test_queries = [\n",
        "    \"sữa cho bé 6 tháng\",\n",
        "    \"tã bỉm size M\",\n",
        "    \"đồ chơi cho trẻ sơ sinh\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference:\\n\")\n",
        "for query in test_queries:\n",
        "    result = generate_json_response(model, tokenizer, query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Result: {json.dumps(result, ensure_ascii=False, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lưu adapter (chỉ LoRA weights, nhỏ)\n",
        "adapter_path = f\"{output_dir}/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "print(\"\\nĐể load lại model sau này:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{model_name}\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lưu Ý\n",
        "\n",
        "- Điều chỉnh `per_device_train_batch_size` và `gradient_accumulation_steps` dựa trên GPU memory\n",
        "- Có thể thử `r=16` nếu `r=8` không đủ hiệu suất\n",
        "- Monitor training loss và validation loss để tránh overfitting\n",
        "- Sử dụng TensorBoard để theo dõi: `tensorboard --logdir ./results/qwen2.5-7b-rephrase-lora/logs`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
