# V·∫•n ƒë·ªÅ: Input d√†i g√¢y l·ªói Out of Memory (OOM)

## T√≥m t·∫Øt ng·∫Øn g·ªçn

**V·∫•n ƒë·ªÅ**: Khi b·∫°n g·ª≠i m·ªôt prompt r·∫•t d√†i (nhi·ªÅu t·ª´/c√¢u) v√†o model, GPU s·∫Ω h·∫øt b·ªô nh·ªõ v√† b√°o l·ªói "CUDA out of memory".

**Nguy√™n nh√¢n ch√≠nh**: Model c·∫ßn t√≠nh to√°n m·ªëi quan h·ªá gi·ªØa T·∫§T C·∫¢ c√°c t·ª´ trong prompt v·ªõi nhau. N·∫øu prompt c√≥ 2000 t·ª´, model ph·∫£i t√≠nh 2000 √ó 2000 = 4 tri·ªáu m·ªëi quan h·ªá! ƒêi·ªÅu n√†y ti√™u t·ªën r·∫•t nhi·ªÅu b·ªô nh·ªõ GPU.

**Gi·∫£i ph√°p ƒë∆°n gi·∫£n**: Gi·ªõi h·∫°n ƒë·ªô d√†i input (v√≠ d·ª•: t·ªëi ƒëa 2000 t·ª´) ho·∫∑c t·∫Øt m·ªôt s·ªë t√≠nh nƒÉng ti·∫øt ki·ªám b·ªô nh·ªõ.

---

## Hi·ªÉu ƒë∆°n gi·∫£n: T·∫°i sao input d√†i l·∫°i t·ªën b·ªô nh·ªõ?

### V√≠ d·ª• d·ªÖ hi·ªÉu

H√£y t∆∞·ªüng t∆∞·ª£ng b·∫°n ƒëang ƒë·ªçc m·ªôt cu·ªën s√°ch:

- **S√°ch ng·∫Øn (100 trang)**: B·∫°n c√≥ th·ªÉ nh·ªõ ƒë∆∞·ª£c c√°c nh√¢n v·∫≠t v√† m·ªëi quan h·ªá gi·ªØa h·ªç ‚Üí T·ªën √≠t "b·ªô nh·ªõ n√£o"
- **S√°ch d√†i (1000 trang)**: B·∫°n ph·∫£i nh·ªõ r·∫•t nhi·ªÅu nh√¢n v·∫≠t, m·ªëi quan h·ªá ph·ª©c t·∫°p ‚Üí T·ªën r·∫•t nhi·ªÅu "b·ªô nh·ªõ n√£o"

Model transformer c≈©ng v·∫≠y:
- **Prompt ng·∫Øn (100 t·ª´)**: Model ch·ªâ c·∫ßn nh·ªõ m·ªëi quan h·ªá gi·ªØa 100 t·ª´ ‚Üí T·ªën √≠t GPU memory
- **Prompt d√†i (2000 t·ª´)**: Model ph·∫£i nh·ªõ m·ªëi quan h·ªá gi·ªØa 2000 t·ª´ ‚Üí T·ªën r·∫•t nhi·ªÅu GPU memory

### C√¥ng th·ª©c ƒë∆°n gi·∫£n

```
B·ªô nh·ªõ c·∫ßn thi·∫øt = B·ªô nh·ªõ model + (S·ªë t·ª´ trong prompt)¬≤ √ó H·∫±ng s·ªë
```

**Quan tr·ªçng**: B·ªô nh·ªõ tƒÉng theo **b√¨nh ph∆∞∆°ng** s·ªë t·ª´, kh√¥ng ph·∫£i tƒÉng tuy·∫øn t√≠nh!

**V√≠ d·ª• c·ª• th·ªÉ**:
- 100 t·ª´ ‚Üí B·ªô nh·ªõ = 6 GB
- 500 t·ª´ ‚Üí B·ªô nh·ªõ = 6.5 GB (tƒÉng 0.5 GB)
- 1000 t·ª´ ‚Üí B·ªô nh·ªõ = 8 GB (tƒÉng 2 GB)
- 2000 t·ª´ ‚Üí B·ªô nh·ªõ = 13 GB (tƒÉng 7 GB!)
- 4000 t·ª´ ‚Üí B·ªô nh·ªõ = 35 GB (OOM - h·∫øt b·ªô nh·ªõ!)

**Nh·∫≠n x√©t**: Khi s·ªë t·ª´ tƒÉng g·∫•p ƒë√¥i (1000 ‚Üí 2000), b·ªô nh·ªõ tƒÉng g·∫•p 4 l·∫ßn (2 GB ‚Üí 8 GB)!

---

## Chi ti·∫øt k·ªπ thu·∫≠t: Model d√πng b·ªô nh·ªõ nh∆∞ th·∫ø n√†o?

Khi model x·ª≠ l√Ω m·ªôt prompt, n√≥ c·∫ßn 3 lo·∫°i b·ªô nh·ªõ:

### 1. B·ªô nh·ªõ cho Model (Model Weights) - C·ªë ƒë·ªãnh

**ƒê√¢y l√† g√¨?**
- ƒê√¢y l√† "ki·∫øn th·ª©c" c·ªßa model ƒë√£ ƒë∆∞·ª£c h·ªçc t·ª´ tr∆∞·ªõc
- Gi·ªëng nh∆∞ "b·ªô n√£o" c·ªßa model

**K√≠ch th∆∞·ªõc:**
- Qwen 7B kh√¥ng quantization: ~14 GB
- Qwen 7B v·ªõi 4-bit quantization: ~5-6 GB

**ƒê·∫∑c ƒëi·ªÉm:**
- K√≠ch th∆∞·ªõc **KH√îNG THAY ƒê·ªîI** d√π prompt d√†i hay ng·∫Øn
- Gi·ªëng nh∆∞ k√≠ch th∆∞·ªõc b·ªô n√£o c·ªßa b·∫°n kh√¥ng ƒë·ªïi d√π ƒë·ªçc s√°ch ng·∫Øn hay d√†i

### 2. B·ªô nh·ªõ cho T√≠nh to√°n (Activation Memory) - Ph·ª• thu·ªôc ƒë·ªô d√†i prompt

**ƒê√¢y l√† g√¨?**
- B·ªô nh·ªõ ƒë·ªÉ l∆∞u k·∫øt qu·∫£ t√≠nh to√°n t·∫°m th·ªùi khi model x·ª≠ l√Ω prompt
- Gi·ªëng nh∆∞ "gi·∫•y nh√°p" khi b·∫°n gi·∫£i b√†i to√°n

**C√≥ 2 ph·∫ßn ch√≠nh:**

#### a) Hidden States (Tr·∫°ng th√°i ·∫©n)
- Model l∆∞u "√Ω nghƒ©a" c·ªßa m·ªói t·ª´ sau m·ªói b∆∞·ªõc x·ª≠ l√Ω
- K√≠ch th∆∞·ªõc: `S·ªë t·ª´ √ó 4096 √ó 2 bytes` cho m·ªói layer
- V√≠ d·ª•: 2000 t·ª´ ‚Üí `2000 √ó 4096 √ó 2 = 16.38 MB` m·ªói layer
- V·ªõi 28 layers ‚Üí `16.38 MB √ó 28 = 458 MB`

#### b) Attention Matrices (Ma tr·∫≠n ch√∫ √Ω) - **QUAN TR·ªåNG NH·∫§T!**
- ƒê√¢y l√† ph·∫ßn t·ªën b·ªô nh·ªõ nh·∫•t!
- Model t√≠nh to√°n m·ª©c ƒë·ªô "ch√∫ √Ω" gi·ªØa m·ªói c·∫∑p t·ª´ v·ªõi nhau
- K√≠ch th∆∞·ªõc: `(S·ªë t·ª´)¬≤ √ó 32 √ó 2 bytes` cho m·ªói layer

**V√≠ d·ª• c·ª• th·ªÉ v·ªõi 2000 t·ª´:**
```
Attention memory m·ªói layer = 2000¬≤ √ó 32 √ó 2 bytes
                            = 4,000,000 √ó 32 √ó 2
                            = 256 MB
```

**V·ªõi 28 layers:**
```
T·ªïng attention memory = 256 MB √ó 28 = 7.17 GB
```

**ƒê√¢y l√† l√Ω do ch√≠nh g√¢y OOM!** Ch·ªâ ri√™ng attention matrices ƒë√£ t·ªën 7 GB cho 2000 t·ª´!

### 3. B·ªô nh·ªõ cho Cache (KV Cache) - T√πy ch·ªçn

**ƒê√¢y l√† g√¨?**
- Khi model generate t·ª´ng t·ª´ m·ªõi, n√≥ c√≥ th·ªÉ l∆∞u l·∫°i k·∫øt qu·∫£ t√≠nh to√°n tr∆∞·ªõc ƒë√≥ ƒë·ªÉ kh√¥ng ph·∫£i t√≠nh l·∫°i
- Gi·ªëng nh∆∞ "ghi ch√∫" ƒë·ªÉ kh√¥ng ph·∫£i ƒë·ªçc l·∫°i t·ª´ ƒë·∫ßu

**K√≠ch th∆∞·ªõc:**
- V·ªõi 2000 t·ª´: ~917 MB
- Ch·ªâ c√≥ khi `use_cache=True`

**L∆∞u √Ω:** C√≥ th·ªÉ t·∫Øt ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ (nh∆∞ng s·∫Ω ch·∫≠m h∆°n m·ªôt ch√∫t)

---

## B·∫£ng so s√°nh: B·ªô nh·ªõ c·∫ßn thi·∫øt theo ƒë·ªô d√†i prompt

**Gi·∫£ s·ª≠ s·ª≠ d·ª•ng Qwen 7B v·ªõi 4-bit quantization (model weights = 5 GB):**

| ƒê·ªô d√†i prompt | Attention Memory | T·ªïng b·ªô nh·ªõ | Ghi ch√∫ |
|---------------|------------------|-------------|---------|
| 100 t·ª´        | 0.02 GB          | ~6 GB       | ‚úÖ An to√†n |
| 500 t·ª´        | 0.45 GB          | ~6.5 GB     | ‚úÖ An to√†n |
| 1000 t·ª´       | 1.8 GB           | ~8 GB       | ‚ö†Ô∏è C·∫ßn c·∫©n th·∫≠n |
| 2000 t·ª´       | 7.2 GB           | ~13 GB      | ‚ùå OOM tr√™n GPU 12GB |
| 4000 t·ª´       | 28.7 GB          | ~35 GB      | ‚ùå OOM tr√™n h·∫ßu h·∫øt GPU |

**Nh·∫≠n x√©t:**
- V·ªõi GPU 12-16 GB: N√™n gi·ªõi h·∫°n prompt ‚â§ 1500 t·ª´
- V·ªõi GPU 24 GB: C√≥ th·ªÉ x·ª≠ l√Ω prompt ‚â§ 2500 t·ª´
- V·ªõi GPU 40+ GB (A100): C√≥ th·ªÉ x·ª≠ l√Ω prompt d√†i h∆°n nhi·ªÅu

---

## T·∫°i sao Attention l·∫°i t·ªën b·ªô nh·ªõ nhi·ªÅu ƒë·∫øn v·∫≠y?

### Gi·∫£i th√≠ch ƒë∆°n gi·∫£n

Khi b·∫°n ƒë·ªçc c√¢u: **"Con m√®o ng·ªìi tr√™n t·∫•m th·∫£m"**

Model c·∫ßn hi·ªÉu:
- T·ª´ "m√®o" li√™n quan ƒë·∫øn "ng·ªìi" nh∆∞ th·∫ø n√†o?
- T·ª´ "ng·ªìi" li√™n quan ƒë·∫øn "th·∫£m" nh∆∞ th·∫ø n√†o?
- T·ª´ "m√®o" li√™n quan ƒë·∫øn "th·∫£m" nh∆∞ th·∫ø n√†o?
- ... v√† t·∫•t c·∫£ c√°c c·∫∑p t·ª´ kh√°c

**V·ªõi 6 t·ª´**, model c·∫ßn t√≠nh: 6 √ó 6 = 36 m·ªëi quan h·ªá

**V·ªõi 2000 t·ª´**, model c·∫ßn t√≠nh: 2000 √ó 2000 = **4,000,000 m·ªëi quan h·ªá!**

M·ªói m·ªëi quan h·ªá c·∫ßn l∆∞u m·ªôt s·ªë (attention score), v√† v·ªõi 28 layers, s·ªë l∆∞·ª£ng n√†y nh√¢n l√™n r·∫•t nhi·ªÅu!

### H√¨nh ·∫£nh minh h·ªça (text)

```
Prompt ng·∫Øn (5 t·ª´):
[1] [2] [3] [4] [5]
 ‚Üì   ‚Üì   ‚Üì   ‚Üì   ‚Üì
Model ch·ªâ c·∫ßn t√≠nh 5√ó5 = 25 m·ªëi quan h·ªá
B·ªô nh·ªõ: Nh·ªè

Prompt d√†i (2000 t·ª´):
[1] [2] [3] ... [2000]
 ‚Üì   ‚Üì   ‚Üì       ‚Üì
Model c·∫ßn t√≠nh 2000√ó2000 = 4,000,000 m·ªëi quan h·ªá!
B·ªô nh·ªõ: R·∫•t l·ªõn (7+ GB)
```

---

## Tr∆∞·ªùng h·ª£p th·ª±c t·∫ø: L·ªói OOM

### Error message th∆∞·ªùng g·∫∑p

```
OutOfMemoryError: CUDA out of memory. 
Tried to allocate 166.00 MiB. 
GPU 0 has a total capacity of 15.70 GiB 
of which 30.81 MiB is free.
```

### Ph√¢n t√≠ch l·ªói

**T√¨nh hu·ªëng:**
- GPU c√≥ t·ªïng c·ªông: **15.70 GB**
- Model ƒë√£ d√πng: **5.71 GB** (model weights + m·ªôt ph·∫ßn activation)
- C√≤n l·∫°i: **30 MB** (r·∫•t √≠t!)
- Khi generate v·ªõi prompt d√†i (~2000 t·ª´), c·∫ßn th√™m: **7-8 GB**
- **K·∫øt qu·∫£**: Kh√¥ng ƒë·ªß b·ªô nh·ªõ ‚Üí L·ªói OOM

**Nguy√™n nh√¢n ph·ª•:**
- C√≥ 2 process kh√°c ƒëang d√πng GPU:
  - Process 3141301: 6.09 GB
  - Process 3144297: 9.58 GB
- T·ªïng: 15.67 GB ƒë√£ ƒë∆∞·ª£c s·ª≠ d·ª•ng
- Process hi·ªán t·∫°i ch·ªâ c√≤n ~30 MB ƒë·ªÉ l√†m vi·ªác

**Gi·∫£i ph√°p:**
1. ƒê√≥ng c√°c process kh√°c ƒëang d√πng GPU
2. Gi·∫£m ƒë·ªô d√†i prompt
3. T·∫Øt KV cache
4. Gi·∫£m max_new_tokens

---

## Gi·∫£i ph√°p chi ti·∫øt

### Gi·∫£i ph√°p 1: Gi·ªõi h·∫°n ƒë·ªô d√†i Input (Truncation) ‚≠ê **KHUY·∫æN NGH·ªä**

**C√°ch l√†m:**
```python
# Ch·ªâ l·∫•y 2000 t·ª´ ƒë·∫ßu ti√™n c·ªßa prompt
model_inputs = tokenizer(
    [text], 
    return_tensors="pt",
    truncation=True,        # B·∫≠t truncation
    max_length=2000,         # Gi·ªõi h·∫°n 2000 tokens
    padding=False
)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Gi·∫£m b·ªô nh·ªõ ƒë√°ng k·ªÉ (t·ª´ 13 GB xu·ªëng ~8 GB v·ªõi 2000 t·ª´)
- ‚úÖ ƒê∆°n gi·∫£n, d·ªÖ √°p d·ª•ng
- ‚úÖ Hi·ªáu qu·∫£ cao

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è M·∫•t th√¥ng tin n·∫øu prompt qu√° d√†i
- ‚ö†Ô∏è C√≥ th·ªÉ ·∫£nh h∆∞·ªüng ch·∫•t l∆∞·ª£ng n·∫øu th√¥ng tin quan tr·ªçng b·ªã c·∫Øt

**Khi n√†o d√πng:**
- Prompt r·∫•t d√†i (> 2000 t·ª´)
- Kh√¥ng c·∫ßn to√†n b·ªô th√¥ng tin trong prompt
- ∆Øu ti√™n tr√°nh OOM h∆°n l√† gi·ªØ nguy√™n prompt

---

### Gi·∫£i ph√°p 2: T·∫Øt KV Cache ‚≠ê **KHUY·∫æN NGH·ªä**

**C√°ch l√†m:**
```python
model.generate(
    **model_inputs,
    use_cache=False,  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
    max_new_tokens=256,
    ...
)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Ti·∫øt ki·ªám ~1 GB b·ªô nh·ªõ (v·ªõi prompt 2000 t·ª´)
- ‚úÖ V·∫´n gi·ªØ ƒë∆∞·ª£c to√†n b·ªô prompt (kh√¥ng m·∫•t th√¥ng tin)
- ‚úÖ D·ªÖ √°p d·ª•ng

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è Ch·∫≠m h∆°n m·ªôt ch√∫t (ph·∫£i t√≠nh l·∫°i attention m·ªói b∆∞·ªõc)
- ‚ö†Ô∏è Kh√¥ng ·∫£nh h∆∞·ªüng nhi·ªÅu ƒë·∫øn b·ªô nh·ªõ attention matrices

**Khi n√†o d√πng:**
- C·∫ßn gi·ªØ to√†n b·ªô prompt
- Ch·∫•p nh·∫≠n ch·∫≠m h∆°n m·ªôt ch√∫t
- K·∫øt h·ª£p v·ªõi c√°c gi·∫£i ph√°p kh√°c

---

### Gi·∫£i ph√°p 3: Gi·∫£m s·ªë t·ª´ Generate

**C√°ch l√†m:**
```python
model.generate(
    **model_inputs,
    max_new_tokens=128,  # Gi·∫£m t·ª´ 512 xu·ªëng 128
    ...
)
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Gi·∫£m b·ªô nh·ªõ cho output sequence
- ‚úÖ Generate nhanh h∆°n
- ‚úÖ Output ng·∫Øn g·ªçn h∆°n

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è C√≥ th·ªÉ kh√¥ng ƒë·ªß d√†i cho m·ªôt s·ªë task
- ‚ö†Ô∏è Kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ ch√≠nh (attention memory)

**Khi n√†o d√πng:**
- K·∫øt h·ª£p v·ªõi c√°c gi·∫£i ph√°p kh√°c
- Task kh√¥ng c·∫ßn output d√†i
- ∆Øu ti√™n t·ªëc ƒë·ªô

---

### Gi·∫£i ph√°p 4: Gi·∫£i ph√≥ng GPU Memory

**C√°ch l√†m:**
```python
import torch
import gc

# Gi·∫£i ph√≥ng b·ªô nh·ªõ kh√¥ng d√πng
torch.cuda.empty_cache()      # X√≥a PyTorch cache
torch.cuda.synchronize()      # ƒê·ª£i GPU ho√†n th√†nh
gc.collect()                  # Python garbage collection
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ ƒê∆°n gi·∫£n
- ‚úÖ C√≥ th·ªÉ gi·∫£i ph√≥ng m·ªôt ph·∫ßn b·ªô nh·ªõ

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è Ch·ªâ gi·∫£i ph√≥ng ƒë∆∞·ª£c m·ªôt ph·∫ßn nh·ªè
- ‚ö†Ô∏è Kh√¥ng gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ c·ªët l√µi (attention memory qu√° l·ªõn)

**Khi n√†o d√πng:**
- Tr∆∞·ªõc khi generate v·ªõi prompt d√†i
- K·∫øt h·ª£p v·ªõi c√°c gi·∫£i ph√°p kh√°c
- Sau khi x·ª≠ l√Ω nhi·ªÅu requests

---

### Gi·∫£i ph√°p 5: S·ª≠ d·ª•ng Model nh·ªè h∆°n

**C√°c l·ª±a ch·ªçn:**
- Qwen 2.5-3B (thay v√¨ 7B) ‚Üí Gi·∫£m ~50% b·ªô nh·ªõ model
- Qwen 2.5-1.5B ‚Üí Gi·∫£m ~75% b·ªô nh·ªõ model

**∆Øu ƒëi·ªÉm:**
- ‚úÖ B·ªô nh·ªõ model th·∫•p h∆°n ƒë√°ng k·ªÉ
- ‚úÖ C√≥ th·ªÉ x·ª≠ l√Ω prompt d√†i h∆°n
- ‚úÖ V·∫´n ƒë·ªß t·ªët cho nhi·ªÅu task

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è Ch·∫•t l∆∞·ª£ng c√≥ th·ªÉ k√©m h∆°n 7B
- ‚ö†Ô∏è V·∫´n g·∫∑p v·∫•n ƒë·ªÅ v·ªõi prompt r·∫•t d√†i (v√¨ attention memory v·∫´n tƒÉng theo b√¨nh ph∆∞∆°ng)

**Khi n√†o d√πng:**
- GPU r·∫•t nh·ªè (< 12 GB)
- Ch·∫•p nh·∫≠n gi·∫£m nh·∫π ch·∫•t l∆∞·ª£ng
- C·∫ßn x·ª≠ l√Ω prompt d√†i th∆∞·ªùng xuy√™n

---

### Gi·∫£i ph√°p 6: ƒê√≥ng c√°c Process kh√°c

**C√°ch l√†m:**
```bash
# Ki·ªÉm tra c√°c process ƒëang d√πng GPU
nvidia-smi

# Kill process n·∫øu c·∫ßn (c·∫©n th·∫≠n!)
kill -9 <PID>
```

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Gi·∫£i ph√≥ng b·ªô nh·ªõ ngay l·∫≠p t·ª©c
- ‚úÖ Hi·ªáu qu·∫£ cao

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è C√≥ th·ªÉ ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c task kh√°c
- ‚ö†Ô∏è C·∫ßn c·∫©n th·∫≠n khi kill process

**Khi n√†o d√πng:**
- C√≥ process kh√¥ng c·∫ßn thi·∫øt ƒëang ch·∫°y
- C·∫ßn b·ªô nh·ªõ ngay l·∫≠p t·ª©c
- ƒê√£ th·ª≠ c√°c gi·∫£i ph√°p kh√°c nh∆∞ng v·∫´n thi·∫øu b·ªô nh·ªõ

---

## Best Practices (Th·ª±c h√†nh t·ªët nh·∫•t)

### 1. Ki·ªÉm tra ƒë·ªô d√†i Input tr∆∞·ªõc khi Generate

```python
# Tokenize v√† ki·ªÉm tra ƒë·ªô d√†i
model_inputs = tokenizer([text], return_tensors="pt")
input_length = model_inputs.input_ids.shape[1]

print(f"Input length: {input_length} tokens")

# T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh n·∫øu qu√° d√†i
if input_length > 1500:
    print(f"‚ö†Ô∏è Input d√†i ({input_length} tokens), s·∫Ω gi·∫£m max_new_tokens")
    max_new_tokens = min(max_new_tokens, 128)
    
    # C√≥ th·ªÉ truncate n·∫øu c·∫ßn
    if input_length > 2000:
        print(f"‚ö†Ô∏è Input qu√° d√†i, s·∫Ω truncate xu·ªëng 2000 tokens")
        model_inputs = tokenizer(
            [text], 
            return_tensors="pt",
            truncation=True,
            max_length=2000
        )
```

### 2. S·ª≠ d·ª•ng Multiple Fallback Configs

```python
# Th·ª≠ c√°c config t·ª´ t·ªëi ∆∞u nh·∫•t ƒë·∫øn ti·∫øt ki·ªám nh·∫•t
generation_configs = [
    # Config 1: T·ªëi ∆∞u nh·∫•t
    {
        "max_new_tokens": 256,
        "use_cache": False,
        "temperature": 0.7,
    },
    # Config 2: Ti·∫øt ki·ªám h∆°n
    {
        "max_new_tokens": 128,
        "use_cache": False,
        "temperature": 0.7,
    },
    # Config 3: Ti·∫øt ki·ªám nh·∫•t
    {
        "max_new_tokens": 64,
        "use_cache": False,
        "do_sample": False,  # Greedy decoding
    }
]

# Th·ª≠ t·ª´ng config cho ƒë·∫øn khi th√†nh c√¥ng
for i, config in enumerate(generation_configs):
    try:
        torch.cuda.empty_cache()
        output = model.generate(**model_inputs, **config)
        print(f"‚úÖ Th√†nh c√¥ng v·ªõi config {i+1}")
        break
    except torch.cuda.OutOfMemoryError:
        if i < len(generation_configs) - 1:
            print(f"‚ö†Ô∏è Config {i+1} th·∫•t b·∫°i, th·ª≠ config {i+2}...")
        else:
            raise RuntimeError("Kh√¥ng ƒë·ªß b·ªô nh·ªõ ngay c·∫£ v·ªõi config ti·∫øt ki·ªám nh·∫•t!")
```

### 3. Monitor GPU Memory

```python
def check_gpu_memory():
    """Ki·ªÉm tra v√† hi·ªÉn th·ªã b·ªô nh·ªõ GPU"""
    if not torch.cuda.is_available():
        print("‚ùå Kh√¥ng c√≥ GPU")
        return
    
    total = torch.cuda.get_device_properties(0).total_memory / 1e9
    allocated = torch.cuda.memory_allocated(0) / 1e9
    reserved = torch.cuda.memory_reserved(0) / 1e9
    free = total - allocated
    
    print("=" * 50)
    print("TH√îNG TIN GPU MEMORY")
    print("=" * 50)
    print(f"Total:     {total:.2f} GB")
    print(f"Allocated: {allocated:.2f} GB ({allocated/total*100:.1f}%)")
    print(f"Reserved:  {reserved:.2f} GB ({reserved/total*100:.1f}%)")
    print(f"Free:      {free:.2f} GB ({free/total*100:.1f}%)")
    
    # C·∫£nh b√°o
    if free < 1.0:
        print("\n‚ö†Ô∏è C·∫¢NH B√ÅO: GPU memory c√≤n r·∫•t √≠t!")
        print("   C√≥ th·ªÉ g·∫∑p l·ªói OOM khi generate v·ªõi prompt d√†i")
    elif free < 2.0:
        print("\n‚ö†Ô∏è L∆∞u √Ω: GPU memory c√≤n √≠t")
        print("   N√™n gi·∫£i ph√≥ng memory tr∆∞·ªõc khi generate")
    else:
        print("\n‚úÖ GPU memory ƒë·ªß ƒë·ªÉ x·ª≠ l√Ω")
    print("=" * 50)

# S·ª≠ d·ª•ng
check_gpu_memory()
```

### 4. H√†m Generate an to√†n (Safe Generate)

```python
def safe_generate(model, tokenizer, prompt, max_input_length=2000, max_new_tokens=256):
    """
    Generate an to√†n v·ªõi t·ª± ƒë·ªông x·ª≠ l√Ω prompt d√†i
    
    Args:
        model: Model ƒë√£ load
        tokenizer: Tokenizer
        prompt: Prompt ƒë·∫ßu v√†o
        max_input_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa input (tokens)
        max_new_tokens: S·ªë tokens t·ªëi ƒëa ƒë·ªÉ generate
    """
    # 1. Clear cache tr∆∞·ªõc
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # 2. Tokenize v·ªõi truncation
    model_inputs = tokenizer(
        [prompt],
        return_tensors="pt",
        truncation=True,
        max_length=max_input_length,
        padding=False
    ).to(model.device)
    
    input_length = model_inputs.input_ids.shape[1]
    
    # 3. C·∫£nh b√°o n·∫øu input b·ªã truncate
    if input_length >= max_input_length:
        print(f"‚ö†Ô∏è Input qu√° d√†i, ƒë√£ truncate xu·ªëng {max_input_length} tokens")
    
    # 4. T·ª± ƒë·ªông gi·∫£m max_new_tokens n·∫øu input d√†i
    effective_max_tokens = max_new_tokens
    if input_length > 1500:
        effective_max_tokens = min(max_new_tokens, 128)
        print(f"‚ö†Ô∏è Input d√†i ({input_length} tokens), gi·∫£m max_new_tokens xu·ªëng {effective_max_tokens}")
    
    # 5. Generate v·ªõi config an to√†n
    try:
        with torch.no_grad():
            output = model.generate(
                **model_inputs,
                max_new_tokens=effective_max_tokens,
                use_cache=False,  # T·∫Øt cache ƒë·ªÉ ti·∫øt ki·ªám
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # 6. Decode output
        generated_text = tokenizer.decode(output[0][input_length:], skip_special_tokens=True)
        return generated_text
        
    except torch.cuda.OutOfMemoryError as e:
        print(f"‚ùå Out of Memory! Input length: {input_length} tokens")
        print("üí° Gi·∫£i ph√°p:")
        print("   1. Gi·∫£m max_input_length")
        print("   2. Gi·∫£m max_new_tokens")
        print("   3. ƒê√≥ng c√°c process kh√°c ƒëang d√πng GPU")
        print("   4. S·ª≠ d·ª•ng model nh·ªè h∆°n")
        raise

# S·ª≠ d·ª•ng
answer = safe_generate(model, tokenizer, long_prompt)
```

---

## T√≥m t·∫Øt v√† K·∫øt lu·∫≠n

### T·∫°i sao Input d√†i g√¢y OOM?

1. **Attention mechanism** t√≠nh to√°n m·ªëi quan h·ªá gi·ªØa T·∫§T C·∫¢ c√°c c·∫∑p t·ª´
2. **B·ªô nh·ªõ tƒÉng theo b√¨nh ph∆∞∆°ng** s·ªë t·ª´ (O(n¬≤))
3. **GPU memory h·∫°n ch·∫ø** (th∆∞·ªùng 12-16 GB)
4. **Nhi·ªÅu process** c√≥ th·ªÉ chia s·∫ª GPU

### Gi·∫£i ph√°p t·ªët nh·∫•t (Recommended)

**K·∫øt h·ª£p 3 gi·∫£i ph√°p:**
1. ‚úÖ **Truncation**: Gi·ªõi h·∫°n input ‚â§ 2000 tokens
2. ‚úÖ **T·∫Øt KV Cache**: `use_cache=False`
3. ‚úÖ **Gi·∫£m max_new_tokens**: 128-256 thay v√¨ 512

**Code m·∫´u:**
```python
# Tokenize v·ªõi truncation
model_inputs = tokenizer(
    [prompt],
    return_tensors="pt",
    truncation=True,
    max_length=2000,
    padding=False
)

# Generate v·ªõi config ti·∫øt ki·ªám
output = model.generate(
    **model_inputs,
    max_new_tokens=128,
    use_cache=False,
    temperature=0.7,
    do_sample=True,
)
```

### Checklist tr∆∞·ªõc khi Generate

- [ ] Ki·ªÉm tra ƒë·ªô d√†i input (n√™n ‚â§ 2000 tokens)
- [ ] Clear GPU cache: `torch.cuda.empty_cache()`
- [ ] Ki·ªÉm tra GPU memory c√≤n l·∫°i (n√™n ‚â• 2 GB)
- [ ] S·ª≠ d·ª•ng `use_cache=False` n·∫øu input d√†i
- [ ] Gi·∫£m `max_new_tokens` n·∫øu input > 1500 tokens
- [ ] C√≥ fallback configs n·∫øu g·∫∑p OOM

---

## T√†i li·ªáu tham kh·∫£o

- [PyTorch Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)
- [Transformers Generation](https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)
- [Attention Mechanism (Original Paper)](https://arxiv.org/abs/1706.03762)
- [Qwen Model Documentation](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
