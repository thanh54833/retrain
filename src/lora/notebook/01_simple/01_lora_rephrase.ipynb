{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHozlDhQ0ux"
      },
      "source": [
        "# Fine-tune Qwen 2.5 7B v·ªõi Dataset Rephrase s·ª≠ d·ª•ng LoRA\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n fine-tune Qwen 2.5 7B Instruct v·ªõi dataset Rephrase ƒë·ªÉ t·∫°o JSON output t·ª´ query ti·∫øng Vi·ªát.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Base Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **PEFT Method**: LoRA (r=8 ho·∫∑c r=16)\n",
        "- **Dataset**: Rephrase (1,000 samples)\n",
        "- **Task**: Text-to-JSON Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odAt-5R9Q0uy",
        "outputId": "f2029113-2462-4f09-d9e3-486ca00996b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\n",
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: 2.9.0+cu126)\n",
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì peft>=0.6.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì datasets>=2.14.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì accelerate>=0.24.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì bitsandbytes>=0.41.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† scikit-learn ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì tqdm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "\n",
            "==================================================\n",
            "‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\n",
            "B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (CH·∫†Y CELL N√ÄY TR∆Ø·ªöC - ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "def check_and_install(package_name, install_cmd_list=None):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "\n",
        "    try:\n",
        "        # Th·ª≠ import ƒë·ªÉ ki·ªÉm tra\n",
        "        if package_import == \"torch\":\n",
        "            import torch\n",
        "            print(f\"‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "        elif package_import == \"sklearn\":\n",
        "            import sklearn\n",
        "            print(f\"‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {sklearn.__version__})\")\n",
        "        else:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "        if install_cmd_list:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + install_cmd_list)\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "\n",
        "# Ph√°t hi·ªán h·ªá ƒëi·ªÅu h√†nh v√† c√†i ƒë·∫∑t PyTorch ph√π h·ª£p\n",
        "system = platform.system()\n",
        "is_macos = system == \"Darwin\"\n",
        "is_linux = system == \"Linux\"\n",
        "\n",
        "if is_macos:\n",
        "    # macOS: C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\n",
        "    print(\"Ph√°t hi·ªán macOS - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "elif is_linux:\n",
        "    # Linux: Th·ª≠ c√†i ƒë·∫∑t v·ªõi CUDA support\n",
        "    print(\"Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\")\n",
        "    try:\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "    except:\n",
        "        # N·∫øu th·∫•t b·∫°i, c√†i ƒë·∫∑t t·ª´ PyPI\n",
        "        print(\"‚ö† Kh√¥ng th·ªÉ c√†i ƒë·∫∑t PyTorch v·ªõi CUDA, ƒëang c√†i ƒë·∫∑t t·ª´ PyPI...\")\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "else:\n",
        "    # Windows ho·∫∑c h·ªá th·ªëng kh√°c: C√†i ƒë·∫∑t t·ª´ PyPI\n",
        "    print(f\"Ph√°t hi·ªán {system} - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c packages kh√°c\n",
        "packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"scikit-learn\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "print(\"B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl34XBNrQ0uz",
        "outputId": "78551678-864d-4e37-9ed6-af7c278b56d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tMFrOVoQ0u0"
      },
      "source": [
        "## B∆∞·ªõc 1: Load v√† X·ª≠ L√Ω Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dur29o90Q0u0",
        "outputId": "568507a9-8087-449a-a15c-c92a7e7ac1cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1000\n",
            "Sample keys: dict_keys(['status', 'count', 'output', 'prompt_version', 'key', 'vector_distance', 'query'])\n",
            "\n",
            "Example sample:\n",
            "{\n",
            "  \"status\": 0,\n",
            "  \"count\": 12,\n",
            "  \"output\": {\n",
            "    \"keyword\": \"combo d·∫ßu tr√†m Cung ƒê√¨nh\",\n",
            "    \"reasoning\": \"\",\n",
            "    \"is_in_scope\": true,\n",
            "    \"message_banner\": \"Ba m·∫π ƒëang c·∫ßn <b>d·∫ßu tr√†m</b> an to√†n cho b√© v√† gia ƒë√¨nh? Con C∆∞ng lu√¥n s·∫µn s√†ng gi√∫p ba m·∫π chƒÉm s√≥c y√™u th∆∞∆°ng! üåø\",\n",
            "    \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y <b>combo d·∫ßu tr√†m ho√†ng cung</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§ó\"\n",
            "  },\n",
            "  \"prompt_version\": \"14\",\n",
            "  \"key\": \"d7c8973dd60986d1383be0676cb7eb1a63b7b632892456cd0fa656cae9efe7c3\",\n",
            "  \"vector_distance\": 0.37749016284942627,\n",
            "  \"query\": \"combo d·∫ßu tr√†m ho√†ng cung\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset_path = \"/content/llm_trace_spe_langfuse_trace.json\"\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(raw_data)}\")\n",
        "print(f\"Sample keys: {raw_data[0].keys()}\")\n",
        "print(f\"\\nExample sample:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znsvyE4TQ0u0",
        "outputId": "20d8f660-c2b6-4e41-f6c0-0d8863735255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted samples: 1000\n",
            "\n",
            "Example:\n",
            "Prompt: Query: combo d·∫ßu tr√†m ho√†ng cung\n",
            "\n",
            "Output JSON:\n",
            "Output: {\"keyword\": \"\", \"is_in_scope\": false, \"reasoning\": \"\", \"message_banner\": \"\", \"message_no_result\": \"\"}...\n"
          ]
        }
      ],
      "source": [
        "def format_prompt(query: str) -> str:\n",
        "    \"\"\"Format input prompt cho model\"\"\"\n",
        "    return f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "def format_output(data: Dict) -> str:\n",
        "    \"\"\"Format output JSON t·ª´ data\"\"\"\n",
        "    output = {\n",
        "        \"keyword\": data.get(\"keyword\", \"\"),\n",
        "        \"is_in_scope\": data.get(\"is_in_scope\", False),\n",
        "        \"reasoning\": data.get(\"reasoning\", \"\"),\n",
        "        \"message_banner\": data.get(\"message_banner\", \"\"),\n",
        "        \"message_no_result\": data.get(\"message_no_result\", \"\")\n",
        "    }\n",
        "    return json.dumps(output, ensure_ascii=False, indent=None)\n",
        "\n",
        "def prepare_dataset(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Chu·∫©n b·ªã dataset cho training\"\"\"\n",
        "    formatted_data = []\n",
        "\n",
        "    for item in raw_data:\n",
        "        query = item.get(\"query\", \"\")\n",
        "        prompt = format_prompt(query)\n",
        "        output = format_output(item)\n",
        "\n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output,\n",
        "            \"query\": query,\n",
        "            \"is_in_scope\": item.get(\"is_in_scope\", False)\n",
        "        })\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Format dataset\n",
        "formatted_data = prepare_dataset(raw_data)\n",
        "print(f\"Formatted samples: {len(formatted_data)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Prompt: {formatted_data[0]['prompt']}\")\n",
        "print(f\"Output: {formatted_data[0]['output'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWAasq5yQ0u1",
        "outputId": "7f9aab1e-fc54-40af-ce22-909841ae8cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 800 samples\n",
            "Validation: 100 samples\n",
            "Test: 100 samples\n"
          ]
        }
      ],
      "source": [
        "# Chia dataset: 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(\n",
        "    formatted_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in formatted_data]  # Stratified split\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in temp_data]\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "print(f\"Test: {len(test_data)} samples\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrHOrCGFQ0u1"
      },
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrZNzxIyQ0u1",
        "outputId": "87640456-748f-418e-fe40-b816ef08ac41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded\n",
            "Vocab size: 151665\n",
            "Pad token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization (t√πy ch·ªçn, ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n",
        "# B·ªè comment n·∫øu c·∫ßn quantization\n",
        "use_quantization = False  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM\n",
        "\n",
        "if use_quantization:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Tokenizer loaded\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U bitsandbytes"
      ],
      "metadata": {
        "id": "wg8aWjjAVg_P"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "8133df368ee249c9a1aa168ffa199c02",
            "632903d025e044f0884d6d134230d77a",
            "4346ab1ef84040c9a84f275681a8ebd4",
            "0583ead6c0a84476bfdad113fa4c5ddd",
            "d4cd9900101b48569716da623644c42d",
            "987c409d44194d1c9e6ba57dc71c9b76",
            "bd02f97a3d9346878c7f3f1595fc02a3",
            "7d5d81d86dd54b85845dd163c647a1aa",
            "b5a9d27059a34bb5ac3d5e3fac978195",
            "638934c89ee2466ea3e1f41e97f20f76",
            "c8e80209a14e49058424cf8ab921c633"
          ]
        },
        "id": "19zvZBpXQ0u1",
        "outputId": "5948ee2f-84ff-4128-a031-4964ede2fec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8133df368ee249c9a1aa168ffa199c02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
            "Model device: cuda:0\n",
            "Total parameters: 7,615,616,512\n",
            "Trainable parameters: 7,615,616,512\n",
            "Trainable %: 100.00%\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not use_quantization else None\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Ki·ªÉm tra s·ªë tham s·ªë\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0iWrMyQ0u2"
      },
      "source": [
        "## B∆∞·ªõc 3: C·∫•u H√¨nh LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V27WdnoxQ0u2",
        "outputId": "260db590-7fdc-4a66-9340-c8427ac5e3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Config:\n",
            "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'k_proj', 'o_proj', 'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)\n"
          ]
        }
      ],
      "source": [
        "# C·∫•u h√¨nh LoRA - Option 1: Conservative (r=8) - Khuy·∫øn ngh·ªã b·∫Øt ƒë·∫ßu\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                          # Rank\n",
        "    lora_alpha=16,               # Alpha = 2 * r\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.1,            # Dropout ƒë·ªÉ tr√°nh overfitting\n",
        "    bias=\"none\",                  # Kh√¥ng train bias\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Ho·∫∑c Option 2: Balanced (r=16) - Uncomment ƒë·ªÉ d√πng\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False\n",
        "# )\n",
        "\n",
        "print(\"LoRA Config:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfr8lRbTQ0u2",
        "outputId": "07467da1-0867-4397-f926-76f97242c2b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,046,272 || all params: 7,620,662,784 || trainable%: 0.0662\n",
            "\n",
            "‚úì Trainable parameters: 5,046,272\n",
            "‚úì Found 1008 LoRA modules\n"
          ]
        }
      ],
      "source": [
        "# √Åp d·ª•ng LoRA\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Ki·ªÉm tra tham s·ªë c√≥ th·ªÉ train\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ƒê·∫£m b·∫£o model ·ªü training mode\n",
        "model.train()\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n‚úì Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    raise ValueError(\"ERROR: No trainable parameters found! LoRA adapter may not have been applied correctly.\")\n",
        "\n",
        "# Verify LoRA modules are present\n",
        "lora_modules = [name for name, module in model.named_modules() if 'lora' in name.lower()]\n",
        "print(f\"‚úì Found {len(lora_modules)} LoRA modules\")\n",
        "if len(lora_modules) == 0:\n",
        "    raise ValueError(\"ERROR: No LoRA modules found! Check target_modules in LoRA config.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0LIRukrQ0u2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "5c64552e686347b4bee155f1bd307986",
            "a07731eadf0340cfa6cc534d9c9b81ae",
            "85f22b1747344161ad23c443a0b2cdeb",
            "912b40298bd04f95b7f6232c3c84f044",
            "e5fd1b1b0e3b43528e5c1c79297dee81",
            "d5fc935572d3492489fdd489c4ef7983",
            "7b778f2da1f14d64a14dea95e28eb4c3",
            "36328f505d514deba73363e13c83cac2",
            "ba7a849612404855b31bc1e96b696031",
            "14c7f6d8e51540ab97894a48139fa40a",
            "a3e7c9014b02404194810901247a402a",
            "37d409c22f1a42fb92bd3db56c825edf",
            "a5d6f2b47c25428c9919ac8951b38366",
            "66cb3d706be747219cf84c7838abb7aa",
            "282fd103102f4c37994916d24b98df9b",
            "ce41b5a66deb4e88aeaefa4ccbd13453",
            "2d9ab380357549f8a6444d57d0019624",
            "61efd15e77504b46a36727d8951bf58a",
            "8b056b4f3752431d99eae93678949b2e",
            "e4da8ade9d924ed8a550dd44ec328584",
            "c9df2caf5fea4a14bc9d79fa3540c14b",
            "0587b55806514743a42089cec14de1b6"
          ]
        },
        "id": "PN-umZG6Q0u2",
        "outputId": "43da852c-626f-47fa-bbed-f65a6c0a495a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing train dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c64552e686347b4bee155f1bd307986"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing validation dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37d409c22f1a42fb92bd3db56c825edf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tokenized: 800 samples\n",
            "Val tokenized: 100 samples\n",
            "Example tokenized length: 45\n",
            "\n",
            "First example structure:\n",
            "  input_ids type: <class 'list'>\n",
            "  input_ids length: 45\n",
            "  Available keys: ['input_ids', 'attention_mask']\n",
            "  ‚úì Labels will be created by DataCollator during batching (this is correct)\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize prompt v√† output\"\"\"\n",
        "    # Combine prompt v√† output\n",
        "    texts = []\n",
        "    for prompt, output in zip(examples[\"prompt\"], examples[\"output\"]):\n",
        "        text = f\"{prompt} {output}\"\n",
        "        texts.append(text)\n",
        "\n",
        "    # Tokenize with padding=False (DataCollator will handle padding during batching)\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=512,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n ƒë·ªô d√†i output\n",
        "        padding=False,  # Don't pad here - DataCollator will pad during batching\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "\n",
        "    # Don't set labels here - DataCollatorForLanguageModeling will automatically\n",
        "    # create labels from input_ids for causal LM (mlm=False)\n",
        "    # It will pad sequences and set padding tokens in labels to -100\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tokenized)} samples\")\n",
        "print(f\"Example tokenized length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "\n",
        "# Verify the structure of the first example\n",
        "print(f\"\\nFirst example structure:\")\n",
        "print(f\"  input_ids type: {type(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  input_ids length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  Available keys: {list(train_tokenized[0].keys())}\")\n",
        "\n",
        "# Note: labels will be created by DataCollatorForLanguageModeling during batching\n",
        "# This is expected - labels don't exist in the tokenized dataset yet\n",
        "if 'labels' in train_tokenized[0]:\n",
        "    print(f\"  labels type: {type(train_tokenized[0]['labels'])}\")\n",
        "    print(f\"  labels length: {len(train_tokenized[0]['labels'])}\")\n",
        "else:\n",
        "    print(f\"  ‚úì Labels will be created by DataCollator during batching (this is correct)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "h8ZUzZ2zQ0u3"
      },
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "# DataCollatorForLanguageModeling automatically handles padding\n",
        "# It will pad sequences to the same length and set padding tokens in labels to -100\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, kh√¥ng ph·∫£i masked LM\n",
        "    pad_to_multiple_of=8  # T·ªëi ∆∞u cho GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AErd_oy0Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 5: C·∫•u H√¨nh Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d5npuKgQ0u3",
        "outputId": "a62be26d-202c-4624-f3d0-597a02bcf384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured\n",
            "Output directory: ./results/qwen2.5-7b-rephrase-lora\n"
          ]
        }
      ],
      "source": [
        "output_dir = \"./results/qwen2.5-7b-rephrase-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=True,  # Ti·∫øt ki·ªám memory\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,  # Ch·ªâ gi·ªØ 3 checkpoints g·∫ßn nh·∫•t\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbc8a523"
      },
      "source": [
        "# Explicitly upgrade transformers to ensure the latest version is used\n",
        "!pip install -U transformers\n",
        "\n",
        "import transformers\n",
        "print(f\"Updated transformers version: {transformers.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt0Gj0S1Q0u3",
        "outputId": "6fb37121-033d-4b8e-c4e4-2291ad1b5cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying model setup...\n",
            "Trainable parameters: 5,046,272\n",
            "Trainer initialized\n",
            "Training samples: 800\n",
            "Validation samples: 100\n"
          ]
        }
      ],
      "source": [
        "# Verify model is ready for training\n",
        "print(\"Verifying model setup...\")\n",
        "model.train()  # Ensure training mode\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    raise RuntimeError(\n",
        "        \"No trainable parameters found! \"\n",
        "        \"Please check that LoRA adapter was applied correctly. \"\n",
        "        \"Run the LoRA configuration cell again.\"\n",
        "    )\n",
        "\n",
        "# Kh·ªüi t·∫°o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "print(f\"Training samples: {len(train_tokenized)}\")\n",
        "print(f\"Validation samples: {len(val_tokenized)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wwGL95Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 6: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU8pFK_AQ0u3",
        "outputId": "06847a11-1e66-4bf4-e996-4d4562c26c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        }
      ],
      "source": [
        "# B·∫Øt ƒë·∫ßu training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# L∆∞u model cu·ªëi c√πng\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nTraining completed! Model saved to {output_dir}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCl559rzQ0u3"
      },
      "source": [
        "## B∆∞·ªõc 7: Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihSCWpv6Q0u3"
      },
      "outputs": [],
      "source": [
        "# Evaluate tr√™n test set\n",
        "print(\"Tokenizing test dataset...\")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSWw8Pr4Q0u4"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° JSON Validity\n",
        "def evaluate_json_validity(model, tokenizer, test_data, max_samples=50):\n",
        "    \"\"\"ƒê√°nh gi√° t·ª∑ l·ªá JSON h·ª£p l·ªá\"\"\"\n",
        "    model.eval()\n",
        "    valid_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for i, item in enumerate(tqdm(test_data[:max_samples], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract JSON (sau \"Output JSON:\")\n",
        "        if \"Output JSON:\" in generated_text:\n",
        "            json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "        else:\n",
        "            json_text = generated_text\n",
        "\n",
        "        # Ki·ªÉm tra JSON validity\n",
        "        try:\n",
        "            parsed = json.loads(json_text)\n",
        "            valid_count += 1\n",
        "        except:\n",
        "            if total_count < 3:  # Print first few errors\n",
        "                print(f\"\\nError parsing JSON {i}:\")\n",
        "                print(f\"Generated: {json_text[:200]}\")\n",
        "\n",
        "        total_count += 1\n",
        "\n",
        "    validity_rate = valid_count / total_count if total_count > 0 else 0\n",
        "    print(f\"\\nJSON Validity Rate: {validity_rate:.2%} ({valid_count}/{total_count})\")\n",
        "    return validity_rate\n",
        "\n",
        "# Ch·∫°y evaluation\n",
        "validity_rate = evaluate_json_validity(model, tokenizer, test_data, max_samples=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksGeSByQ0u4"
      },
      "source": [
        "## B∆∞·ªõc 8: Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNz8StLmQ0u4"
      },
      "outputs": [],
      "source": [
        "def generate_json_response(model, tokenizer, query: str, max_length=512):\n",
        "    \"\"\"Generate JSON response t·ª´ query\"\"\"\n",
        "    prompt = f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract JSON\n",
        "    if \"Output JSON:\" in generated_text:\n",
        "        json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "    else:\n",
        "        json_text = generated_text\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        result = json.loads(json_text)\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        print(f\"Generated text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Test inference\n",
        "test_queries = [\n",
        "    \"s·ªØa cho b√© 6 th√°ng\",\n",
        "    \"t√£ b·ªâm size M\",\n",
        "    \"ƒë·ªì ch∆°i cho tr·∫ª s∆° sinh\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference:\\n\")\n",
        "for query in test_queries:\n",
        "    result = generate_json_response(model, tokenizer, query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Result: {json.dumps(result, ensure_ascii=False, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4513E8QYQ0u4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "499xUKxQQ0u4"
      },
      "outputs": [],
      "source": [
        "# L∆∞u adapter (ch·ªâ LoRA weights, nh·ªè)\n",
        "adapter_path = f\"{output_dir}/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "print(\"\\nƒê·ªÉ load l·∫°i model sau n√†y:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{model_name}\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhWJGA6_Q0u5"
      },
      "source": [
        "## L∆∞u √ù\n",
        "\n",
        "- ƒêi·ªÅu ch·ªânh `per_device_train_batch_size` v√† `gradient_accumulation_steps` d·ª±a tr√™n GPU memory\n",
        "- C√≥ th·ªÉ th·ª≠ `r=16` n·∫øu `r=8` kh√¥ng ƒë·ªß hi·ªáu su·∫•t\n",
        "- Monitor training loss v√† validation loss ƒë·ªÉ tr√°nh overfitting\n",
        "- S·ª≠ d·ª•ng TensorBoard ƒë·ªÉ theo d√µi: `tensorboard --logdir ./results/qwen2.5-7b-rephrase-lora/logs`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8133df368ee249c9a1aa168ffa199c02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_632903d025e044f0884d6d134230d77a",
              "IPY_MODEL_4346ab1ef84040c9a84f275681a8ebd4",
              "IPY_MODEL_0583ead6c0a84476bfdad113fa4c5ddd"
            ],
            "layout": "IPY_MODEL_d4cd9900101b48569716da623644c42d"
          }
        },
        "632903d025e044f0884d6d134230d77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_987c409d44194d1c9e6ba57dc71c9b76",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bd02f97a3d9346878c7f3f1595fc02a3",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4346ab1ef84040c9a84f275681a8ebd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d5d81d86dd54b85845dd163c647a1aa",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5a9d27059a34bb5ac3d5e3fac978195",
            "value": 4
          }
        },
        "0583ead6c0a84476bfdad113fa4c5ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_638934c89ee2466ea3e1f41e97f20f76",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c8e80209a14e49058424cf8ab921c633",
            "value": "‚Äá4/4‚Äá[01:03&lt;00:00,‚Äá31.77s/it]"
          }
        },
        "d4cd9900101b48569716da623644c42d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987c409d44194d1c9e6ba57dc71c9b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd02f97a3d9346878c7f3f1595fc02a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d5d81d86dd54b85845dd163c647a1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a9d27059a34bb5ac3d5e3fac978195": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "638934c89ee2466ea3e1f41e97f20f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8e80209a14e49058424cf8ab921c633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c64552e686347b4bee155f1bd307986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a07731eadf0340cfa6cc534d9c9b81ae",
              "IPY_MODEL_85f22b1747344161ad23c443a0b2cdeb",
              "IPY_MODEL_912b40298bd04f95b7f6232c3c84f044"
            ],
            "layout": "IPY_MODEL_e5fd1b1b0e3b43528e5c1c79297dee81"
          }
        },
        "a07731eadf0340cfa6cc534d9c9b81ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5fc935572d3492489fdd489c4ef7983",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7b778f2da1f14d64a14dea95e28eb4c3",
            "value": "Map:‚Äá100%"
          }
        },
        "85f22b1747344161ad23c443a0b2cdeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36328f505d514deba73363e13c83cac2",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba7a849612404855b31bc1e96b696031",
            "value": 800
          }
        },
        "912b40298bd04f95b7f6232c3c84f044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14c7f6d8e51540ab97894a48139fa40a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a3e7c9014b02404194810901247a402a",
            "value": "‚Äá800/800‚Äá[00:00&lt;00:00,‚Äá3327.71‚Äáexamples/s]"
          }
        },
        "e5fd1b1b0e3b43528e5c1c79297dee81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5fc935572d3492489fdd489c4ef7983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b778f2da1f14d64a14dea95e28eb4c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36328f505d514deba73363e13c83cac2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba7a849612404855b31bc1e96b696031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "14c7f6d8e51540ab97894a48139fa40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3e7c9014b02404194810901247a402a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d409c22f1a42fb92bd3db56c825edf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5d6f2b47c25428c9919ac8951b38366",
              "IPY_MODEL_66cb3d706be747219cf84c7838abb7aa",
              "IPY_MODEL_282fd103102f4c37994916d24b98df9b"
            ],
            "layout": "IPY_MODEL_ce41b5a66deb4e88aeaefa4ccbd13453"
          }
        },
        "a5d6f2b47c25428c9919ac8951b38366": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d9ab380357549f8a6444d57d0019624",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_61efd15e77504b46a36727d8951bf58a",
            "value": "Map:‚Äá100%"
          }
        },
        "66cb3d706be747219cf84c7838abb7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b056b4f3752431d99eae93678949b2e",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4da8ade9d924ed8a550dd44ec328584",
            "value": 100
          }
        },
        "282fd103102f4c37994916d24b98df9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9df2caf5fea4a14bc9d79fa3540c14b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0587b55806514743a42089cec14de1b6",
            "value": "‚Äá100/100‚Äá[00:00&lt;00:00,‚Äá2506.29‚Äáexamples/s]"
          }
        },
        "ce41b5a66deb4e88aeaefa4ccbd13453": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9ab380357549f8a6444d57d0019624": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61efd15e77504b46a36727d8951bf58a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b056b4f3752431d99eae93678949b2e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4da8ade9d924ed8a550dd44ec328584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c9df2caf5fea4a14bc9d79fa3540c14b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0587b55806514743a42089cec14de1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}