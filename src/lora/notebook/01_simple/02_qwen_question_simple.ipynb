{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Q&A with Qwen 7B\n",
        "\n",
        "Notebook này hướng dẫn load và sử dụng Qwen 7B model để thực hiện Q&A đơn giản.\n",
        "\n",
        "## Thông tin\n",
        "- **Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **Task**: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang kiểm tra và cài đặt dependencies...\n",
            "\n",
            "✓ torch đã được cài đặt\n",
            "✓ transformers>=4.35.0 đã được cài đặt\n",
            "✓ accelerate>=0.24.0 đã được cài đặt\n",
            "\n",
            "==================================================\n",
            "✓ Tất cả dependencies đã sẵn sàng!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (chạy cell này trước nếu chưa cài đặt)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def check_and_install(package_name):\n",
        "    \"\"\"Kiểm tra và cài đặt package nếu chưa có\"\"\"\n",
        "    try:\n",
        "        __import__(package_name.split('>=')[0].split('==')[0].split('<')[0].strip())\n",
        "        print(f\"✓ {package_name} đã được cài đặt\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"⚠ {package_name} chưa được cài đặt. Đang cài đặt...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"✓ {package_name} đã được cài đặt\")\n",
        "        return False\n",
        "\n",
        "print(\"Đang kiểm tra và cài đặt dependencies...\\n\")\n",
        "\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ Tất cả dependencies đã sẵn sàng!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 1: Import thư viện\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Kiểm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Sử dụng CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 2: Load Model và Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e81c425dc141a38cba2154ecf508b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4f7b66ec6714e8c88d57d586921baa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25cb0fbedc7146b5944b8e221569001c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5bb13a018ba4e4ab66b0b2c1327dae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adbb9cae44d14b648b2535eea86fa0f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7655534ee764fb88c63d78c08f268b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b685e349ec214663b27184dc638beab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b435680dde4841a1a3d71086262cd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f76fca6680284a288947115aa4350c50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba33efd04d2b442896fba374461b706a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb39de3cc991405aad673e5bcd3b24fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "680fdc588b5049b98df06092f633a446",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a30c9bd3eaa8401ba4b92d6e1913cfac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
            "✓ Model device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Move to device if not using device_map\n",
        "if not torch.cuda.is_available():\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "print(f\"✓ Model loaded: {model_name}\")\n",
        "print(f\"✓ Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 3: Hàm Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Hàm ask_question đã sẵn sàng!\n"
          ]
        }
      ],
      "source": [
        "def ask_question(question: str, max_new_tokens: int = 512, temperature: float = 0.7):\n",
        "    \"\"\"\n",
        "    Hàm để hỏi đáp với model\n",
        "    \n",
        "    Args:\n",
        "        question: Câu hỏi cần trả lời\n",
        "        max_new_tokens: Số token tối đa để generate\n",
        "        temperature: Nhiệt độ cho sampling (0.0-1.0)\n",
        "    \"\"\"\n",
        "    # Format prompt cho Qwen Instruct model\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"✓ Hàm ask_question đã sẵn sàng!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 4: Test Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TESTING Q&A\n",
            "============================================================\n",
            "\n",
            "[1] Câu hỏi: Xin chào! Bạn là ai?\n",
            "------------------------------------------------------------\n",
            "Trả lời: Xin chào! Tôi là Qwen, một mô hình ngôn ngữ lớn được tạo ra bởi Alibaba Cloud. Tôi có thể giúp bạn với nhiều câu hỏi và nhiệm vụ liên quan đến ngôn ngữ tự nhiên. Bạn có câu hỏi hay chủ đề nào muốn thảo luận không?\n",
            "============================================================\n",
            "\n",
            "[2] Câu hỏi: Python là gì?\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test với một số câu hỏi đơn giản\n",
        "test_questions = [\n",
        "    \"Xin chào! Bạn là ai?\",\n",
        "    \"Python là gì?\",\n",
        "    \"Giải thích ngắn gọn về machine learning\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TESTING Q&A\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n[{i}] Câu hỏi: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    answer = ask_question(question)\n",
        "    print(f\"Trả lời: {answer}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sử dụng tùy chỉnh\n",
        "\n",
        "Bạn có thể sử dụng hàm `ask_question()` với bất kỳ câu hỏi nào:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thử với câu hỏi của bạn\n",
        "my_question = \"Hãy giải thích về transformer architecture trong deep learning\"\n",
        "\n",
        "answer = ask_question(my_question, max_new_tokens=256, temperature=0.7)\n",
        "print(f\"Câu hỏi: {my_question}\")\n",
        "print(f\"\\nTrả lời:\\n{answer}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
