{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Q&A with Qwen 7B\n",
        "\n",
        "Notebook này hướng dẫn load và sử dụng Qwen 7B model để thực hiện Q&A đơn giản.\n",
        "\n",
        "## Thông tin\n",
        "- **Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **Task**: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Đang kiểm tra và cài đặt dependencies...\n",
            "\n",
            "✓ torch đã được cài đặt\n",
            "✓ transformers>=4.35.0 đã được cài đặt\n",
            "✓ accelerate>=0.24.0 đã được cài đặt\n",
            "\n",
            "==================================================\n",
            "✓ Tất cả dependencies đã sẵn sàng!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (chạy cell này trước nếu chưa cài đặt)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def check_and_install(package_name):\n",
        "    \"\"\"Kiểm tra và cài đặt package nếu chưa có\"\"\"\n",
        "    try:\n",
        "        __import__(package_name.split('>=')[0].split('==')[0].split('<')[0].strip())\n",
        "        print(f\"✓ {package_name} đã được cài đặt\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"⚠ {package_name} chưa được cài đặt. Đang cài đặt...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"✓ {package_name} đã được cài đặt\")\n",
        "        return False\n",
        "\n",
        "print(\"Đang kiểm tra và cài đặt dependencies...\\n\")\n",
        "\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"✓ Tất cả dependencies đã sẵn sàng!\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 1: Import thư viện\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Kiểm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Sử dụng CPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 2: Load Model và Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e81c425dc141a38cba2154ecf508b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4f7b66ec6714e8c88d57d586921baa5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25cb0fbedc7146b5944b8e221569001c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5bb13a018ba4e4ab66b0b2c1327dae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "adbb9cae44d14b648b2535eea86fa0f0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7655534ee764fb88c63d78c08f268b9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b685e349ec214663b27184dc638beab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2b435680dde4841a1a3d71086262cd5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f76fca6680284a288947115aa4350c50",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba33efd04d2b442896fba374461b706a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb39de3cc991405aad673e5bcd3b24fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "680fdc588b5049b98df06092f633a446",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a30c9bd3eaa8401ba4b92d6e1913cfac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
            "✓ Model device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Move to device if not using device_map\n",
        "if not torch.cuda.is_available():\n",
        "    model = model.to(\"cpu\")\n",
        "\n",
        "print(f\"✓ Model loaded: {model_name}\")\n",
        "print(f\"✓ Model device: {next(model.parameters()).device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 3: Hàm Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Hàm ask_question đã sẵn sàng!\n"
          ]
        }
      ],
      "source": [
        "def ask_question(question: str, max_new_tokens: int = 512, temperature: float = 0.7):\n",
        "    \"\"\"\n",
        "    Hàm để hỏi đáp với model\n",
        "    \n",
        "    Args:\n",
        "        question: Câu hỏi cần trả lời\n",
        "        max_new_tokens: Số token tối đa để generate\n",
        "        temperature: Nhiệt độ cho sampling (0.0-1.0)\n",
        "    \"\"\"\n",
        "    # Format prompt cho Qwen Instruct model\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"✓ Hàm ask_question đã sẵn sàng!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bước 4: Test Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TESTING Q&A\n",
            "============================================================\n",
            "\n",
            "[1] Câu hỏi: Xin chào! Bạn là ai?\n",
            "------------------------------------------------------------\n",
            "Trả lời: Xin chào! Tôi là Qwen, một mô hình ngôn ngữ lớn được tạo ra bởi Alibaba Cloud. Tôi có thể giúp bạn với nhiều câu hỏi và nhiệm vụ liên quan đến ngôn ngữ tự nhiên. Bạn có câu hỏi hay chủ đề nào muốn thảo luận không?\n",
            "============================================================\n",
            "\n",
            "[2] Câu hỏi: Python là gì?\n",
            "------------------------------------------------------------\n",
            "Trả lời: Python là một ngôn ngữ lập trình phổ biến và linh hoạt được thiết kế để dễ đọc và viết. Dưới đây là một số đặc điểm chính của Python:\n",
            "\n",
            "1. **Dễ học và sử dụng**: Python có cú pháp đơn giản, dễ hiểu, phù hợp cho cả người mới bắt đầu và các nhà phát triển chuyên nghiệp.\n",
            "\n",
            "2. **Đa mục đích**: Có thể dùng để phát triển web, phân tích dữ liệu, trí tuệ nhân tạo, khoa học máy tính, tự động hóa, và nhiều lĩnh vực khác.\n",
            "\n",
            "3. **Thư viện đa dạng**: Python có một thư viện mở rộng lớn, giúp giải quyết hầu hết mọi nhu cầu lập trình.\n",
            "\n",
            "4. **Hỗ trợ lập trình hướng đối tượng**: Cho phép tạo ra lớp và đối tượng, giúp quản lý mã nguồn một cách tổ chức và tái sử dụng.\n",
            "\n",
            "5. **Mô-đun và gói**: Dễ dàng chia nhỏ chương trình thành các mô-đun nhỏ hơn để quản lý.\n",
            "\n",
            "6. **Giải thích và dịch trước**: Python là ngôn ngữ giải thích, nghĩa là mã được thực thi ngay lập tức khi được chạy, hoặc có thể được chuyển đổi thành mã máy bằng công cụ dịch trước.\n",
            "\n",
            "7. **Phát triển cộng đồng mạnh mẽ**: Có một cộng đồng lập trình viên lớn hỗ trợ nhau, cung cấp tài liệu, hướng dẫn và giải quyết vấn đề.\n",
            "\n",
            "8. **Đa nền tảng**: Hoạt động trên nhiều hệ điều hành như Windows, macOS, Linux, v.v.\n",
            "\n",
            "Python được sử dụng rộng rãi trong nhiều ngành công nghiệp và nghiên cứu, từ công nghệ thông tin đến khoa học tự nhiên, y tế, và kinh doanh.\n",
            "============================================================\n",
            "\n",
            "[3] Câu hỏi: Giải thích ngắn gọn về machine learning\n",
            "------------------------------------------------------------\n",
            "Trả lời: Machine Learning là một nhánh của trí tuệ nhân tạo (AI) tập trung vào việc phát triển các thuật toán và mô hình cho máy tính để họ có thể học từ dữ liệu và cải thiện hiệu suất của mình mà không cần được lập trình cụ thể. Cụ thể hơn:\n",
            "\n",
            "1. **Học Đào Tạo**: Máy học tự động cải thiện qua thời gian khi tiếp xúc với dữ liệu mới.\n",
            "2. **Khám Phá Mô Hình**: Máy học tìm ra các quy luật ẩn trong dữ liệu mà con người có thể bỏ lỡ.\n",
            "3. **Áp Dụng Hiệu Quả**: Sử dụng mô hình đã học để dự đoán hoặc quyết định trong các tình huống tương tự.\n",
            "\n",
            "Máy học bao gồm nhiều kỹ thuật như học có giám sát (supervised learning), học không giám sát (unsupervised learning), học tăng cường (reinforcement learning), v.v., tùy thuộc vào mục tiêu và loại dữ liệu bạn đang xử lý.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Test với một số câu hỏi đơn giản\n",
        "test_questions = [\n",
        "    \"Xin chào! Bạn là ai?\",\n",
        "    \"Python là gì?\",\n",
        "    \"Giải thích ngắn gọn về machine learning\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TESTING Q&A\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n[{i}] Câu hỏi: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    answer = ask_question(question)\n",
        "    print(f\"Trả lời: {answer}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sử dụng tùy chỉnh\n",
        "\n",
        "Bạn có thể sử dụng hàm `ask_question()` với bất kỳ câu hỏi nào:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Thử với câu hỏi của bạn\n",
        "my_question = \"Hãy giải thích về transformer architecture trong deep learning\"\n",
        "\n",
        "answer = ask_question(my_question, max_new_tokens=256, temperature=0.7)\n",
        "print(f\"Câu hỏi: {my_question}\")\n",
        "print(f\"\\nTrả lời:\\n{answer}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
