{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simple Q&A with Qwen 7B\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n load v√† s·ª≠ d·ª•ng Qwen 7B model ƒë·ªÉ th·ª±c hi·ªán Q&A ƒë∆°n gi·∫£n.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **Task**: Question Answering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Python version: 3.10.19 | packaged by conda-forge | (main, Oct 22 2025, 22:29:10) [GCC 14.3.0]\n",
            "‚ö† torch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch) (4.15.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx>=2.5.1 (from torch)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.10/site-packages (from torch) (2025.12.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Downloading torch-2.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (899.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m95.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m97.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.5.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.3/170.3 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading markupsafe-3.0.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (20 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22/22\u001b[0m [torch]m21/22\u001b[0m [torch]-cusolver-cu12]2]2]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.1 triton-3.5.1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† transformers>=4.35.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting transformers>=4.35.0\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (3.20.0)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers>=4.35.0)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy>=1.17 (from transformers>=4.35.0)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (6.0.3)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.35.0)\n",
            "  Downloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
            "Collecting requests (from transformers>=4.35.0)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.35.0)\n",
            "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers>=4.35.0)\n",
            "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /venv/main/lib/python3.10/site-packages (from transformers>=4.35.0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (2025.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.35.0) (1.2.0)\n",
            "Collecting charset_normalizer<4,>=2 (from requests->transformers>=4.35.0)\n",
            "  Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->transformers>=4.35.0) (3.11)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->transformers>=4.35.0)\n",
            "  Downloading urllib3-2.6.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->transformers>=4.35.0) (2025.11.12)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m791.7/791.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "Downloading urllib3-2.6.2-py3-none-any.whl (131 kB)\n",
            "Installing collected packages: urllib3, safetensors, regex, numpy, charset_normalizer, requests, huggingface-hub, tokenizers, transformers\n",
            "\u001b[2K  Attempting uninstall: huggingface-hub\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K    Found existing installation: huggingface_hub 1.2.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K    Uninstalling huggingface_hub-1.2.2:m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K      Successfully uninstalled huggingface_hub-1.2.2‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/9\u001b[0m [charset_normalizer]\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9/9\u001b[0m [transformers][0m [transformers]ub]\n",
            "\u001b[1A\u001b[2KSuccessfully installed charset_normalizer-3.4.4 huggingface-hub-0.36.0 numpy-2.2.6 regex-2025.11.3 requests-2.32.5 safetensors-0.7.0 tokenizers-0.22.1 transformers-4.57.3 urllib3-2.6.2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† accelerate>=0.24.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting accelerate>=0.24.0\n",
            "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (25.0)\n",
            "Requirement already satisfied: psutil in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (7.1.3)\n",
            "Requirement already satisfied: pyyaml in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (6.0.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (2.9.1)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /venv/main/lib/python3.10/site-packages (from accelerate>=0.24.0) (0.7.0)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.12.0)\n",
            "Requirement already satisfied: requests in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /venv/main/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.24.0) (1.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /venv/main/lib/python3.10/site-packages (from torch>=2.0.0->accelerate>=0.24.0) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.24.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.24.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2.6.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /venv/main/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.24.0) (2025.11.12)\n",
            "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-1.12.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì accelerate>=0.24.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† bitsandbytes>=0.41.0 ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "Collecting bitsandbytes>=0.41.0\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (2.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.9 in /venv/main/lib/python3.10/site-packages (from bitsandbytes>=0.41.0) (25.0)\n",
            "Requirement already satisfied: filelock in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (2025.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.5.1 in /venv/main/lib/python3.10/site-packages (from torch<3,>=2.3->bitsandbytes>=0.41.0) (3.5.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /venv/main/lib/python3.10/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes>=0.41.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /venv/main/lib/python3.10/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes>=0.41.0) (3.0.3)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.49.0\n",
            "‚úì bitsandbytes>=0.41.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úì BitsAndBytesConfig s·∫µn s√†ng cho quantization\n",
            "\n",
            "==================================================\n",
            "‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (ch·∫°y cell n√†y tr∆∞·ªõc n·∫øu ch∆∞a c√†i ƒë·∫∑t)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def check_and_install(package_name):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "    \n",
        "    # ƒê·∫∑c bi·ªát x·ª≠ l√Ω torch - c√≥ th·ªÉ c√≥ l·ªói compatibility v·ªõi Python 3.12\n",
        "    if package_import == \"torch\":\n",
        "        try:\n",
        "            import torch\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "            return True\n",
        "        except (ImportError, ValueError) as e:\n",
        "            if isinstance(e, ValueError) and (\"METH_CLASS\" in str(e) or \"METH_STATIC\" in str(e)):\n",
        "                print(f\"‚ö† {package_name} c√≥ v·∫•n ƒë·ªÅ compatibility v·ªõi Python {sys.version_info.major}.{sys.version_info.minor}\")\n",
        "                print(\"   ‚ö†Ô∏è L·ªñI: PyTorch kh√¥ng t∆∞∆°ng th√≠ch!\")\n",
        "                print(\"   üí° Gi·∫£i ph√°p: Ch·∫°y cell ti·∫øp theo (cell fix PyTorch) ƒë·ªÉ c√†i ƒë·∫∑t l·∫°i PyTorch\")\n",
        "                print(\"   Ho·∫∑c ch·∫°y l·ªánh sau trong terminal:\")\n",
        "                print(\"   !pip uninstall -y torch torchvision torchaudio\")\n",
        "                print(\"   !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
        "                raise ValueError(\"PyTorch kh√¥ng t∆∞∆°ng th√≠ch. Vui l√≤ng ch·∫°y cell fix PyTorch tr∆∞·ªõc.\")\n",
        "            else:\n",
        "                print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "                # C√†i ƒë·∫∑t PyTorch t∆∞∆°ng th√≠ch v·ªõi Python 3.12\n",
        "                if sys.version_info >= (3, 12):\n",
        "                    subprocess.check_call([\n",
        "                        sys.executable, \"-m\", \"pip\", \"install\", \n",
        "                        \"--upgrade\", \"torch\", \"torchvision\", \"torchaudio\", \n",
        "                        \"--index-url\", \"https://download.pytorch.org/whl/cu118\"\n",
        "                    ])\n",
        "                else:\n",
        "                    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name])\n",
        "                print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "                return False\n",
        "    elif package_import == \"bitsandbytes\":\n",
        "        # ƒê·∫∑c bi·ªát x·ª≠ l√Ω bitsandbytes - c·∫ßn upgrade ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch\n",
        "        try:\n",
        "            import bitsandbytes as bnb\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {bnb.__version__})\")\n",
        "            # Ki·ªÉm tra xem c√≥ th·ªÉ import ƒë∆∞·ª£c kh√¥ng\n",
        "            from transformers import BitsAndBytesConfig\n",
        "            print(\"   ‚úì BitsAndBytesConfig c√≥ th·ªÉ s·ª≠ d·ª•ng\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", package_name])\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return False\n",
        "    else:\n",
        "        # X·ª≠ l√Ω c√°c package kh√°c\n",
        "        try:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return True\n",
        "        except ImportError:\n",
        "            print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "            return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "packages = [\n",
        "    \"torch\",\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",  # C·∫ßn cho quantization\n",
        "]\n",
        "\n",
        "try:\n",
        "    for package in packages:\n",
        "        check_and_install(package)\n",
        "    \n",
        "    # Verify bitsandbytes after installation\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        print(\"\\n‚úì BitsAndBytesConfig s·∫µn s√†ng cho quantization\")\n",
        "    except ImportError:\n",
        "        print(\"\\n‚ö† BitsAndBytesConfig ch∆∞a s·∫µn s√†ng - c√≥ th·ªÉ c·∫ßn restart kernel\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "    print(\"=\"*50)\n",
        "except ValueError as e:\n",
        "    if \"PyTorch\" in str(e):\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"‚ùå PyTorch kh√¥ng t∆∞∆°ng th√≠ch!\")\n",
        "        print(\"=\"*50)\n",
        "        print(\"\\nVui l√≤ng:\")\n",
        "        print(\"1. Ch·∫°y cell ti·∫øp theo (cell 'Fix PyTorch compatibility')\")\n",
        "        print(\"2. Sau ƒë√≥ restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "    else:\n",
        "        raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gi·∫£i th√≠ch: T·∫°i sao c·∫ßn import torch v√† transformers?\n",
        "\n",
        "### 1. **`torch` (PyTorch)** - Framework Deep Learning\n",
        "- **M·ª•c ƒë√≠ch**: PyTorch l√† th∆∞ vi·ªán c·ªët l√µi ƒë·ªÉ l√†m vi·ªác v·ªõi m√¥ h√¨nh deep learning\n",
        "- **Ch·ª©c nƒÉng trong code n√†y**:\n",
        "  - Ki·ªÉm tra GPU/CUDA: `torch.cuda.is_available()` - xem c√≥ GPU kh√¥ng\n",
        "  - Qu·∫£n l√Ω b·ªô nh·ªõ GPU: `torch.cuda.empty_cache()` - gi·∫£i ph√≥ng b·ªô nh·ªõ GPU\n",
        "  - X·ª≠ l√Ω tensor: T·∫•t c·∫£ t√≠nh to√°n c·ªßa model ƒë·ªÅu d√πng tensor c·ªßa PyTorch\n",
        "  - ƒê·ªãnh d·∫°ng d·ªØ li·ªáu: `torch.float16` - d√πng ƒë·ªÉ gi·∫£m b·ªô nh·ªõ\n",
        "\n",
        "### 2. **`transformers`** - Th∆∞ vi·ªán Hugging Face\n",
        "- **`AutoModelForCausalLM`**: Load m√¥ h√¨nh ng√¥n ng·ªØ (nh∆∞ Qwen 7B)\n",
        "  - T·ª± ƒë·ªông nh·∫≠n di·ªán ki·ªÉu model v√† load ƒë√∫ng c√°ch\n",
        "  - H·ªó tr·ª£ quantization, device mapping\n",
        "  \n",
        "- **`AutoTokenizer`**: X·ª≠ l√Ω text (tokenize/encode/decode)\n",
        "  - Chuy·ªÉn text th√†nh s·ªë (tokens) ƒë·ªÉ model hi·ªÉu\n",
        "  - Chuy·ªÉn tokens th√†nh text khi model tr·∫£ l·ªùi\n",
        "  \n",
        "- **`BitsAndBytesConfig`**: C·∫•u h√¨nh quantization (4-bit, 8-bit)\n",
        "  - Gi·∫£m b·ªô nh·ªõ t·ª´ ~14GB xu·ªëng ~4-5GB\n",
        "  - Cho ph√©p ch·∫°y model l·ªõn tr√™n GPU nh·ªè h∆°n\n",
        "\n",
        "### 3. **T·∫°i sao c·∫ßn ki·ªÉm tra GPU?**\n",
        "- Model Qwen 7B r·∫•t l·ªõn (~14GB), c·∫ßn GPU ƒë·ªÉ ch·∫°y nhanh\n",
        "- N·∫øu kh√¥ng c√≥ GPU, model s·∫Ω ch·∫°y tr√™n CPU (r·∫•t ch·∫≠m)\n",
        "- Code n√†y ki·ªÉm tra GPU ƒë·ªÉ:\n",
        "  - Quy·∫øt ƒë·ªãnh c√≥ d√πng quantization kh√¥ng\n",
        "  - Qu·∫£n l√Ω b·ªô nh·ªõ GPU hi·ªáu qu·∫£\n",
        "  - Tr√°nh l·ªói \"Out of Memory\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 1: Import th∆∞ vi·ªán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 4060 Ti\n",
            "CUDA memory: 16.86 GB\n",
            "CUDA memory free: 16.86 GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "    print(f\"CUDA memory free: {torch.cuda.get_device_properties(0).total_memory / 1e9 - torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"S·ª≠ d·ª•ng CPU\")\n",
        "\n",
        "# H√†m ƒë·ªÉ clear GPU memory\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"X√≥a cache GPU ƒë·ªÉ gi·∫£i ph√≥ng b·ªô nh·ªõ\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.synchronize()\n",
        "        print(\"‚úì ƒê√£ x√≥a GPU cache\")\n",
        "\n",
        "# clear_gpu_memory()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
            "Loading tokenizer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd6d8ea4ccae4fd3a69cefa1319f6747",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f91c85dadadf4b9293817c014d6cbf26",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d83723e8d045454fa4ac132e2291c446",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5d87b2848d5460fb2ff8d0d70e57cb8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99160bd120c94c078d65ceedd40509c6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b05412073b394e4e98acd7c5d13cdfc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2749d00934c4bd093879104e66a2177",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05267c321dfa42b1a9c33ffd181632f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40dbe6bbf7bf4197b27952dd4c23b5d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5c5ec604efea4e55bed1a3f731f19150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a32d7cfb57b24271bf9d7cc2b1b228da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fd18b085775541adbb445bd4f85a251a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1d85389186b64324b4c19beb098179ce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
            "‚úì Model devices: 0\n",
            "‚úì GPU memory allocated: 5.56 GB\n",
            "‚úì GPU memory reserved: 6.37 GB\n",
            "\n",
            "============================================================\n",
            "KI·ªÇM TRA LOAD MODEL\n",
            "============================================================\n",
            "‚úì Tokenizer v√† Model ƒë√£ s·∫µn s√†ng!\n",
            "  - Tokenizer: Qwen2TokenizerFast\n",
            "  - Model: Qwen2ForCausalLM\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\n",
        "# S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ gi·∫£m memory usage t·ª´ ~14GB xu·ªëng ~4-5GB\n",
        "use_quantization = True  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM (>16GB)\n",
        "\n",
        "# Ki·ªÉm tra bitsandbytes tr∆∞·ªõc khi s·ª≠ d·ª•ng quantization\n",
        "bitsandbytes_available = False\n",
        "if use_quantization and torch.cuda.is_available():\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        import bitsandbytes as bnb\n",
        "        bitsandbytes_available = True\n",
        "        print(\"‚ö†Ô∏è S·ª≠ d·ª•ng 4-bit quantization ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ\")\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_quant_type=\"nf4\"\n",
        "        )\n",
        "    except (ImportError, ModuleNotFoundError) as e:\n",
        "        print(\"‚ö†Ô∏è bitsandbytes kh√¥ng kh·∫£ d·ª•ng - t·∫Øt quantization\")\n",
        "        print(f\"   L·ªói: {e}\")\n",
        "        print(\"   üí° Ch·∫°y cell 'Fix bitsandbytes installation' ƒë·ªÉ c√†i ƒë·∫∑t l·∫°i\")\n",
        "        print(\"   Ho·∫∑c ƒë·∫∑t use_quantization = False ƒë·ªÉ ti·∫øp t·ª•c kh√¥ng d√πng quantization\")\n",
        "        use_quantization = False\n",
        "        quantization_config = None\n",
        "        bitsandbytes_available = False\n",
        "else:\n",
        "    quantization_config = None\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng quantization (kh√¥ng c√≥ CUDA)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Kh√¥ng s·ª≠ d·ª•ng quantization (c·∫ßn nhi·ªÅu VRAM)\")\n",
        "\n",
        "print(\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Loading model...\")\n",
        "try:\n",
        "    # Configure device_map based on quantization\n",
        "    if use_quantization and torch.cuda.is_available():\n",
        "        # For quantization, use a specific GPU device to avoid CPU offloading\n",
        "        # This ensures all quantized modules stay on GPU\n",
        "        device_map_config = 0  # Use first GPU\n",
        "    elif torch.cuda.is_available():\n",
        "        # For non-quantized models, auto device_map is fine\n",
        "        device_map_config = \"auto\"\n",
        "    else:\n",
        "        # No CUDA available\n",
        "        device_map_config = None\n",
        "    \n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if (torch.cuda.is_available() and not use_quantization) else None,\n",
        "        device_map=device_map_config,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    # Move to device if not using device_map\n",
        "    if not torch.cuda.is_available() and device_map_config is None:\n",
        "        model = model.to(\"cpu\")\n",
        "    \n",
        "    print(f\"‚úì Model loaded: {model_name}\")\n",
        "    \n",
        "    # Device detection\n",
        "    if hasattr(model, 'hf_device_map') and model.hf_device_map:\n",
        "        devices = set(model.hf_device_map.values())\n",
        "        # Convert device IDs to strings (they can be int or str)\n",
        "        device_strs = [str(d) for d in devices]\n",
        "        print(f\"‚úì Model devices: {', '.join(sorted(device_strs))}\")\n",
        "    else:\n",
        "        print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
        "    \n",
        "    # Ki·ªÉm tra memory usage\n",
        "    if torch.cuda.is_available():\n",
        "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "        print(f\"‚úì GPU memory allocated: {allocated:.2f} GB\")\n",
        "        print(f\"‚úì GPU memory reserved: {reserved:.2f} GB\")\n",
        "        \n",
        "except ImportError as e:\n",
        "    if \"bitsandbytes\" in str(e).lower():\n",
        "        print(\"‚ùå Error: bitsandbytes ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t ho·∫∑c c√≥ v·∫•n ƒë·ªÅ\")\n",
        "        print(\"\\nüí° Gi·∫£i ph√°p:\")\n",
        "        print(\"1. Ch·∫°y cell 'Fix bitsandbytes installation' (cell tr∆∞·ªõc ƒë√≥)\")\n",
        "        print(\"2. Restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "        print(\"\\nHo·∫∑c t·∫Øt quantization b·∫±ng c√°ch:\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell\")\n",
        "        raise\n",
        "    else:\n",
        "        raise\n",
        "except ValueError as e:\n",
        "    error_msg = str(e)\n",
        "    if \"CPU or the disk\" in error_msg or \"dispatched on the CPU\" in error_msg:\n",
        "        print(f\"‚ùå Error: {error_msg}\")\n",
        "        print(\"\\nüí° V·∫•n ƒë·ªÅ: Model kh√¥ng ƒë·ªß GPU RAM ƒë·ªÉ load v·ªõi quantization.\")\n",
        "        print(\"\\nüîß Gi·∫£i ph√°p:\")\n",
        "        print(\"1. T·∫Øt quantization (khuy·∫øn ngh·ªã):\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell (c·∫ßn ~14GB VRAM)\")\n",
        "        print(\"\\n2. Ho·∫∑c s·ª≠ d·ª•ng model nh·ªè h∆°n:\")\n",
        "        print(\"   - Thay ƒë·ªïi model_name th√†nh 'Qwen/Qwen2.5-3B-Instruct'\")\n",
        "        print(\"   - Ho·∫∑c 'Qwen/Qwen2.5-1.5B-Instruct'\")\n",
        "        print(\"\\n3. Ho·∫∑c gi·∫£i ph√≥ng GPU memory:\")\n",
        "        print(\"   - Restart kernel\")\n",
        "        print(\"   - ƒê√≥ng c√°c ·ª©ng d·ª•ng kh√°c ƒëang d√πng GPU\")\n",
        "    else:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "    raise\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading model: {e}\")\n",
        "    if use_quantization and \"bitsandbytes\" in str(e).lower():\n",
        "        print(\"\\nüí° Gi·∫£i ph√°p:\")\n",
        "        print(\"1. Ch·∫°y cell 'Fix bitsandbytes installation'\")\n",
        "        print(\"2. Restart kernel\")\n",
        "        print(\"3. Ch·∫°y l·∫°i cell n√†y\")\n",
        "        print(\"\\nHo·∫∑c t·∫Øt quantization:\")\n",
        "        print(\"   - ƒê·∫∑t use_quantization = False ·ªü ƒë·∫ßu cell n√†y\")\n",
        "        print(\"   - Ch·∫°y l·∫°i cell\")\n",
        "    raise\n",
        "\n",
        "# Ki·ªÉm tra xem tokenizer v√† model ƒë√£ ƒë∆∞·ª£c load ch∆∞a\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"KI·ªÇM TRA LOAD MODEL\")\n",
        "print(\"=\"*60)\n",
        "try:\n",
        "    if 'tokenizer' not in globals() or tokenizer is None:\n",
        "        raise NameError(\"tokenizer ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    if 'model' not in globals() or model is None:\n",
        "        raise NameError(\"model ch∆∞a ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n",
        "    print(\"‚úì Tokenizer v√† Model ƒë√£ s·∫µn s√†ng!\")\n",
        "    print(f\"  - Tokenizer: {type(tokenizer).__name__}\")\n",
        "    print(f\"  - Model: {type(model).__name__}\")\n",
        "    print(\"=\"*60)\n",
        "except NameError as e:\n",
        "    print(\"‚ùå L·ªói:\", str(e))\n",
        "    print(\"\\nüí° Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh load model ·ªü tr√™n!\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 3: H√†m Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì H√†m ask_question ƒë√£ s·∫µn s√†ng!\n"
          ]
        }
      ],
      "source": [
        "def ask_question(question: str, max_new_tokens: int = 512, temperature: float = 0.7):\n",
        "    \"\"\"\n",
        "    H√†m ƒë·ªÉ h·ªèi ƒë√°p v·ªõi model (t·ªëi ∆∞u memory)\n",
        "    \n",
        "    Args:\n",
        "        question: C√¢u h·ªèi c·∫ßn tr·∫£ l·ªùi\n",
        "        max_new_tokens: S·ªë token t·ªëi ƒëa ƒë·ªÉ generate (gi·∫£m n·∫øu h·∫øt memory)\n",
        "        temperature: Nhi·ªát ƒë·ªô cho sampling (0.0-1.0)\n",
        "    \"\"\"\n",
        "    # Clear cache tr∆∞·ªõc khi generate\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Format prompt cho Qwen Instruct model\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": question}\n",
        "    ]\n",
        "    \n",
        "    # Apply chat template\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "    \n",
        "    # Generate v·ªõi c√°c t√πy ch·ªçn t·ªëi ∆∞u memory\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                # T·ªëi ∆∞u memory\n",
        "                use_cache=True,  # S·ª≠ d·ª•ng KV cache ƒë·ªÉ tƒÉng t·ªëc\n",
        "            )\n",
        "    except torch.cuda.OutOfMemoryError as e:\n",
        "        print(f\"‚ö†Ô∏è Out of Memory! ƒêang th·ª≠ v·ªõi max_new_tokens nh·ªè h∆°n...\")\n",
        "        # Clear cache v√† th·ª≠ l·∫°i v·ªõi max_new_tokens nh·ªè h∆°n\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        # Th·ª≠ l·∫°i v·ªõi max_new_tokens gi·∫£m ƒëi m·ªôt n·ª≠a\n",
        "        new_max_tokens = max_new_tokens // 2\n",
        "        if new_max_tokens < 50:\n",
        "            raise RuntimeError(\"Kh√¥ng ƒë·ªß memory ngay c·∫£ v·ªõi max_new_tokens nh·ªè nh·∫•t. H√£y s·ª≠ d·ª•ng quantization ho·∫∑c gi·∫£m k√≠ch th∆∞·ªõc model.\")\n",
        "        \n",
        "        print(f\"Th·ª≠ l·∫°i v·ªõi max_new_tokens={new_max_tokens}\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **model_inputs,\n",
        "                max_new_tokens=new_max_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True if temperature > 0 else False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                use_cache=True,\n",
        "            )\n",
        "    \n",
        "    # Decode response\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Clear cache sau khi generate\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    return response\n",
        "\n",
        "print(\"‚úì H√†m ask_question ƒë√£ s·∫µn s√†ng!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B∆∞·ªõc 4: Test Q&A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TESTING Q&A\n",
            "============================================================\n",
            "\n",
            "[1] C√¢u h·ªèi: Xin ch√†o! B·∫°n l√† ai?\n",
            "------------------------------------------------------------\n",
            "Tr·∫£ l·ªùi: Xin ch√†o! T√¥i l√† Qwen, m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë∆∞·ª£c t·∫°o ra b·ªüi Alibaba Cloud. T√¥i c√≥ th·ªÉ gi√∫p tr·∫£ l·ªùi c√¢u h·ªèi, vi·∫øt vƒÉn b·∫£n v√† tr√≤ chuy·ªán v·ªõi ng∆∞·ªùi d√πng nh∆∞ b·∫°n. T√¥i r·∫•t vui ƒë∆∞·ª£c g·∫∑p b·∫°n!\n",
            "============================================================\n",
            "\n",
            "[2] C√¢u h·ªèi: Python l√† g√¨?\n",
            "------------------------------------------------------------\n",
            "Tr·∫£ l·ªùi: Python l√† m·ªôt ng√¥n ng·ªØ l·∫≠p tr√¨nh ph·ªï bi·∫øn v√† linh ho·∫°t ƒë∆∞·ª£c ph√°t tri·ªÉn v√†o nƒÉm 1991 b·ªüi Guido van Rossum. Python n·ªïi ti·∫øng v√¨ c√∫ ph√°p ƒë∆°n gi·∫£n v√† d·ªÖ ƒë·ªçc, khi·∫øn n√≥ tr·ªü th√†nh m·ªôt l·ª±a ch·ªçn t·ªët cho c·∫£ ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu l·∫´n chuy√™n gia l·∫≠p tr√¨nh.\n",
            "\n",
            "M·ªôt s·ªë ƒë·∫∑c ƒëi·ªÉm ch√≠nh c·ªßa Python bao g·ªìm:\n",
            "\n",
            "1. C√∫ ph√°p r√µ r√†ng v√† d·ªÖ ƒë·ªçc.\n",
            "2. Linh ho·∫°t v·ªõi nhi·ªÅu ki·ªÉu d·ªØ li·ªáu t√≠ch h·ª£p.\n",
            "3. C√≥ kh·∫£ nƒÉng vi·∫øt m√£ ng·∫Øn g·ªçn v√† hi·ªáu qu·∫£.\n",
            "4. C√≥ th·ªÉ ch·∫°y tr√™n nhi·ªÅu n·ªÅn t·∫£ng (Windows, Linux, macOS).\n",
            "5. C√≥ m·ªôt th∆∞ vi·ªán l·ªõn v·ªõi nhi·ªÅu c√¥ng c·ª• v√† module.\n",
            "6. ƒê∆∞·ª£c s·ª≠ d·ª•ng trong nhi·ªÅu lƒ©nh v·ª±c nh∆∞ ph√°t tri·ªÉn web, ph√¢n t√≠ch d·ªØ li·ªáu, tr√≠ tu·ªá nh√¢n t·∫°o, khoa h·ªçc m√°y t√≠nh, v.v.\n",
            "\n",
            "Python ƒë∆∞·ª£c bi·∫øt ƒë·∫øn r·ªông r√£i trong c·ªông ƒë·ªìng l·∫≠p tr√¨nh vi√™n v√† ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c d·ª± √°n t·ª´ nh·ªè ƒë·∫øn l·ªõn.\n",
            "============================================================\n",
            "\n",
            "[3] C√¢u h·ªèi: Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ machine learning\n",
            "------------------------------------------------------------\n",
            "Tr·∫£ l·ªùi: Machine Learning (H·ªçc M√°y) l√† m·ªôt nh√°nh c·ªßa tr√≠ tu·ªá nh√¢n t·∫°o t·∫≠p trung v√†o vi·ªác ph√°t tri·ªÉn c√°c thu·∫≠t to√°n v√† m√¥ h√¨nh th·ªëng k√™ ƒë·ªÉ cho m√°y t√≠nh c√≥ kh·∫£ nƒÉng t·ª± ƒë·ªông h·ªçc h·ªèi v√† c·∫£i thi·ªán t·ª´ kinh nghi·ªám m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c r√µ r√†ng ch·ªâ ƒë·∫°o. C·ª• th·ªÉ h∆°n:\n",
            "\n",
            "1. **H·ªçc t·ª´ d·ªØ li·ªáu**: Machine Learning s·ª≠ d·ª•ng d·ªØ li·ªáu ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh, gi√∫p m√°y t√≠nh nh·∫≠n bi·∫øt m·∫´u, xu h∆∞·ªõng v√† m·ªëi quan h·ªá trong d·ªØ li·ªáu.\n",
            "\n",
            "2. **K√≠ch th∆∞·ªõc kh√¥ng ph·ª• thu·ªôc v√†o con ng∆∞·ªùi**: M√¥ h√¨nh h·ªçc m√°y c√≥ th·ªÉ x·ª≠ l√Ω l∆∞·ª£ng l·ªõn d·ªØ li·ªáu m√† kh√¥ng c·∫ßn con ng∆∞·ªùi can thi·ªáp.\n",
            "\n",
            "3. **ƒê·ªÅ xu·∫•t v√† d·ª± ƒëo√°n**: Sau khi ƒë∆∞·ª£c hu·∫•n luy·ªán, m√¥ h√¨nh c√≥ th·ªÉ ƒë·ªÅ xu·∫•t h√†nh ƒë·ªông ho·∫∑c d·ª± ƒëo√°n k·∫øt qu·∫£ t∆∞∆°ng lai d·ª±a tr√™n d·ªØ li·ªáu ƒë√£ h·ªçc.\n",
            "\n",
            "4. **T·ª± c·∫£i thi·ªán**: C√°c m√¥ h√¨nh h·ªçc m√°y th∆∞·ªùng ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ c·∫£i thi·ªán ch√≠nh n√≥ theo th·ªùi gian, d·ª±a tr√™n th√™m d·ªØ li·ªáu m·ªõi.\n",
            "\n",
            "5. **Lo·∫°i thu·∫≠t to√°n**:\n",
            "   - **H·ªçc c√≥ gi√°m s√°t**: C·∫ßn nh√£n (ƒë√°p √°n ƒë√∫ng) cho m·ªói m·∫´u ƒë·ªÉ hu·∫•n luy·ªán.\n",
            "   - **H·ªçc kh√¥ng gi√°m s√°t**: Kh√¥ng c·∫ßn nh√£n, t·∫≠p trung v√†o kh√°m ph√° m·∫´u trong d·ªØ li·ªáu.\n",
            "   - **H·ªçc tƒÉng c∆∞·ªùng**: T·∫≠p trung v√†o vi·ªác ƒë∆∞a ra quy·∫øt ƒë·ªãnh th√¥ng qua t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng.\n",
            "\n",
            "Machine Learning ƒë√£ ƒë∆∞·ª£c ·ª©ng d·ª•ng r·ªông r√£i trong nhi·ªÅu lƒ©nh v·ª±c nh∆∞ y t·∫ø, t√†i ch√≠nh, marketing, an ninh m·∫°ng, v√† nhi·ªÅu h∆°n n·ªØa.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Test v·ªõi m·ªôt s·ªë c√¢u h·ªèi ƒë∆°n gi·∫£n\n",
        "test_questions = [\n",
        "    \"Xin ch√†o! B·∫°n l√† ai?\",\n",
        "    \"Python l√† g√¨?\",\n",
        "    \"Gi·∫£i th√≠ch ng·∫Øn g·ªçn v·ªÅ machine learning\",\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TESTING Q&A\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n[{i}] C√¢u h·ªèi: {question}\")\n",
        "    print(\"-\" * 60)\n",
        "    answer = ask_question(question)\n",
        "    print(f\"Tr·∫£ l·ªùi: {answer}\")\n",
        "    print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è Out of Memory! ƒêang th·ª≠ v·ªõi max_new_tokens nh·ªè h∆°n...\n",
            "Th·ª≠ l·∫°i v·ªõi max_new_tokens=256\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 30.81 MiB is free. Process 3141301 has 6.09 GiB memory in use. Process 3144297 has 9.58 GiB memory in use. Of the allocated memory 5.71 GiB is allocated by PyTorch, and 231.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 32\u001b[0m, in \u001b[0;36mask_question\u001b[0;34m(question, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m         generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# T·ªëi ∆∞u memory\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# S·ª≠ d·ª•ng KV cache ƒë·ªÉ tƒÉng t·ªëc\u001b[39;49;00m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:449\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03mExample:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m\"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:384\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:249\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 249\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:46\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 30.81 MiB is free. Process 3141301 has 6.09 GiB memory in use. Process 3144297 has 9.58 GiB memory in use. Of the allocated memory 5.61 GiB is allocated by PyTorch, and 337.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 177\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Th·ª≠ v·ªõi c√¢u h·ªèi c·ªßa b·∫°n\u001b[39;00m\n\u001b[1;32m      2\u001b[0m my_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m# H∆Ø·ªöNG D·∫™N H·ªÜ TH·ªêNG\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mB·∫°n l√† chuy√™n gia t·ªëi ∆∞u t·ª´ kh√≥a t√¨m ki·∫øm s·∫£n ph·∫©m cho Concung(m·ªôt chu·ªói si√™u th·ªã m·∫π v√† b√© l·ªõn nh·∫•t t·∫°i Vi·ªát Nam). Nhi·ªám v·ª• c·ªßa b·∫°n l√† bi·∫øn ƒë·ªïi c√¢u h·ªèi c·ªßa kh√°ch h√†ng th√†nh t·ª´ kh√≥a t√¨m ki·∫øm hi·ªáu qu·∫£ nh·∫•t v√† t·∫°o message banner th√¢n thi·ªán.\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124m                CONCUNG_SUGGESTIONS: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxe √¥ t√¥ ƒë·ªìng h·ªì, v√≤ng tay b√© g√°i ƒë·ªìng h·ªì, v·ªõ ƒë·ªìng Natri b√© trai\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 177\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC√¢u h·ªèi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmy_question\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTr·∫£ l·ªùi:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[7], line 55\u001b[0m, in \u001b[0;36mask_question\u001b[0;34m(question, max_new_tokens, temperature)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTh·ª≠ l·∫°i v·ªõi max_new_tokens=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_max_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 55\u001b[0m         generated_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_max_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m            \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Decode response\u001b[39;00m\n\u001b[1;32m     66\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     67\u001b[0m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs\u001b[38;5;241m.\u001b[39minput_ids, generated_ids)\n\u001b[1;32m     68\u001b[0m ]\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/utils/_contextlib.py:120\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:449\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    431\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    432\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/generic.py:1072\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1072\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:384\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 384\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    397\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    398\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    399\u001b[0m )\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:249\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    248\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 249\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/transformers/models/qwen2/modeling_qwen2.py:46\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 46\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 166.00 MiB. GPU 0 has a total capacity of 15.70 GiB of which 30.81 MiB is free. Process 3141301 has 6.09 GiB memory in use. Process 3144297 has 9.58 GiB memory in use. Of the allocated memory 5.71 GiB is allocated by PyTorch, and 231.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Th·ª≠ v·ªõi c√¢u h·ªèi c·ªßa b·∫°n\n",
        "my_question = \"\"\"\n",
        "# H∆Ø·ªöNG D·∫™N H·ªÜ TH·ªêNG\n",
        "B·∫°n l√† chuy√™n gia t·ªëi ∆∞u t·ª´ kh√≥a t√¨m ki·∫øm s·∫£n ph·∫©m cho Concung(m·ªôt chu·ªói si√™u th·ªã m·∫π v√† b√© l·ªõn nh·∫•t t·∫°i Vi·ªát Nam). Nhi·ªám v·ª• c·ªßa b·∫°n l√† bi·∫øn ƒë·ªïi c√¢u h·ªèi c·ªßa kh√°ch h√†ng th√†nh t·ª´ kh√≥a t√¨m ki·∫øm hi·ªáu qu·∫£ nh·∫•t v√† t·∫°o message banner th√¢n thi·ªán.\n",
        "\n",
        "# NHI·ªÜM V·ª§ CH√çNH\n",
        "1.  **T·ªëi ∆∞u h√≥a T·ª´ kh√≥a t√¨m ki·∫øm (`keyword`):** Chuy·ªÉn ƒë·ªïi `USER_QUERY` th√†nh t·ª´ kh√≥a t√¨m ki·∫øm ng·∫Øn g·ªçn, ch√≠nh x√°c v√† hi·ªáu qu·∫£ nh·∫•t cho h·ªá th·ªëng c·ªßa Concung.\n",
        "2.  **T·∫°o Tin nh·∫Øn Banner (`message_banner`):** So·∫°n m·ªôt tin nh·∫Øn banner th√¢n thi·ªán ƒë·ªÉ x√°c nh·∫≠n ƒë√£ hi·ªÉu y√™u c·∫ßu c·ªßa kh√°ch h√†ng.\n",
        "3.  **T·∫°o Tin nh·∫Øn D·ª± ph√≤ng (`message_no_result`):** So·∫°n m·ªôt tin nh·∫Øn d·ª± ph√≤ng ƒë∆∞·ª£c c√° nh√¢n h√≥a trong tr∆∞·ªùng h·ª£p kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m, gi√∫p gi·ªØ ch√¢n kh√°ch h√†ng v√† g·ª£i √Ω c·∫£i thi·ªán t√¨m ki·∫øm.\n",
        "\n",
        "# QUY T·∫ÆC PH·∫†M VI X·ª¨ L√ù (QUAN TR·ªåNG)\n",
        "## is_in_scope = true khi QUERY th·ªèa m√£n T·∫§T C·∫¢ ƒëi·ªÅu ki·ªán sau:\n",
        "1.  **Li√™n quan ƒë·∫øn h·ªá sinh th√°i Con C∆∞ng**: QUERY ƒë·ªÅ c·∫≠p ƒë·∫øn m·ªôt trong c√°c ch·ªß ƒë·ªÅ sau:\n",
        "    *   **T√¨m ki·∫øm s·∫£n ph·∫©m**: T√¨m ki·∫øm, mua s·∫Øm, ho·∫∑c so s√°nh s·∫£n ph·∫©m (v√≠ d·ª•: \"s·ªØa cho b√©\", \"t√£ Huggies\", \"kem ch·ªëng hƒÉm\").\n",
        "    *   **Th√¥ng tin & H∆∞·ªõng d·∫´n**: C√°c b√†i vi·∫øt, c·∫©m nang, ho·∫∑c m·∫πo li√™n quan ƒë·∫øn m·∫π, b√©, v√† gia ƒë√¨nh (v√≠ d·ª•: \"c√°ch chƒÉm s√≥c tr·∫ª s∆° sinh\", \"th·ª±c ƒë∆°n ƒÉn d·∫∑m\", \"b√≠ quy·∫øt gi√∫p m·∫π b·∫ßu ƒë√≥n T·∫øt an to√†n\", \"b·ªánh vi·ªán ph·ª• s·∫£n uy t√≠n\").\n",
        "    *   **Th√¥ng tin v·ªÅ Con C∆∞ng**: C√°c ch√≠nh s√°ch, ch∆∞∆°ng tr√¨nh khuy·∫øn m√£i, ho·∫∑c th√¥ng tin v·ªÅ c√¥ng ty (v√≠ d·ª•: \"ch√≠nh s√°ch ƒë·ªïi tr·∫£\", \"khuy·∫øn m√£i th√°ng 10\", \"giao h√†ng\", \"giao nh·∫≠n\").\n",
        "2.  **H·ª£p ph√°p**: N·ªôi dung kh√¥ng vi ph·∫°m ph√°p lu·∫≠t Vi·ªát Nam.\n",
        "3.  **Kh√¥ng ƒë·ªôc h·∫°i**: Kh√¥ng ch·ª©a n·ªôi dung ph·∫£n c·∫£m, th√¥ t·ª•c, c√≥ h·∫°i.\n",
        "\n",
        "## is_in_scope = false khi QUERY thu·ªôc m·ªôt trong c√°c tr∆∞·ªùng h·ª£p sau:\n",
        "- **Kh√¥ng li√™n quan ƒë·∫øn h·ªá sinh th√°i Con C∆∞ng**: C√°c c√¢u h·ªèi ki·∫øn th·ª©c t·ªïng qu√°t kh√¥ng li√™n quan ƒë·∫øn m·∫π, b√© v√† gia ƒë√¨nh (v√≠ d·ª•: \"th·ªß ƒë√¥ c·ªßa Ph√°p l√† g√¨\", \"k·∫øt qu·∫£ x·ªï s·ªë\"), tin t·ª©c kh√¥ng li√™n quan.\n",
        "- **N·ªôi dung ƒë·ªôc h·∫°i**: Th√¥ t·ª•c, ph·∫£n c·∫£m, k√≠ch ƒë·ªông b·∫°o l·ª±c, ph√¢n bi·ªát ch·ªßng t·ªôc.\n",
        "- **Ch√≠nh tr·ªã**: Quan ƒëi·ªÉm ch√≠nh tr·ªã, ·ª©ng vi√™n, ch√≠nh s√°ch, b·∫ßu c·ª≠.\n",
        "- **B·∫•t h·ª£p ph√°p**: Vi ph·∫°m ph√°p lu·∫≠t, ma t√∫y, v≈© kh√≠, h√†ng c·∫•m.\n",
        "- **So s√°nh ƒë·ªëi th·ªß**: So s√°nh tr·ª±c ti·∫øp Con C∆∞ng v·ªõi c√°c n·ªÅn t·∫£ng kh√°c (v√≠ d·ª•: \"gi√° ·ªü Shopee r·∫ª h∆°n ph·∫£i kh√¥ng\").\n",
        "- **T·∫•n c√¥ng danh ti·∫øng**: Ph·ªâ b√°ng c√° nh√¢n, doanh nghi·ªáp.\n",
        "\n",
        "# QUY T·∫ÆC T·ªêI ∆ØU T·ª™ KH√ìA (CH·ªà KHI is_in_scope = true)\n",
        "## 1. Ph√¢n T√≠ch √ù ƒê·ªãnh\n",
        "- X√°c ƒë·ªãnh nhu c·∫ßu s·∫£n ph·∫©m c·ªët l√µi t·ª´ c√¢u h·ªèi kh√°ch h√†ng\n",
        "- ∆Øu ti√™n c√°c nh√≥m s·∫£n ph·∫©m ch·ªß l·ª±c: m·∫π v√† b√©, th·ªùi trang, ƒëi·ªán t·ª≠, gia d·ª•ng, th·ª±c ph·∫©m\n",
        "\n",
        "## 2. X√¢y D·ª±ng T·ª´ Kh√≥a\n",
        "- **X·ª≠ L√Ω Truy V·∫•n V·ªÅ Th∆∞∆°ng Hi·ªáu Con C∆∞ng**: N·∫øu `USER_QUERY` ch·ªâ l√† \"Con C∆∞ng\" (ho·∫∑c c√°c bi·∫øn th·ªÉ vi·∫øt kh√¥ng d·∫•u/sai ch√≠nh t·∫£ nh∆∞ \"concung\", \"con cung\"), `keyword` ph·∫£i l√† \"Gi·ªõi thi·ªáu Con C∆∞ng\".\n",
        "- S·ª≠ d·ª•ng t√™n s·∫£n ph·∫©m c·ª• th·ªÉ, kh√¥ng d√πng m√¥ t·∫£ chung chung\n",
        "- Bao g·ªìm thu·ªôc t√≠nh li√™n quan: ƒë·ªô tu·ªïi, th∆∞∆°ng hi·ªáu, lo·∫°i, ch·ª©c nƒÉng, k√≠ch th∆∞·ªõc\n",
        "- ∆Øu ti√™n t√™n s·∫£n ph·∫©m h∆°n m√¥ t·∫£ v·∫•n ƒë·ªÅ\n",
        "- T·ªëi ƒëa 20 t·ª´\n",
        "- S·ª≠ d·ª•ng thu·∫≠t ng·ªØ ti·∫øng Vi·ªát ph√π h·ª£p v·ªõi th·ªã tr∆∞·ªùng Vi·ªát Nam\n",
        "  - **S·ª¨ D·ª§NG CONCUNG_SUGGESTIONS (CH·ªà 2 TR∆Ø·ªú·ªúNG H·ª¢P)**:\n",
        "  - **QUY T·∫ÆC NGHI√äM NG·∫∂T (√ÅP D·ª§NG CHO C·∫¢ 2 TR∆Ø·ªú·ªúNG H·ª¢P)**:\n",
        "      - **QUAN TR·ªåNG**: N·∫æU `USER_QUERY` C√ì T·ª™ 2 T·ª™ TR·ªû L√äN V√Ä ƒê√É R√ï NGHƒ®A, B·ªé QUA `CONCUNG_SUGGESTIONS` (tr·ª´ tr∆∞·ªùng h·ª£p 2: s·ª≠a l·ªói ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu).\n",
        "      - KH√îNG copy tr·ª±c ti·∫øp c√°c c·ª•m t·ª´ t·ª´ CONCUNG_SUGGESTIONS v√†o keyword\n",
        "      - CH·ªà d√πng CONCUNG_SUGGESTIONS ƒë·ªÉ hi·ªÉu ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m, sau ƒë√≥ t·∫°o keyword\n",
        "      - GI·ªÆ NGUY√äN t·ª´ kh√≥a g·ªëc t·ª´ QUERY (bao g·ªìm t√™n th∆∞∆°ng hi·ªáu n·∫øu c√≥)\n",
        "      - B·ªî SUNG th√¥ng tin lo·∫°i s·∫£n ph·∫©m chung chung v√†o keyword (v√≠ d·ª•: \"s·ªØa\", \"t√£\", \"qu·∫ßn √°o\"), kh√¥ng ph·∫£i c√°c bi·∫øn th·ªÉ chi ti·∫øt.\n",
        "      - KH√îNG s·ª≠ d·ª•ng CONCUNG_SUGGESTIONS cho message_banner\n",
        "  - **Tr∆∞·ªùng h·ª£p 1 ‚Äî Keyword ng·∫Øn/kh√¥ng r√µ**: Khi QUERY ch·ªâ c√≥ 1 t·ª´ v√† ho√†n to√†n m∆° h·ªì, kh√¥ng ƒëo√°n ƒë∆∞·ª£c lo·∫°i s·∫£n ph·∫©m.\n",
        "    - **ƒêI·ªÄU KI·ªÜN S·ª¨ D·ª§NG**: CH·ªà khi QUERY = 1 t·ª´ V√Ä kh√¥ng th·ªÉ x√°c ƒë·ªãnh ƒë∆∞·ª£c ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m\n",
        "    - **M·ª§C ƒê√çCH DUY NH·∫§T**: D√πng CONCUNG_SUGGESTIONS ƒë·ªÉ x√°c ƒë·ªãnh ng√†nh h√†ng/lo·∫°i s·∫£n ph·∫©m trong c·ª≠a h√†ng Concung\n",
        "    - **V√≠ d·ª•**:\n",
        "      - QUERY \"nan\" + CONCUNG_SUGGESTIONS \"s·ªØa nan, nan 2, nan 3\" ‚Üí Ph√¢n t√≠ch: ng√†nh h√†ng = s·ªØa ‚Üí Keyword \"s·ªØa nan cho b√©\"\n",
        "      - QUERY \"milo\" + CONCUNG_SUGGESTIONS \"milo 3in1, milo √∫c\" ‚Üí Ph√¢n t√≠ch: ng√†nh h√†ng = s·ªØa ‚Üí Keyword \"s·ªØa milo\"\n",
        "      - QUERY \"animo\" ‚Üí Ph√¢n t√≠ch: animo l√† th∆∞∆°ng hi·ªáu ƒë·ªôc quy·ªÅn c·ªßa Concung ‚Üí Keyword \"s·∫£n ph·∫©m c·ªßa Animo\"\n",
        "  - **Tr∆∞·ªùng h·ª£p 2 ‚Äî S·ª≠a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu (ƒë·ªô tin c·∫≠y cao)**: Khi QUERY c√≥ d·∫•u hi·ªáu sai ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu V√Ä t·∫•t c·∫£ m·ª•c trong CONCUNG_SUGGESTIONS c√πng m·ªôt th∆∞∆°ng hi·ªáu (v√≠ d·ª• ƒë·ªÅu l√† Ensure).\n",
        "    - **M·ª§C ƒê√çCH DUY NH·∫§T**: Chu·∫©n h√≥a t√™n th∆∞∆°ng hi·ªáu trong keyword theo th∆∞∆°ng hi·ªáu chung t·ª´ CONCUNG_SUGGESTIONS.\n",
        "    - **V√≠ d·ª•**:\n",
        "      - QUERY \"en sua\" + CONCUNG_SUGGESTIONS \"s·ªØa Ensure, s·ªØa Ensure Gold, S·ªØa Ensure 237ml\" ‚Üí Keyword \"s·ªØa Ensure dinh d∆∞·ª°ng cho ng∆∞·ªùi l·ªõn\"\n",
        "  - **L∆ØU √ù**: N·∫øu QUERY ‚â• 2 t·ª´ ho·∫∑c ƒë√£ r√µ r√†ng v·ªÅ s·∫£n ph·∫©m ‚Üí B·ªé QUA CONCUNG_SUGGESTIONS, ngo·∫°i tr·ª´ Tr∆∞·ªùng h·ª£p 2 (s·ª≠a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu ‚Äì ƒë·ªô tin c·∫≠y cao)\n",
        "\n",
        "## 3. X·ª≠ L√Ω ƒê·∫∑c Bi·ªát Cho S·∫£n Ph·∫©m M·∫π v√† B√©\n",
        "- Chuy·ªÉn ƒë·ªïi v·∫•n ƒë·ªÅ s·ª©c kh·ªèe th√†nh nhu c·∫ßu s·∫£n ph·∫©m c·ª• th·ªÉ\n",
        "- X√°c ƒë·ªãnh ƒë·ªô tu·ªïi/c√¢n n·∫∑ng ƒë·ªÉ g·ª£i √Ω size ph√π h·ª£p\n",
        "- T·∫≠p trung v√†o gi·∫£i ph√°p s·∫£n ph·∫©m thay v√¨ m√¥ t·∫£ tri·ªáu ch·ª©ng\n",
        "\n",
        "# QUY T·∫ÆC T·∫†O MESSAGE BANNER (CH·ªà KHI is_in_scope = true)\n",
        "## C·∫•u tr√∫c message banner\n",
        "1. **Th√¥ng ƒëi·ªáp quan t√¢m**: C√¢u n√≥i th·∫•u hi·ªÉu, h·ªó tr·ª£ b·∫±ng ti·∫øng Vi·ªát (t·ªëi ƒëa 100 k√Ω t·ª±)\n",
        "2. **G·ª£i √Ω s·∫£n ph·∫©m**: K·∫øt n·ªëi t√¨m ki·∫øm v·ªõi nhu c·∫ßu/c·∫£m x√∫c c·ªßa kh√°ch h√†ng. Vi·ªác c√≥ nh·∫Øc t√™n th∆∞∆°ng hi·ªáu hay kh√¥ng tu√¢n theo c√°c quy t·∫Øc ∆∞u ti√™n d∆∞·ªõi ƒë√¢y.\n",
        "\n",
        "   - **QUY T·∫ÆC ∆ØU TI√äN (KI·ªÇM TRA ƒê·∫¶U TI√äN)**:\n",
        "       - **M·ª•c ƒë√≠ch**: X√°c nh·∫≠n ch√≠nh x√°c th∆∞∆°ng hi·ªáu khi h·ªá th·ªëng c√≥ ƒë·ªô tin c·∫≠y cao.\n",
        "       - **ƒêI·ªÄU KI·ªÜN**: Ph·∫£i th·ªèa m√£n **C·∫¢ HAI** ƒëi·ªÅu ki·ªán sau: 1) `USER_QUERY` c√≥ ch·ª©a t√™n th∆∞∆°ng hi·ªáu (ho·∫∑c bi·∫øn th·ªÉ sai ch√≠nh t·∫£) V√Ä 2) T·∫•t c·∫£ `CONCUNG_SUGGESTIONS` c√πng tr·ªè v·ªÅ m·ªôt th∆∞∆°ng hi·ªáu duy nh·∫•t ƒë·ªÉ x√°c nh·∫≠n.\n",
        "\n",
        "   - **QUY T·∫ÆC CHUNG (√ÅP D·ª§NG KHI QUY T·∫ÆC ∆ØU TI√äN KH√îNG TH·ªéA M√ÉN)**:\n",
        "       - **M·ª•c ƒë√≠ch**: M√¥ t·∫£ s·∫£n ph·∫©m chung khi kh√¥ng ch·∫Øc ch·∫Øn v·ªÅ th∆∞∆°ng hi·ªáu.\n",
        "\n",
        "   - **S·ª≠ d·ª•ng th·∫ª <b>**: D√πng ƒë·ªÉ nh·∫•n m·∫°nh t·ª´ kh√≥a c√≥ gi√° tr·ªã t√¨m ki·∫øm cao v√† li√™n quan tr·ª±c ti·∫øp ƒë·∫øn 'keyword' ƒë∆∞·ª£c t·∫°o ra.\n",
        "3. **H√†nh ƒë·ªông b·∫Øt bu·ªôc (S√ÅNG T·∫†O BANNER M·ªöI)**:\n",
        "   - **N·∫øu `QUY T·∫ÆC ∆ØU TI√äN` ƒë∆∞·ª£c th·ªèa m√£n**: `message_banner` **B·∫ÆT BU·ªòC** ph·∫£i nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a. D·ª±a v√†o `USER_QUERY`, h√£y **s√°ng t·∫°o m·ªôt th√¥ng ƒëi·ªáp t·ª± nhi√™n v√† th√¢n thi·ªán c√≥ l·ªìng gh√©p t√™n th∆∞∆°ng hi·ªáu** thay v√¨ d√πng m·∫´u c√≥ s·∫µn. T√™n th∆∞∆°ng hi·ªáu ph·∫£i ƒë∆∞·ª£c ƒë·∫∑t trong th·∫ª `<b>`.\n",
        "   - **N·∫øu `QUY T·∫ÆC ∆ØU TI√äN` kh√¥ng ƒë∆∞·ª£c th·ªèa m√£n**: √Åp d·ª•ng `QUY T·∫ÆC CHUNG`. **Ph√¢n t√≠ch s√¢u `USER_QUERY` ƒë·ªÉ n·∫Øm b·∫Øt √Ω ƒë·ªãnh v√† c·∫£m x√∫c c·ªßa kh√°ch h√†ng. T·ª´ ƒë√≥, h√£y t·ª± s√°ng t·∫°o m·ªôt `message_banner` ho√†n to√†n m·ªõi, ph√π h·ª£p v√† ƒë·ªôc ƒë√°o.** Banner c·∫ßn t√≠ch h·ª£p **m√¥ t·∫£ lo·∫°i s·∫£n ph·∫©m ho·∫∑c t√≠nh nƒÉng chung** ƒë∆∞·ª£c r√∫t ra t·ª´ `keyword` m·ªôt c√°ch t·ª± nhi√™n. TUY·ªÜT ƒê·ªêI KH√îNG nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu c·ª• th·ªÉ v√† kh√¥ng l·∫∑p l·∫°i c√°c m·∫´u c√¢u nh√†m ch√°n.\n",
        "4. **Emoji**: M·ªôt emoji ph√π h·ª£p (ch·ªß ƒë·ªÅ gia ƒë√¨nh/s·∫£n ph·∫©m t∆∞∆°ng ·ª©ng)\n",
        "5. **Gi·ªçng ƒëi·ªáu**: Quan t√¢m, h·ªó tr·ª£, ƒë√°ng tin c·∫≠y, r·∫•t Vi·ªát Nam\n",
        "6. **C√°ch x∆∞ng h√¥**: S·ª≠ d·ª•ng \"Con C∆∞ng\" ƒë·ªÉ x∆∞ng v√† \"Ba m·∫π\" ƒë·ªÉ g·ªçi kh√°ch h√†ng\n",
        "\n",
        "## H∆∞·ªõng d·∫´n t·∫°o banner\n",
        "- S·ª≠ d·ª•ng ng√¥n ng·ªØ ti·∫øng Vi·ªát nh·∫π nh√†ng, quan t√¢m v·ªõi c√°ch x∆∞ng h√¥ \"Con C∆∞ng\" v√† \"Ba m·∫π\"\n",
        "- Th·ªÉ hi·ªán s·ª± hi·ªÉu bi·∫øt v·ªÅ nhu c·∫ßu kh√°ch h√†ng\n",
        "- X√¢y d·ª±ng ni·ªÅm tin v√† s·ª± t·ª± tin\n",
        "- Tr√°nh ƒë∆∞a ra l·ªùi khuy√™n y t·∫ø ho·∫∑c g√¢y √°p l·ª±c\n",
        "- K·∫øt n·ªëi v·ªõi t√¨nh c·∫£m v√† s·ª± chƒÉm s√≥c\n",
        "- Ph√π h·ª£p v·ªõi vƒÉn h√≥a Vi·ªát Nam\n",
        "- **B·∫ÆT BU·ªòC**: H·∫°n ch·∫ø t·ªëi ƒëa vi·ªác nh·∫Øc t√™n th∆∞∆°ng hi·ªáu c·ª• th·ªÉ, tr·ª´ tr∆∞·ªùng h·ª£p ngo·∫°i l·ªá ƒë√£ n√™u tr√™n.\n",
        "\n",
        "# QUY T·∫ÆC T·∫†O MESSAGE KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M (message_no_result)\n",
        "\n",
        "## C·∫•u Tr√∫c B·∫Øt Bu·ªôc (2 Ph·∫ßn)\n",
        "1.  **B√†y T·ªè ƒê·ªìng C·∫£m**: Lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng m·ªôt c√¢u th·ªÉ hi·ªán s·ª± ti·∫øc nu·ªëi nh·∫π nh√†ng. H√£y ƒëa d·∫°ng c√°ch m·ªü ƒë·∫ßu.\n",
        "    - *V√≠ d·ª•*: \"√îi, Con C∆∞ng ch∆∞a t√¨m th·∫•y...\", \"Con C∆∞ng t√¨m k·ªπ m√† ch∆∞a ra...\", \"Hmm, Con C∆∞ng t√¨m ch∆∞a ra r·ªìi...\", \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m ƒë∆∞·ª£c s·∫£n ph·∫©m...\"\n",
        "2.  **H∆∞·ªõng D·∫´n Gi·∫£i Ph√°p Th√¥ng Minh**: Ph√¢n t√≠ch `USER_QUERY` ƒë·ªÉ ƒë∆∞a ra **ph∆∞∆°ng ph√°p** c·∫£i thi·ªán t√¨m ki·∫øm, **kh√¥ng g·ª£i √Ω t·ª´ kh√≥a c·ª• th·ªÉ**.\n",
        "    - **Query c√≥ th·ªÉ sai ch√≠nh t·∫£**: G·ª£i √Ω \"Ba m·∫π ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m nh√©!\".\n",
        "    - **Query qu√° d√†i/chi ti·∫øt**: G·ª£i √Ω \"Ba m·∫π th·ª≠ d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n xem sao ·∫°!\".\n",
        "    - **Query qu√° ng·∫Øn/chung chung**: G·ª£i √Ω \"Ba m·∫π th·ª≠ b·ªï sung th√™m th√¥ng tin chi ti·∫øt (v√≠ d·ª•: th∆∞∆°ng hi·ªáu, ƒë·ªô tu·ªïi) nh√©!\".\n",
        "    - **C√°c tr∆∞·ªùng h·ª£p kh√°c**: G·ª£i √Ω \"Ba m·∫π th·ª≠ t√¨m v·ªõi m·ªôt t·ª´ kh√≥a kh√°c nh√©!\".\n",
        "\n",
        "## Nguy√™n T·∫Øc V√†ng\n",
        "- **Gi·ªçng ƒëi·ªáu & X∆∞ng H√¥**: Lu√¥n nh·∫π nh√†ng, ƒë·ªìng c·∫£m, h·ªó tr·ª£. X∆∞ng h√¥ \"Con C∆∞ng\" v√† \"Ba m·∫π\".\n",
        "- **ƒê·ªô d√†i**: Ng·∫Øn g·ªçn (15-25 t·ª´, t·ªëi ƒëa 150 k√Ω t·ª±), vƒÉn phong ƒëa d·∫°ng, kh√¥ng l·∫∑p l·∫°i.\n",
        "- **Th√°i ƒë·ªô**: Tr√°nh kh·∫≥ng ƒë·ªãnh \"kh√¥ng c√≥\", thay b·∫±ng \"ch∆∞a t√¨m th·∫•y\". Tuy·ªát ƒë·ªëi kh√¥ng c√≥ h√†m √Ω ng∆∞·ªùi d√πng t√¨m sai.\n",
        "- **T√™n Th∆∞∆°ng Hi·ªáu**: **TUY·ªÜT ƒê·ªêI KH√îNG** nh·∫Øc ƒë·∫øn t√™n th∆∞∆°ng hi·ªáu, tr·ª´ khi l·∫∑p l·∫°i ch√≠nh x√°c `USER_QUERY` ƒë·ªÉ x√°c nh·∫≠n.\n",
        "- **Nh·∫•n m·∫°nh**: Khi l·∫∑p l·∫°i `USER_QUERY`, ƒë·∫∑t trong th·∫ª `<b>`. V√≠ d·ª•: \"Con C∆∞ng t√¨m m√£i ch∆∞a ra <b>t√£ gi·∫•y Huggies</b>.\"\n",
        "- **K·∫øt th√∫c**: Lu√¥n c√≥ m·ªôt emoji th√¢n thi·ªán ·ªü cu·ªëi.\n",
        "\n",
        "# V√ç D·ª§ MINH H·ªåA\n",
        "\n",
        "## C√°c tr∆∞·ªùng h·ª£p is_in_scope = true\n",
        "**ƒê·∫ßu v√†o**: \"b√© 6 th√°ng b·ªã t√°o b√≥n n√™n u·ªëng s·ªØa g√¨\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"s·ªØa c√¥ng th·ª©c d·ªÖ ti√™u h√≥a cho b√© 6 th√°ng ch·ªëng t√°o b√≥n\", \"message_banner\": \"Con kh√≥c ƒë√™m nhi·ªÅu ph·∫£i kh√¥ng ba m·∫π? Con C∆∞ng t√¨m ƒë∆∞·ª£c nh·ªØng <b>s·ªØa c√¥ng th·ª©c</b> t·ªët nh·∫•t cho b√©! üçº\", \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y s·∫£n ph·∫©m cho b√©. Ba m·∫π th·ª≠ t√¨m b·∫±ng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§î\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"t√£ gi·∫•y huggies\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"t√£ gi·∫•y huggies cho b√©\", \"message_banner\": \"Ba m·∫π ƒëang t√¨m <b>t√£ Huggies</b> ph·∫£i kh√¥ng? Con C∆∞ng c√≥ nhi·ªÅu l·ª±a ch·ªçn ti·ªán l·ª£i v√† an to√†n cho b√© y√™u nh√©! üë∂\", \"message_no_result\": \"Con C∆∞ng t√¨m m√£i ch∆∞a ra <b>t√£ gi·∫•y Huggies</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c t√¨m v·ªõi t·ª´ kh√≥a chung h∆°n nh√©! ü•∫\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"bao cao su\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": true, \"keyword\": \"bao cao su an to√†n\", \"message_banner\": \"S·∫£n ph·∫©m ch√≠nh h√£ng, <b>ch·∫•t l∆∞·ª£ng ƒë·∫£m b·∫£o</b> - Con C∆∞ng tin ba m·∫π s·∫Ω an t√¢m cho s·ª©c kh·ªèe gia ƒë√¨nh üíï\", \"message_no_result\": \"Hmm, Con C∆∞ng ch∆∞a t√¨m ra <b>bao cao su</b>. Ba m·∫π th·ª≠ b·ªï sung th√™m th√¥ng tin chi ti·∫øt v·ªÅ s·∫£n ph·∫©m nh∆∞ th∆∞∆°ng hi·ªáu mong mu·ªën nh√©! ü§ó\"}`\n",
        "\n",
        "## C√°c tr∆∞·ªùng h·ª£p is_in_scope = false\n",
        "**ƒê·∫ßu v√†o**: \"th·ªùi ti·∫øt h√¥m nay nh∆∞ th·∫ø n√†o\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"R·∫•t ti·∫øc, Con C∆∞ng kh√¥ng c√≥ th√¥ng tin v·ªÅ th·ªùi ti·∫øt. Ba m·∫π th·ª≠ t√¨m ki·∫øm m·ªôt s·∫£n ph·∫©m c·ª• th·ªÉ nh√©! ü§î\", \"reasoning\": \"Truy v·∫•n h·ªèi v·ªÅ th·ªùi ti·∫øt, kh√¥ng li√™n quan ƒë·∫øn s·∫£n ph·∫©m.\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"Shopee c√≥ t·ªët h∆°n Concung kh√¥ng\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"Con C∆∞ng r·∫•t ti·∫øc kh√¥ng th·ªÉ so s√°nh v·ªõi n·ªÅn t·∫£ng kh√°c. Ba m·∫π c·∫ßn t√¨m s·∫£n ph·∫©m g√¨ ·∫°? ü§ó\", \"reasoning\": \"Truy v·∫•n so s√°nh v·ªõi ƒë·ªëi th·ªß c·∫°nh tranh.\"}`\n",
        "\n",
        "**ƒê·∫ßu v√†o**: \"c√°ch n·∫•u ch√°o cho b√©\"\n",
        "**ƒê·∫ßu ra**: `{\"is_in_scope\": false, \"keyword\": \"\", \"message_banner\": \"\", \"message_no_result\": \"Con C∆∞ng ch∆∞a c√≥ c√¥ng th·ª©c n·∫•u ƒÉn ·∫°. Ba m·∫π c√≥ mu·ªën t√¨m c√°c lo·∫°i ch√°o ƒÉn li·ªÅn dinh d∆∞·ª°ng cho b√© kh√¥ng? üç≤\", \"reasoning\": \"Truy v·∫•n h·ªèi v·ªÅ ki·∫øn th·ª©c chung, kh√¥ng ph·∫£i t√¨m ki·∫øm s·∫£n ph·∫©m.\"}`\n",
        "\n",
        "\n",
        "# Y√äU C·∫¶U ƒê·∫¶U RA\n",
        "Ch·ªâ tr·∫£ v·ªÅ ƒë·ªãnh d·∫°ng JSON h·ª£p l·ªá:\n",
        "```json\n",
        "{\n",
        "  \"is_in_scope\": boolean,\n",
        "  \"keyword\": \"t·ª´ kh√≥a t√¨m ki·∫øm ƒë∆∞·ª£c t·ªëi ∆∞u (ch·ªâ khi is_in_scope = true)\",\n",
        "  \"message_banner\": \"banner th√¢n thi·ªán b·∫±ng ti·∫øng Vi·ªát (lu√¥n t·∫°o khi is_in_scope = true)\",\n",
        "  \"message_no_result\": \"th√¥ng b√°o khi kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m (lu√¥n ƒë∆∞·ª£c t·∫°o)\",\n",
        "  \"reasoning\":\"l√Ω do is_in_scope = false 10 - 20 t·ª´\"\n",
        "}\n",
        "```\n",
        "\n",
        "# C√ÅC B∆Ø·ªöC X·ª¨ L√ù\n",
        "1. **Ki·ªÉm tra ph·∫°m vi**: ƒê√°nh gi√° QUERY theo quy t·∫Øc `is_in_scope`.\n",
        "2. **T·∫°o message_no_result**: Lu√¥n t·∫°o `message_no_result` theo c√°c quy t·∫Øc ·ªü tr√™n, b·∫•t k·ªÉ `is_in_scope` l√† g√¨.\n",
        "3. **N·∫øu is_in_scope = false**: Tr·∫£ v·ªÅ JSON v·ªõi `is_in_scope=false`, `keyword=\"\"`, `message_banner=\"\"`, `message_no_result` ƒë√£ t·∫°o, v√† `reasoning` gi·∫£i th√≠ch ng·∫Øn g·ªçn.\n",
        "4. **N·∫øu is_in_scope = true**:\n",
        "   - **Ph√¢n t√≠ch QUERY**: X√°c ƒë·ªãnh ƒë·ªô chi ti·∫øt v√† r√µ r√†ng c·ªßa query.\n",
        "   - **X·ª≠ l√Ω CONCUNG_SUGGESTIONS** (khi c·∫ßn thi·∫øt):\n",
        "     - Ph√¢n t√≠ch suggestions ƒë·ªÉ t√¨m th∆∞∆°ng hi·ªáu ch√≠nh x√°c v√† th√¥ng tin s·∫£n ph·∫©m.\n",
        "     - ∆Øu ti√™n s·ª≠ d·ª•ng th∆∞∆°ng hi·ªáu t·ª´ suggestions n·∫øu c√≥.\n",
        "     - Gi·ªØ nguy√™n t·ª´ kh√≥a c·ªët l√µi t·ª´ QUERY.\n",
        "     - Ch·ªâ b·ªï sung th√¥ng tin lo·∫°i s·∫£n ph·∫©m chung t·ª´ suggestions, kh√¥ng th√™m c√°c chi ti·∫øt c·ª• th·ªÉ.\n",
        "     - KH√îNG s·ª≠ d·ª•ng suggestions cho `message_banner`.\n",
        "     - Chu·∫©n h√≥a ch√≠nh t·∫£ th∆∞∆°ng hi·ªáu (ƒë·ªô tin c·∫≠y cao): N·∫øu t·∫•t c·∫£ CONCUNG_SUGGESTIONS c√πng m·ªôt th∆∞∆°ng hi·ªáu ‚Üí s·ª≠a th∆∞∆°ng hi·ªáu trong `keyword` theo t√™n ƒë√∫ng; kh√¥ng th√™m thu·ªôc t√≠nh m·ªõi t·ª´ CONCUNG_SUGGESTIONS.\n",
        "   - Tr√≠ch xu·∫•t nhu c·∫ßu s·∫£n ph·∫©m c·ªët l√µi.\n",
        "   - X√°c ƒë·ªãnh thu·ªôc t√≠nh s·∫£n ph·∫©m (ƒë·ªô tu·ªïi, size, th∆∞∆°ng hi·ªáu...).\n",
        "   - Chuy·ªÉn ƒë·ªïi v·∫•n ƒë·ªÅ th√†nh gi·∫£i ph√°p s·∫£n ph·∫©m.\n",
        "   - T·ªëi ∆∞u th√†nh t·ª´ kh√≥a t√¨m ki·∫øm hi·ªáu qu·∫£ (C√ì TH·ªÇ bao g·ªìm t√™n th∆∞∆°ng hi·ªáu t·ª´ query g·ªëc).\n",
        "   - **T·∫°o `message_banner` theo quy tr√¨nh nghi√™m ng·∫∑t sau**:\n",
        "     - **B1: Ki·ªÉm tra \"QUY T·∫ÆC ∆ØU TI√äN\"**. N·∫øu ƒë·ªß ƒëi·ªÅu ki·ªán, t·∫°o banner C√ì ch·ª©a t√™n th∆∞∆°ng hi·ªáu v√† chuy·ªÉn sang b∆∞·ªõc ti·∫øp theo.\n",
        "     - **B2: N·∫øu kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán ∆∞u ti√™n**, t·∫°o banner theo \"QUY T·∫ÆC CHUNG\" (tuy·ªát ƒë·ªëi kh√¥ng ch·ª©a t√™n th∆∞∆°ng hi·ªáu).\n",
        "   - **CRITICAL**: ƒê·∫£m b·∫£o `message_banner` tu√¢n th·ªß ƒë√∫ng quy tr√¨nh B1-B2 ·ªü tr√™n. `message_no_result` ch·ªâ ƒë∆∞·ª£c ph√©p ch·ª©a t√™n th∆∞∆°ng hi·ªáu khi tr√≠ch d·∫´n l·∫°i `USER_QUERY` v√† ƒë·∫∑t trong th·∫ª `<b>`.\n",
        "   - Tr·∫£ v·ªÅ JSON v·ªõi `keyword`, `message_banner`, v√† `message_no_result` ƒë√£ ƒë∆∞·ª£c t·∫°o.\n",
        "-----\n",
        "                # D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO\n",
        "                QUERY: ƒë·ªìng h·ªì chim nh·∫°i gi·ªçng \n",
        "                CONCUNG_SUGGESTIONS: \"xe √¥ t√¥ ƒë·ªìng h·ªì, v√≤ng tay b√© g√°i ƒë·ªìng h·ªì, v·ªõ ƒë·ªìng Natri b√© trai\" \n",
        "\"\"\"\n",
        "\n",
        "answer = ask_question(my_question,  temperature=0.7)\n",
        "print(f\"C√¢u h·ªèi: {my_question}\")\n",
        "print(f\"\\nTr·∫£ l·ªùi:\\n{answer}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "# X√≥a GPU cache n·∫øu g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ memory\n",
        "clear_gpu_memory()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    print(f\"GPU memory allocated: {allocated:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {reserved:.2f} GB\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
