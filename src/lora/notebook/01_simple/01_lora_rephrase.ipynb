{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHozlDhQ0ux"
      },
      "source": [
        "# Fine-tune Qwen 2.5 7B v·ªõi Dataset Rephrase s·ª≠ d·ª•ng LoRA\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n fine-tune Qwen 2.5 7B Instruct v·ªõi dataset Rephrase ƒë·ªÉ t·∫°o JSON output t·ª´ query ti·∫øng Vi·ªát.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Base Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **PEFT Method**: LoRA (r=8 ho·∫∑c r=16)\n",
        "- **Dataset**: Rephrase (1,000 samples)\n",
        "- **Task**: Text-to-JSON Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odAt-5R9Q0uy",
        "outputId": "5e080db3-a30b-4377-932c-75e3a6c8ed60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\n",
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: 2.9.0+cu126)\n",
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì peft>=0.6.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì datasets>=2.14.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì accelerate>=0.24.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì bitsandbytes>=0.41.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† scikit-learn ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì tqdm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "\n",
            "==================================================\n",
            "‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\n",
            "B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (CH·∫†Y CELL N√ÄY TR∆Ø·ªöC - ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "def check_and_install(package_name, install_cmd_list=None):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "\n",
        "    try:\n",
        "        # Th·ª≠ import ƒë·ªÉ ki·ªÉm tra\n",
        "        if package_import == \"torch\":\n",
        "            import torch\n",
        "            print(f\"‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "        elif package_import == \"sklearn\":\n",
        "            import sklearn\n",
        "            print(f\"‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {sklearn.__version__})\")\n",
        "        else:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "        if install_cmd_list:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + install_cmd_list)\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "\n",
        "# Ph√°t hi·ªán h·ªá ƒëi·ªÅu h√†nh v√† c√†i ƒë·∫∑t PyTorch ph√π h·ª£p\n",
        "system = platform.system()\n",
        "is_macos = system == \"Darwin\"\n",
        "is_linux = system == \"Linux\"\n",
        "\n",
        "if is_macos:\n",
        "    # macOS: C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\n",
        "    print(\"Ph√°t hi·ªán macOS - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "elif is_linux:\n",
        "    # Linux: Th·ª≠ c√†i ƒë·∫∑t v·ªõi CUDA support\n",
        "    print(\"Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\")\n",
        "    try:\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "    except:\n",
        "        # N·∫øu th·∫•t b·∫°i, c√†i ƒë·∫∑t t·ª´ PyPI\n",
        "        print(\"‚ö† Kh√¥ng th·ªÉ c√†i ƒë·∫∑t PyTorch v·ªõi CUDA, ƒëang c√†i ƒë·∫∑t t·ª´ PyPI...\")\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "else:\n",
        "    # Windows ho·∫∑c h·ªá th·ªëng kh√°c: C√†i ƒë·∫∑t t·ª´ PyPI\n",
        "    print(f\"Ph√°t hi·ªán {system} - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c packages kh√°c\n",
        "packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"scikit-learn\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "print(\"B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl34XBNrQ0uz",
        "outputId": "4939fd2c-494e-4f18-88fd-61fed70fcd52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "CUDA memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tMFrOVoQ0u0"
      },
      "source": [
        "## B∆∞·ªõc 1: Load v√† X·ª≠ L√Ω Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dur29o90Q0u0",
        "outputId": "f973d7a9-b8db-4217-d837-92755471d289"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 1000\n",
            "Sample keys: dict_keys(['status', 'count', 'output', 'prompt_version', 'key', 'vector_distance', 'query'])\n",
            "\n",
            "Example sample:\n",
            "{\n",
            "  \"status\": 0,\n",
            "  \"count\": 12,\n",
            "  \"output\": {\n",
            "    \"keyword\": \"combo d·∫ßu tr√†m Cung ƒê√¨nh\",\n",
            "    \"reasoning\": \"\",\n",
            "    \"is_in_scope\": true,\n",
            "    \"message_banner\": \"Ba m·∫π ƒëang c·∫ßn <b>d·∫ßu tr√†m</b> an to√†n cho b√© v√† gia ƒë√¨nh? Con C∆∞ng lu√¥n s·∫µn s√†ng gi√∫p ba m·∫π chƒÉm s√≥c y√™u th∆∞∆°ng! üåø\",\n",
            "    \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y <b>combo d·∫ßu tr√†m ho√†ng cung</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§ó\"\n",
            "  },\n",
            "  \"prompt_version\": \"14\",\n",
            "  \"key\": \"d7c8973dd60986d1383be0676cb7eb1a63b7b632892456cd0fa656cae9efe7c3\",\n",
            "  \"vector_distance\": 0.37749016284942627,\n",
            "  \"query\": \"combo d·∫ßu tr√†m ho√†ng cung\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "dataset_path = \"/content/llm_trace_spe_langfuse_trace.json\"\n",
        "\n",
        "with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "print(f\"Total samples: {len(raw_data)}\")\n",
        "print(f\"Sample keys: {raw_data[0].keys()}\")\n",
        "print(f\"\\nExample sample:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znsvyE4TQ0u0",
        "outputId": "cca97501-bbd1-4d56-8be6-9b8ca57e87de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Formatted samples: 1000\n",
            "\n",
            "Example:\n",
            "Prompt: Query: combo d·∫ßu tr√†m ho√†ng cung\n",
            "\n",
            "Output JSON:\n",
            "Output: {\"keyword\": \"\", \"is_in_scope\": false, \"reasoning\": \"\", \"message_banner\": \"\", \"message_no_result\": \"\"}...\n"
          ]
        }
      ],
      "source": [
        "def format_prompt(query: str) -> str:\n",
        "    \"\"\"Format input prompt cho model\"\"\"\n",
        "    return f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "def format_output(data: Dict) -> str:\n",
        "    \"\"\"Format output JSON t·ª´ data\"\"\"\n",
        "    output = {\n",
        "        \"keyword\": data.get(\"keyword\", \"\"),\n",
        "        \"is_in_scope\": data.get(\"is_in_scope\", False),\n",
        "        \"reasoning\": data.get(\"reasoning\", \"\"),\n",
        "        \"message_banner\": data.get(\"message_banner\", \"\"),\n",
        "        \"message_no_result\": data.get(\"message_no_result\", \"\")\n",
        "    }\n",
        "    return json.dumps(output, ensure_ascii=False, indent=None)\n",
        "\n",
        "def prepare_dataset(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Chu·∫©n b·ªã dataset cho training\"\"\"\n",
        "    formatted_data = []\n",
        "\n",
        "    for item in raw_data:\n",
        "        query = item.get(\"query\", \"\")\n",
        "        prompt = format_prompt(query)\n",
        "        output = format_output(item)\n",
        "\n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output,\n",
        "            \"query\": query,\n",
        "            \"is_in_scope\": item.get(\"is_in_scope\", False)\n",
        "        })\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Format dataset\n",
        "formatted_data = prepare_dataset(raw_data)\n",
        "print(f\"Formatted samples: {len(formatted_data)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Prompt: {formatted_data[0]['prompt']}\")\n",
        "print(f\"Output: {formatted_data[0]['output'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWAasq5yQ0u1",
        "outputId": "fd4c6b28-041c-4a57-a5bf-14823c164ab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 800 samples\n",
            "Validation: 100 samples\n",
            "Test: 100 samples\n"
          ]
        }
      ],
      "source": [
        "# Chia dataset: 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(\n",
        "    formatted_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in formatted_data]  # Stratified split\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in temp_data]\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "print(f\"Test: {len(test_data)} samples\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrHOrCGFQ0u1"
      },
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrZNzxIyQ0u1",
        "outputId": "34367bd1-2605-4717-b2b8-bc893efef038"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer loaded\n",
            "Vocab size: 151665\n",
            "Pad token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization (t√πy ch·ªçn, ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n",
        "# B·ªè comment n·∫øu c·∫ßn quantization\n",
        "use_quantization = True  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM\n",
        "\n",
        "if use_quantization:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Tokenizer loaded\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg8aWjjAVg_P",
        "outputId": "1e3a1a0c-b426-426a-9c2f-685a1f05edfd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: bitsandbytes\n",
            "Version: 0.49.0\n",
            "Summary: k-bit optimizers and matrix multiplication routines.\n",
            "Home-page: https://github.com/bitsandbytes-foundation/bitsandbytes\n",
            "Author: \n",
            "Author-email: Tim Dettmers <dettmers@cs.washington.edu>\n",
            "License: \n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: numpy, packaging, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "6565b33ff6294c46a80ab389079ea252",
            "d4cfbd4fda9c47d9b30e7f150355e3f2",
            "4be36299021b43bab0ed5d8db5bd7047",
            "b48b97fd92b6476e98b0a8ca6528535f",
            "db635d3502394df2b560f7205de9817e",
            "4d8271ab4eff485381ba91a6b35baaa1",
            "55db64a4d86446e19d3ea7f7b7e851ba",
            "1a6f3e2163df4daca4c642b86ce183a3",
            "88f12a80f92b47e5965352bd43f3f276",
            "6a8339501d554beb8b19eb206cefcaf0",
            "38ca6717c0cb4f32bb70343291487fbf"
          ]
        },
        "id": "19zvZBpXQ0u1",
        "outputId": "697f33b0-885b-4c55-ce87-7fdf9b86b1c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6565b33ff6294c46a80ab389079ea252"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: Qwen/Qwen2.5-7B-Instruct\n",
            "Model device: cuda:0\n",
            "Total parameters: 4,352,972,288\n",
            "Trainable parameters: 1,090,199,040\n",
            "Trainable %: 25.04%\n"
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not use_quantization else None\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Ki·ªÉm tra s·ªë tham s·ªë\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0iWrMyQ0u2"
      },
      "source": [
        "## B∆∞·ªõc 3: C·∫•u H√¨nh LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V27WdnoxQ0u2",
        "outputId": "ad13a4fc-1bd9-4197-9828-d024ded75197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LoRA Config:\n",
            "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules={'k_proj', 'v_proj', 'q_proj', 'o_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)\n"
          ]
        }
      ],
      "source": [
        "# C·∫•u h√¨nh LoRA - Option 1: Conservative (r=8) - Khuy·∫øn ngh·ªã b·∫Øt ƒë·∫ßu\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                          # Rank\n",
        "    lora_alpha=16,               # Alpha = 2 * r\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.1,            # Dropout ƒë·ªÉ tr√°nh overfitting\n",
        "    bias=\"none\",                  # Kh√¥ng train bias\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Ho·∫∑c Option 2: Balanced (r=16) - Uncomment ƒë·ªÉ d√πng\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False\n",
        "# )\n",
        "\n",
        "print(\"LoRA Config:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rfr8lRbTQ0u2",
        "outputId": "3d757728-b506-44bc-9fe2-21ccf95b5105"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Applying LoRA adapter...\n",
            "\n",
            "==================================================\n",
            "trainable params: 5,046,272 || all params: 7,620,662,784 || trainable%: 0.0662\n",
            "==================================================\n",
            "\n",
            "‚úì Trainable parameters: 5,046,272\n",
            "‚úì Found 1008 LoRA modules\n",
            "\n",
            "Verifying gradient requirements...\n",
            "‚úì Final trainable parameters: 5,046,272\n",
            "‚úì Model is ready for training!\n"
          ]
        }
      ],
      "source": [
        "# √Åp d·ª•ng LoRA\n",
        "print(\"Applying LoRA adapter...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Ki·ªÉm tra tham s·ªë c√≥ th·ªÉ train\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "model.print_trainable_parameters()\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ƒê·∫£m b·∫£o model ·ªü training mode\n",
        "model.train()\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n‚úì Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No trainable parameters found!\")\n",
        "    print(\"This might be because target_modules don't match the model architecture.\")\n",
        "    print(\"\\nChecking available attention modules in the model...\")\n",
        "\n",
        "    # List available modules\n",
        "    attention_modules = []\n",
        "    for name, module in model.named_modules():\n",
        "        if any(x in name for x in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]):\n",
        "            attention_modules.append(name)\n",
        "\n",
        "    if attention_modules:\n",
        "        print(f\"\\nFound {len(attention_modules)} potential target modules:\")\n",
        "        for i, mod in enumerate(attention_modules[:10]):  # Show first 10\n",
        "            print(f\"  {i+1}. {mod}\")\n",
        "        if len(attention_modules) > 10:\n",
        "            print(f\"  ... and {len(attention_modules) - 10} more\")\n",
        "\n",
        "    raise ValueError(\n",
        "        \"ERROR: No trainable parameters found! LoRA adapter may not have been applied correctly.\\n\"\n",
        "        \"Possible solutions:\\n\"\n",
        "        \"1. Check if target_modules match the model architecture (see list above)\\n\"\n",
        "        \"2. Try using target_modules='all-linear' instead\\n\"\n",
        "        \"3. Re-run the model loading cell and this cell\"\n",
        "    )\n",
        "\n",
        "# Verify LoRA modules are present\n",
        "lora_modules = [name for name, module in model.named_modules() if 'lora' in name.lower()]\n",
        "print(f\"‚úì Found {len(lora_modules)} LoRA modules\")\n",
        "if len(lora_modules) == 0:\n",
        "    raise ValueError(\"ERROR: No LoRA modules found! Check target_modules in LoRA config.\")\n",
        "\n",
        "# Ensure all LoRA parameters have requires_grad=True\n",
        "print(\"\\nVerifying gradient requirements...\")\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' in name.lower() and not param.requires_grad:\n",
        "        print(f\"‚ö†Ô∏è  Warning: {name} does not require grad, enabling it...\")\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Final verification\n",
        "final_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"‚úì Final trainable parameters: {final_trainable:,}\")\n",
        "print(\"‚úì Model is ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0LIRukrQ0u2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272,
          "referenced_widgets": [
            "6a98b388a4bc4b1688809cf9acc66e5f",
            "936a59c784c343a7b0a2a23e70084b2d",
            "ec9109a8cb1142b397a5989cc13e93db",
            "6c91c83d2827495ea65fd50429cfdfb9",
            "7467eae085204bc0908eb41a8f0f54b2",
            "112d800bc39d48c797ccff6f14656535",
            "f79f9d6954714a12973b7469b39d57c8",
            "7f1f1f3661094a7fa2e4def8fe5cfc3c",
            "936ac8b8de3f4cff9ffceef4a8a328c0",
            "88b5a22810324cbf996f2498c177b190",
            "589d8c31c77344378f0ba38317981689",
            "7b21311a0baf4cd2a8e7c7acb515b6ae",
            "51c6c935d93e49268d4ff670752623c5",
            "d07c4cae35034717ac7e4c0ef0d826d4",
            "6651eb8e54d840c79727edf4a59124d6",
            "59c7b2e436964ce49a4e7e644b5050cb",
            "2d690fd9fbca47359d02133a2d86b7c3",
            "35db2811ea7049ccae4772a765571e1a",
            "697f65f40d0743e48b50f69da690596d",
            "1d8e1d2c3ce04e8e9c9d405adf5b55bc",
            "10a9f725999449b1a9e31aed43646fe7",
            "732fc4ea02e64433941539f8cf3686bf"
          ]
        },
        "id": "PN-umZG6Q0u2",
        "outputId": "7bb20f67-0140-49ca-d26b-85cc0ef36d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing train dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a98b388a4bc4b1688809cf9acc66e5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing validation dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b21311a0baf4cd2a8e7c7acb515b6ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tokenized: 800 samples\n",
            "Val tokenized: 100 samples\n",
            "Example tokenized length: 60\n",
            "\n",
            "First example structure:\n",
            "  input_ids type: <class 'list'>\n",
            "  input_ids length: 60\n",
            "  Available keys: ['input_ids', 'attention_mask']\n",
            "  ‚úì Labels will be created by DataCollator during batching (this is correct)\n"
          ]
        }
      ],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize prompt v√† output\"\"\"\n",
        "    # Combine prompt v√† output\n",
        "    texts = []\n",
        "    for prompt, output in zip(examples[\"prompt\"], examples[\"output\"]):\n",
        "        text = f\"{prompt} {output}\"\n",
        "        texts.append(text)\n",
        "\n",
        "    # Tokenize with padding=True\n",
        "    # Reduced max_length to save memory (512 -> 384)\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=384,  # Reverted to 384 for memory efficiency\n",
        "        padding=True,  # Set to True for consistent tensor shapes\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "\n",
        "    # Don't set labels here - DataCollatorForLanguageModeling will automatically\n",
        "    # create labels from input_ids for causal LM (mlm=False)\n",
        "    # It will pad sequences and set padding tokens in labels to -100\n",
        "    # This manual labels creation was likely interfering.\n",
        "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() # REMOVED\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tokenized)} samples\")\n",
        "print(f\"Example tokenized length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "\n",
        "# Verify the structure of the first example\n",
        "print(f\"\\nFirst example structure:\")\n",
        "print(f\"  input_ids type: {type(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  input_ids length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  Available keys: {list(train_tokenized[0].keys())}\")\n",
        "\n",
        "# Note: labels will be created by DataCollatorForLanguageModeling during batching\n",
        "# This is expected - labels don't exist in the tokenized dataset yet\n",
        "if 'labels' in train_tokenized[0]:\n",
        "    print(f\"  labels type: {type(train_tokenized[0]['labels'])}\")\n",
        "    print(f\"  labels length: {len(train_tokenized[0]['labels'])}\")\n",
        "else:\n",
        "    print(f\"  ‚úì Labels will be created by DataCollator during batching (this is correct)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "h8ZUzZ2zQ0u3"
      },
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "# DataCollatorForLanguageModeling automatically handles padding\n",
        "# It will pad sequences to the same length and set padding tokens in labels to -100\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, kh√¥ng ph·∫£i masked LM\n",
        "    pad_to_multiple_of=8  # T·ªëi ∆∞u cho GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AErd_oy0Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 5: C·∫•u H√¨nh Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d5npuKgQ0u3",
        "outputId": "0c540ded-57db-49ce-cfbf-ff7671702179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments configured\n",
            "Output directory: ./results/qwen2.5-7b-rephrase-lora\n"
          ]
        }
      ],
      "source": [
        "output_dir = \"./results/qwen2.5-7b-rephrase-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=True,  # Ti·∫øt ki·ªám memory\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,  # Ch·ªâ gi·ªØ 3 checkpoints g·∫ßn nh·∫•t\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbc8a523",
        "outputId": "e9cdc24f-2b5e-4a99-a908-3d03d2c23dff"
      },
      "source": [
        "# Explicitly upgrade transformers to ensure the latest version is used\n",
        "!pip install -U transformers\n",
        "\n",
        "import transformers\n",
        "print(f\"Updated transformers version: {transformers.__version__}\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Updated transformers version: 4.57.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kt0Gj0S1Q0u3",
        "outputId": "0c21de95-f8d6-4e9e-afae-a0fb16da4a0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying model setup...\n",
            "Trainable parameters: 5,046,272\n",
            "Trainer initialized\n",
            "Training samples: 800\n",
            "Validation samples: 100\n"
          ]
        }
      ],
      "source": [
        "# Verify model is ready for training\n",
        "print(\"Verifying model setup...\")\n",
        "model.train()  # Ensure training mode\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    raise RuntimeError(\n",
        "        \"No trainable parameters found! \"\n",
        "        \"Please check that LoRA adapter was applied correctly. \"\n",
        "        \"Run the LoRA configuration cell again.\"\n",
        "    )\n",
        "\n",
        "# Kh·ªüi t·∫°o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "print(f\"Training samples: {len(train_tokenized)}\")\n",
        "print(f\"Validation samples: {len(val_tokenized)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wwGL95Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 6: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "NU8pFK_AQ0u3",
        "outputId": "83107438-f710-4676-9d43-132468b4b03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2474877976.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# B·∫Øt ƒë·∫ßu training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# L∆∞u model cu·ªëi c√πng\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2846\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2847\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2848\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2849\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_lomo_optimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "# B·∫Øt ƒë·∫ßu training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# L∆∞u model cu·ªëi c√πng\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nTraining completed! Model saved to {output_dir}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCl559rzQ0u3"
      },
      "source": [
        "## B∆∞·ªõc 7: Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihSCWpv6Q0u3"
      },
      "outputs": [],
      "source": [
        "# Evaluate tr√™n test set\n",
        "print(\"Tokenizing test dataset...\")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSWw8Pr4Q0u4"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° JSON Validity\n",
        "def evaluate_json_validity(model, tokenizer, test_data, max_samples=50):\n",
        "    \"\"\"ƒê√°nh gi√° t·ª∑ l·ªá JSON h·ª£p l·ªá\"\"\"\n",
        "    model.eval()\n",
        "    valid_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for i, item in enumerate(tqdm(test_data[:max_samples], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract JSON (sau \"Output JSON:\")\n",
        "        if \"Output JSON:\" in generated_text:\n",
        "            json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "        else:\n",
        "            json_text = generated_text\n",
        "\n",
        "        # Ki·ªÉm tra JSON validity\n",
        "        try:\n",
        "            parsed = json.loads(json_text)\n",
        "            valid_count += 1\n",
        "        except:\n",
        "            if total_count < 3:  # Print first few errors\n",
        "                print(f\"\\nError parsing JSON {i}:\")\n",
        "                print(f\"Generated: {json_text[:200]}\")\n",
        "\n",
        "        total_count += 1\n",
        "\n",
        "    validity_rate = valid_count / total_count if total_count > 0 else 0\n",
        "    print(f\"\\nJSON Validity Rate: {validity_rate:.2%} ({valid_count}/{total_count})\")\n",
        "    return validity_rate\n",
        "\n",
        "# Ch·∫°y evaluation\n",
        "validity_rate = evaluate_json_validity(model, tokenizer, test_data, max_samples=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksGeSByQ0u4"
      },
      "source": [
        "## B∆∞·ªõc 8: Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNz8StLmQ0u4"
      },
      "outputs": [],
      "source": [
        "def generate_json_response(model, tokenizer, query: str, max_length=512):\n",
        "    \"\"\"Generate JSON response t·ª´ query\"\"\"\n",
        "    prompt = f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract JSON\n",
        "    if \"Output JSON:\" in generated_text:\n",
        "        json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "    else:\n",
        "        json_text = generated_text\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        result = json.loads(json_text)\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        print(f\"Generated text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Test inference\n",
        "test_queries = [\n",
        "    \"s·ªØa cho b√© 6 th√°ng\",\n",
        "    \"t√£ b·ªâm size M\",\n",
        "    \"ƒë·ªì ch∆°i cho tr·∫ª s∆° sinh\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference:\\n\")\n",
        "for query in test_queries:\n",
        "    result = generate_json_response(model, tokenizer, query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Result: {json.dumps(result, ensure_ascii=False, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4513E8QYQ0u4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "499xUKxQQ0u4"
      },
      "outputs": [],
      "source": [
        "# L∆∞u adapter (ch·ªâ LoRA weights, nh·ªè)\n",
        "adapter_path = f\"{output_dir}/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "print(\"\\nƒê·ªÉ load l·∫°i model sau n√†y:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{model_name}\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhWJGA6_Q0u5"
      },
      "source": [
        "## L∆∞u √ù\n",
        "\n",
        "- ƒêi·ªÅu ch·ªânh `per_device_train_batch_size` v√† `gradient_accumulation_steps` d·ª±a tr√™n GPU memory\n",
        "- C√≥ th·ªÉ th·ª≠ `r=16` n·∫øu `r=8` kh√¥ng ƒë·ªß hi·ªáu su·∫•t\n",
        "- Monitor training loss v√† validation loss ƒë·ªÉ tr√°nh overfitting\n",
        "- S·ª≠ d·ª•ng TensorBoard ƒë·ªÉ theo d√µi: `tensorboard --logdir ./results/qwen2.5-7b-rephrase-lora/logs`\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6565b33ff6294c46a80ab389079ea252": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d4cfbd4fda9c47d9b30e7f150355e3f2",
              "IPY_MODEL_4be36299021b43bab0ed5d8db5bd7047",
              "IPY_MODEL_b48b97fd92b6476e98b0a8ca6528535f"
            ],
            "layout": "IPY_MODEL_db635d3502394df2b560f7205de9817e"
          }
        },
        "d4cfbd4fda9c47d9b30e7f150355e3f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d8271ab4eff485381ba91a6b35baaa1",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_55db64a4d86446e19d3ea7f7b7e851ba",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "4be36299021b43bab0ed5d8db5bd7047": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a6f3e2163df4daca4c642b86ce183a3",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88f12a80f92b47e5965352bd43f3f276",
            "value": 4
          }
        },
        "b48b97fd92b6476e98b0a8ca6528535f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a8339501d554beb8b19eb206cefcaf0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_38ca6717c0cb4f32bb70343291487fbf",
            "value": "‚Äá4/4‚Äá[02:07&lt;00:00,‚Äá29.26s/it]"
          }
        },
        "db635d3502394df2b560f7205de9817e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d8271ab4eff485381ba91a6b35baaa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55db64a4d86446e19d3ea7f7b7e851ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a6f3e2163df4daca4c642b86ce183a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88f12a80f92b47e5965352bd43f3f276": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a8339501d554beb8b19eb206cefcaf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ca6717c0cb4f32bb70343291487fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a98b388a4bc4b1688809cf9acc66e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_936a59c784c343a7b0a2a23e70084b2d",
              "IPY_MODEL_ec9109a8cb1142b397a5989cc13e93db",
              "IPY_MODEL_6c91c83d2827495ea65fd50429cfdfb9"
            ],
            "layout": "IPY_MODEL_7467eae085204bc0908eb41a8f0f54b2"
          }
        },
        "936a59c784c343a7b0a2a23e70084b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112d800bc39d48c797ccff6f14656535",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f79f9d6954714a12973b7469b39d57c8",
            "value": "Map:‚Äá100%"
          }
        },
        "ec9109a8cb1142b397a5989cc13e93db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f1f1f3661094a7fa2e4def8fe5cfc3c",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_936ac8b8de3f4cff9ffceef4a8a328c0",
            "value": 800
          }
        },
        "6c91c83d2827495ea65fd50429cfdfb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88b5a22810324cbf996f2498c177b190",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_589d8c31c77344378f0ba38317981689",
            "value": "‚Äá800/800‚Äá[00:00&lt;00:00,‚Äá2433.22‚Äáexamples/s]"
          }
        },
        "7467eae085204bc0908eb41a8f0f54b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "112d800bc39d48c797ccff6f14656535": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79f9d6954714a12973b7469b39d57c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f1f1f3661094a7fa2e4def8fe5cfc3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "936ac8b8de3f4cff9ffceef4a8a328c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "88b5a22810324cbf996f2498c177b190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589d8c31c77344378f0ba38317981689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b21311a0baf4cd2a8e7c7acb515b6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_51c6c935d93e49268d4ff670752623c5",
              "IPY_MODEL_d07c4cae35034717ac7e4c0ef0d826d4",
              "IPY_MODEL_6651eb8e54d840c79727edf4a59124d6"
            ],
            "layout": "IPY_MODEL_59c7b2e436964ce49a4e7e644b5050cb"
          }
        },
        "51c6c935d93e49268d4ff670752623c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d690fd9fbca47359d02133a2d86b7c3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_35db2811ea7049ccae4772a765571e1a",
            "value": "Map:‚Äá100%"
          }
        },
        "d07c4cae35034717ac7e4c0ef0d826d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697f65f40d0743e48b50f69da690596d",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d8e1d2c3ce04e8e9c9d405adf5b55bc",
            "value": 100
          }
        },
        "6651eb8e54d840c79727edf4a59124d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10a9f725999449b1a9e31aed43646fe7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_732fc4ea02e64433941539f8cf3686bf",
            "value": "‚Äá100/100‚Äá[00:00&lt;00:00,‚Äá1603.31‚Äáexamples/s]"
          }
        },
        "59c7b2e436964ce49a4e7e644b5050cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d690fd9fbca47359d02133a2d86b7c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35db2811ea7049ccae4772a765571e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "697f65f40d0743e48b50f69da690596d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d8e1d2c3ce04e8e9c9d405adf5b55bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "10a9f725999449b1a9e31aed43646fe7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "732fc4ea02e64433941539f8cf3686bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}