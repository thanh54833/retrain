{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTHozlDhQ0ux"
      },
      "source": [
        "# Fine-tune Qwen 2.5 7B v·ªõi Dataset Rephrase s·ª≠ d·ª•ng LoRA\n",
        "\n",
        "Notebook n√†y h∆∞·ªõng d·∫´n fine-tune Qwen 2.5 7B Instruct v·ªõi dataset Rephrase ƒë·ªÉ t·∫°o JSON output t·ª´ query ti·∫øng Vi·ªát.\n",
        "\n",
        "## Th√¥ng tin\n",
        "- **Base Model**: Qwen/Qwen2.5-7B-Instruct\n",
        "- **PEFT Method**: LoRA (r=8 ho·∫∑c r=16)\n",
        "- **Dataset**: Rephrase (1,000 samples)\n",
        "- **Task**: Text-to-JSON Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odAt-5R9Q0uy",
        "outputId": "10f43caf-87b0-4535-a433-840bb1283e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\n",
            "\n",
            "Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\n",
            "‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: 2.9.0+cpu)\n",
            "‚úì transformers>=4.35.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì peft>=0.6.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì datasets>=2.14.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì accelerate>=0.24.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì bitsandbytes>=0.41.0 ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚ö† scikit-learn ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\n",
            "‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "‚úì tqdm ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\n",
            "\n",
            "==================================================\n",
            "‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\n",
            "B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (CH·∫†Y CELL N√ÄY TR∆Ø·ªöC - ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import platform\n",
        "\n",
        "def check_and_install(package_name, install_cmd_list=None):\n",
        "    \"\"\"Ki·ªÉm tra v√† c√†i ƒë·∫∑t package n·∫øu ch∆∞a c√≥\"\"\"\n",
        "    package_import = package_name.split('>=')[0].split('==')[0].split('<')[0].strip()\n",
        "\n",
        "    try:\n",
        "        # Th·ª≠ import ƒë·ªÉ ki·ªÉm tra\n",
        "        if package_import == \"torch\":\n",
        "            import torch\n",
        "            print(f\"‚úì torch ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {torch.__version__})\")\n",
        "        elif package_import == \"sklearn\":\n",
        "            import sklearn\n",
        "            print(f\"‚úì scikit-learn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (version: {sklearn.__version__})\")\n",
        "        else:\n",
        "            __import__(package_import)\n",
        "            print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return True\n",
        "    except ImportError:\n",
        "        print(f\"‚ö† {package_name} ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t. ƒêang c√†i ƒë·∫∑t...\")\n",
        "        if install_cmd_list:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + install_cmd_list)\n",
        "        else:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        print(f\"‚úì {package_name} ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t\")\n",
        "        return False\n",
        "\n",
        "print(\"ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t dependencies...\\n\")\n",
        "\n",
        "# Ph√°t hi·ªán h·ªá ƒëi·ªÅu h√†nh v√† c√†i ƒë·∫∑t PyTorch ph√π h·ª£p\n",
        "system = platform.system()\n",
        "is_macos = system == \"Darwin\"\n",
        "is_linux = system == \"Linux\"\n",
        "\n",
        "if is_macos:\n",
        "    # macOS: C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\n",
        "    print(\"Ph√°t hi·ªán macOS - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI (kh√¥ng c√≥ CUDA support)\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "elif is_linux:\n",
        "    # Linux: Th·ª≠ c√†i ƒë·∫∑t v·ªõi CUDA support\n",
        "    print(\"Ph√°t hi·ªán Linux - C√†i ƒë·∫∑t PyTorch v·ªõi CUDA support\")\n",
        "    try:\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\", \"--index-url\", \"https://download.pytorch.org/whl/cu118\"])\n",
        "    except:\n",
        "        # N·∫øu th·∫•t b·∫°i, c√†i ƒë·∫∑t t·ª´ PyPI\n",
        "        print(\"‚ö† Kh√¥ng th·ªÉ c√†i ƒë·∫∑t PyTorch v·ªõi CUDA, ƒëang c√†i ƒë·∫∑t t·ª´ PyPI...\")\n",
        "        check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "else:\n",
        "    # Windows ho·∫∑c h·ªá th·ªëng kh√°c: C√†i ƒë·∫∑t t·ª´ PyPI\n",
        "    print(f\"Ph√°t hi·ªán {system} - C√†i ƒë·∫∑t PyTorch t·ª´ PyPI\")\n",
        "    check_and_install(\"torch\", [\"torch\", \"torchvision\", \"torchaudio\"])\n",
        "\n",
        "# C√†i ƒë·∫∑t c√°c packages kh√°c\n",
        "packages = [\n",
        "    \"transformers>=4.35.0\",\n",
        "    \"peft>=0.6.0\",\n",
        "    \"datasets>=2.14.0\",\n",
        "    \"accelerate>=0.24.0\",\n",
        "    \"bitsandbytes>=0.41.0\",\n",
        "    \"scikit-learn\",\n",
        "    \"tqdm\"\n",
        "]\n",
        "\n",
        "for package in packages:\n",
        "    check_and_install(package)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"‚úì T·∫•t c·∫£ dependencies ƒë√£ s·∫µn s√†ng!\")\n",
        "print(\"B·∫°n c√≥ th·ªÉ ch·∫°y cell ti·∫øp theo ƒë·ªÉ import c√°c th∆∞ vi·ªán.\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl34XBNrQ0uz",
        "outputId": "605ba5a9-d028-4d71-ac4b-a9edce9be265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: False\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, List\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Ki·ªÉm tra GPU\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tMFrOVoQ0u0"
      },
      "source": [
        "## B∆∞·ªõc 1: Load v√† X·ª≠ L√Ω Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dur29o90Q0u0",
        "outputId": "3636093d-54b2-42cc-d8dc-eefed7461ede"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total samples: 1000\n",
            "Sample keys: dict_keys(['status', 'count', 'output', 'prompt_version', 'key', 'vector_distance', 'query'])\n",
            "\n",
            "Example sample:\n",
            "{\n",
            "  \"status\": 0,\n",
            "  \"count\": 12,\n",
            "  \"output\": {\n",
            "    \"keyword\": \"combo d·∫ßu tr√†m Cung ƒê√¨nh\",\n",
            "    \"reasoning\": \"\",\n",
            "    \"is_in_scope\": true,\n",
            "    \"message_banner\": \"Ba m·∫π ƒëang c·∫ßn <b>d·∫ßu tr√†m</b> an to√†n cho b√© v√† gia ƒë√¨nh? Con C∆∞ng lu√¥n s·∫µn s√†ng gi√∫p ba m·∫π chƒÉm s√≥c y√™u th∆∞∆°ng! üåø\",\n",
            "    \"message_no_result\": \"Ti·∫øc qu√°, Con C∆∞ng ch∆∞a t√¨m th·∫•y <b>combo d·∫ßu tr√†m ho√†ng cung</b>. Ba m·∫π th·ª≠ ki·ªÉm tra l·∫°i t√™n s·∫£n ph·∫©m ho·∫∑c d√πng t·ª´ kh√≥a ng·∫Øn g·ªçn h∆°n nh√©! ü§ó\"\n",
            "  },\n",
            "  \"prompt_version\": \"14\",\n",
            "  \"key\": \"d7c8973dd60986d1383be0676cb7eb1a63b7b632892456cd0fa656cae9efe7c3\",\n",
            "  \"vector_distance\": 0.37749016284942627,\n",
            "  \"query\": \"combo d·∫ßu tr√†m ho√†ng cung\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load dataset from URL\n",
        "import urllib.request\n",
        "\n",
        "dataset_url = \"https://raw.githubusercontent.com/thanh54833/retrain/refs/heads/main/src/lora/dataset/01_simple/01_dataset_rephrase.json\"\n",
        "\n",
        "# Fetch dataset from URL\n",
        "with urllib.request.urlopen(dataset_url) as response:\n",
        "    raw_data = json.loads(response.read().decode('utf-8'))\n",
        "\n",
        "print(f\"Total samples: {len(raw_data)}\")\n",
        "print(f\"Sample keys: {raw_data[0].keys()}\")\n",
        "print(f\"\\nExample sample:\")\n",
        "print(json.dumps(raw_data[0], ensure_ascii=False, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znsvyE4TQ0u0",
        "outputId": "d015bccc-4be5-40fa-f1c4-b0c3e413b9ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Formatted samples: 1000\n",
            "\n",
            "Example:\n",
            "Prompt: Query: combo d·∫ßu tr√†m ho√†ng cung\n",
            "\n",
            "Output JSON:\n",
            "Output: {\"keyword\": \"\", \"is_in_scope\": false, \"reasoning\": \"\", \"message_banner\": \"\", \"message_no_result\": \"\"}...\n"
          ]
        }
      ],
      "source": [
        "def format_prompt(query: str) -> str:\n",
        "    \"\"\"Format input prompt cho model\"\"\"\n",
        "    return f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "def format_output(data: Dict) -> str:\n",
        "    \"\"\"Format output JSON t·ª´ data\"\"\"\n",
        "    output = {\n",
        "        \"keyword\": data.get(\"keyword\", \"\"),\n",
        "        \"is_in_scope\": data.get(\"is_in_scope\", False),\n",
        "        \"reasoning\": data.get(\"reasoning\", \"\"),\n",
        "        \"message_banner\": data.get(\"message_banner\", \"\"),\n",
        "        \"message_no_result\": data.get(\"message_no_result\", \"\")\n",
        "    }\n",
        "    return json.dumps(output, ensure_ascii=False, indent=None)\n",
        "\n",
        "def prepare_dataset(raw_data: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Chu·∫©n b·ªã dataset cho training\"\"\"\n",
        "    formatted_data = []\n",
        "\n",
        "    for item in raw_data:\n",
        "        query = item.get(\"query\", \"\")\n",
        "        prompt = format_prompt(query)\n",
        "        output = format_output(item)\n",
        "\n",
        "        formatted_data.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": output,\n",
        "            \"query\": query,\n",
        "            \"is_in_scope\": item.get(\"is_in_scope\", False)\n",
        "        })\n",
        "\n",
        "    return formatted_data\n",
        "\n",
        "# Format dataset\n",
        "formatted_data = prepare_dataset(raw_data)\n",
        "print(f\"Formatted samples: {len(formatted_data)}\")\n",
        "print(\"\\nExample:\")\n",
        "print(f\"Prompt: {formatted_data[0]['prompt']}\")\n",
        "print(f\"Output: {formatted_data[0]['output'][:150]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWAasq5yQ0u1",
        "outputId": "49acb97b-976a-4e1d-a41c-860a8a5f3354"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 800 samples\n",
            "Validation: 100 samples\n",
            "Test: 100 samples\n"
          ]
        }
      ],
      "source": [
        "# Chia dataset: 80% train, 10% validation, 10% test\n",
        "train_data, temp_data = train_test_split(\n",
        "    formatted_data,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in formatted_data]  # Stratified split\n",
        ")\n",
        "\n",
        "val_data, test_data = train_test_split(\n",
        "    temp_data,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=[item['is_in_scope'] for item in temp_data]\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "print(f\"Test: {len(test_data)} samples\")\n",
        "\n",
        "# Convert to HuggingFace Dataset\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "test_dataset = Dataset.from_list(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrHOrCGFQ0u1"
      },
      "source": [
        "## B∆∞·ªõc 2: Load Model v√† Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrZNzxIyQ0u1",
        "outputId": "fb463eeb-302a-453d-f249-0d5df4c94bbc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a0f06b69a1ea402a96c87158aa8ccf7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb59f3cce464d5f91521dcb1acfce5d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "841501b6e4104acf9e859e65bb66d92f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f9900e5c1544f52b1963c6137c8eb65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded\n",
            "Vocab size: 151665\n",
            "Pad token: <|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "# C·∫•u h√¨nh quantization (t√πy ch·ªçn, ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ)\n",
        "# B·ªè comment n·∫øu c·∫ßn quantization\n",
        "use_quantization = True  # ƒê·∫∑t False n·∫øu c√≥ ƒë·ªß VRAM\n",
        "\n",
        "if use_quantization:\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "else:\n",
        "    quantization_config = None\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "# ƒê·∫£m b·∫£o c√≥ pad token\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(\"Tokenizer loaded\")\n",
        "print(f\"Vocab size: {len(tokenizer)}\")\n",
        "print(f\"Pad token: {tokenizer.pad_token}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg8aWjjAVg_P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "19zvZBpXQ0u1",
        "outputId": "03516159-5210-4dd3-a901-3c633d076d30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1461452798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5027\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5028\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5029\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5031\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    128\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16 if not use_quantization else None\n",
        ")\n",
        "\n",
        "print(f\"Model loaded: {model_name}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Ki·ªÉm tra s·ªë tham s·ªë\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total parameters: {total_params:,}\")\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vG0iWrMyQ0u2"
      },
      "source": [
        "## B∆∞·ªõc 3: C·∫•u H√¨nh LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V27WdnoxQ0u2"
      },
      "outputs": [],
      "source": [
        "# C·∫•u h√¨nh LoRA - Option 1: Conservative (r=8) - Khuy·∫øn ngh·ªã b·∫Øt ƒë·∫ßu\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                          # Rank\n",
        "    lora_alpha=16,               # Alpha = 2 * r\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Attention layers\n",
        "    lora_dropout=0.1,            # Dropout ƒë·ªÉ tr√°nh overfitting\n",
        "    bias=\"none\",                  # Kh√¥ng train bias\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False\n",
        ")\n",
        "\n",
        "# Ho·∫∑c Option 2: Balanced (r=16) - Uncomment ƒë·ªÉ d√πng\n",
        "# lora_config = LoraConfig(\n",
        "#     r=16,\n",
        "#     lora_alpha=32,\n",
        "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     task_type=TaskType.CAUSAL_LM,\n",
        "#     inference_mode=False\n",
        "# )\n",
        "\n",
        "print(\"LoRA Config:\")\n",
        "print(lora_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfr8lRbTQ0u2"
      },
      "outputs": [],
      "source": [
        "# √Åp d·ª•ng LoRA\n",
        "print(\"Applying LoRA adapter...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Ki·ªÉm tra tham s·ªë c√≥ th·ªÉ train\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "model.print_trainable_parameters()\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ƒê·∫£m b·∫£o model ·ªü training mode\n",
        "model.train()\n",
        "\n",
        "# Verify trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\n‚úì Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    print(\"\\n‚ö†Ô∏è  WARNING: No trainable parameters found!\")\n",
        "    print(\"This might be because target_modules don't match the model architecture.\")\n",
        "    print(\"\\nChecking available attention modules in the model...\")\n",
        "\n",
        "    # List available modules\n",
        "    attention_modules = []\n",
        "    for name, module in model.named_modules():\n",
        "        if any(x in name for x in [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]):\n",
        "            attention_modules.append(name)\n",
        "\n",
        "    if attention_modules:\n",
        "        print(f\"\\nFound {len(attention_modules)} potential target modules:\")\n",
        "        for i, mod in enumerate(attention_modules[:10]):  # Show first 10\n",
        "            print(f\"  {i+1}. {mod}\")\n",
        "        if len(attention_modules) > 10:\n",
        "            print(f\"  ... and {len(attention_modules) - 10} more\")\n",
        "\n",
        "    raise ValueError(\n",
        "        \"ERROR: No trainable parameters found! LoRA adapter may not have been applied correctly.\\n\"\n",
        "        \"Possible solutions:\\n\"\n",
        "        \"1. Check if target_modules match the model architecture (see list above)\\n\"\n",
        "        \"2. Try using target_modules='all-linear' instead\\n\"\n",
        "        \"3. Re-run the model loading cell and this cell\"\n",
        "    )\n",
        "\n",
        "# Verify LoRA modules are present\n",
        "lora_modules = [name for name, module in model.named_modules() if 'lora' in name.lower()]\n",
        "print(f\"‚úì Found {len(lora_modules)} LoRA modules\")\n",
        "if len(lora_modules) == 0:\n",
        "    raise ValueError(\"ERROR: No LoRA modules found! Check target_modules in LoRA config.\")\n",
        "\n",
        "# Ensure all LoRA parameters have requires_grad=True\n",
        "print(\"\\nVerifying gradient requirements...\")\n",
        "for name, param in model.named_parameters():\n",
        "    if 'lora' in name.lower() and not param.requires_grad:\n",
        "        print(f\"‚ö†Ô∏è  Warning: {name} does not require grad, enabling it...\")\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Final verification\n",
        "final_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"‚úì Final trainable parameters: {final_trainable:,}\")\n",
        "print(\"‚úì Model is ready for training!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0LIRukrQ0u2"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN-umZG6Q0u2"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    \"\"\"Tokenize prompt v√† output\"\"\"\n",
        "    # Combine prompt v√† output\n",
        "    texts = []\n",
        "    for prompt, output in zip(examples[\"prompt\"], examples[\"output\"]):\n",
        "        text = f\"{prompt} {output}\"\n",
        "        texts.append(text)\n",
        "\n",
        "    # Tokenize with padding=True\n",
        "    # Reduced max_length to save memory (512 -> 384)\n",
        "    tokenized = tokenizer(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        max_length=384,  # Reverted to 384 for memory efficiency\n",
        "        padding=True,  # Set to True for consistent tensor shapes\n",
        "        return_tensors=None  # Return lists, not tensors\n",
        "    )\n",
        "\n",
        "    # Don't set labels here - DataCollatorForLanguageModeling will automatically\n",
        "    # create labels from input_ids for causal LM (mlm=False)\n",
        "    # It will pad sequences and set padding tokens in labels to -100\n",
        "    # This manual labels creation was likely interfering.\n",
        "    # tokenized[\"labels\"] = tokenized[\"input_ids\"].copy() # REMOVED\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "# Tokenize datasets\n",
        "print(\"Tokenizing train dataset...\")\n",
        "train_tokenized = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "\n",
        "print(\"Tokenizing validation dataset...\")\n",
        "val_tokenized = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")\n",
        "\n",
        "print(f\"Train tokenized: {len(train_tokenized)} samples\")\n",
        "print(f\"Val tokenized: {len(val_tokenized)} samples\")\n",
        "print(f\"Example tokenized length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "\n",
        "# Verify the structure of the first example\n",
        "print(f\"\\nFirst example structure:\")\n",
        "print(f\"  input_ids type: {type(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  input_ids length: {len(train_tokenized[0]['input_ids'])}\")\n",
        "print(f\"  Available keys: {list(train_tokenized[0].keys())}\")\n",
        "\n",
        "# Note: labels will be created by DataCollatorForLanguageModeling during batching\n",
        "# This is expected - labels don't exist in the tokenized dataset yet\n",
        "if 'labels' in train_tokenized[0]:\n",
        "    print(f\"  labels type: {type(train_tokenized[0]['labels'])}\")\n",
        "    print(f\"  labels length: {len(train_tokenized[0]['labels'])}\")\n",
        "else:\n",
        "    print(f\"  ‚úì Labels will be created by DataCollator during batching (this is correct)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8ZUzZ2zQ0u3"
      },
      "outputs": [],
      "source": [
        "# Data Collator\n",
        "# DataCollatorForLanguageModeling automatically handles padding\n",
        "# It will pad sequences to the same length and set padding tokens in labels to -100\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # Causal LM, kh√¥ng ph·∫£i masked LM\n",
        "    pad_to_multiple_of=8  # T·ªëi ∆∞u cho GPU\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AErd_oy0Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 5: C·∫•u H√¨nh Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d5npuKgQ0u3"
      },
      "outputs": [],
      "source": [
        "output_dir = \"./results/qwen2.5-7b-rephrase-lora\"\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=4,  # ƒêi·ªÅu ch·ªânh d·ª±a tr√™n GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    fp16=True,  # Mixed precision training\n",
        "    gradient_checkpointing=False,  # Disabled: kh√¥ng t∆∞∆°ng th√≠ch v·ªõi 4-bit quantization\n",
        "    optim=\"adamw_torch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"tensorboard\",\n",
        "    logging_dir=f\"{output_dir}/logs\",\n",
        "    remove_unused_columns=False,\n",
        "    save_total_limit=3,  # Ch·ªâ gi·ªØ 3 checkpoints g·∫ßn nh·∫•t\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbc8a523"
      },
      "outputs": [],
      "source": [
        "# Explicitly upgrade transformers to ensure the latest version is used\n",
        "!pip install -U transformers\n",
        "\n",
        "import transformers\n",
        "print(f\"Updated transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt0Gj0S1Q0u3"
      },
      "outputs": [],
      "source": [
        "# Verify model is ready for training\n",
        "print(\"Verifying model setup...\")\n",
        "model.train()  # Ensure training mode\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "if trainable_params == 0:\n",
        "    raise RuntimeError(\n",
        "        \"No trainable parameters found! \"\n",
        "        \"Please check that LoRA adapter was applied correctly. \"\n",
        "        \"Run the LoRA configuration cell again.\"\n",
        "    )\n",
        "\n",
        "# Kh·ªüi t·∫°o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=val_tokenized,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized\")\n",
        "print(f\"Training samples: {len(train_tokenized)}\")\n",
        "print(f\"Validation samples: {len(val_tokenized)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_wwGL95Q0u3"
      },
      "source": [
        "## B∆∞·ªõc 6: Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU8pFK_AQ0u3"
      },
      "outputs": [],
      "source": [
        "# B·∫Øt ƒë·∫ßu training\n",
        "print(\"Starting training...\")\n",
        "train_result = trainer.train()\n",
        "\n",
        "# L∆∞u model cu·ªëi c√πng\n",
        "trainer.save_model()\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"\\nTraining completed! Model saved to {output_dir}\")\n",
        "print(f\"Training loss: {train_result.training_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCl559rzQ0u3"
      },
      "source": [
        "## B∆∞·ªõc 7: Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihSCWpv6Q0u3"
      },
      "outputs": [],
      "source": [
        "# Evaluate tr√™n test set\n",
        "print(\"Tokenizing test dataset...\")\n",
        "test_tokenized = test_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=test_dataset.column_names\n",
        ")\n",
        "\n",
        "eval_results = trainer.evaluate(eval_dataset=test_tokenized)\n",
        "print(\"\\nTest Results:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"{key}: {value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSWw8Pr4Q0u4"
      },
      "outputs": [],
      "source": [
        "# ƒê√°nh gi√° JSON Validity\n",
        "def evaluate_json_validity(model, tokenizer, test_data, max_samples=50):\n",
        "    \"\"\"ƒê√°nh gi√° t·ª∑ l·ªá JSON h·ª£p l·ªá\"\"\"\n",
        "    model.eval()\n",
        "    valid_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for i, item in enumerate(tqdm(test_data[:max_samples], desc=\"Evaluating\")):\n",
        "        prompt = item[\"prompt\"]\n",
        "\n",
        "        # Tokenize input\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract JSON (sau \"Output JSON:\")\n",
        "        if \"Output JSON:\" in generated_text:\n",
        "            json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "        else:\n",
        "            json_text = generated_text\n",
        "\n",
        "        # Ki·ªÉm tra JSON validity\n",
        "        try:\n",
        "            parsed = json.loads(json_text)\n",
        "            valid_count += 1\n",
        "        except:\n",
        "            if total_count < 3:  # Print first few errors\n",
        "                print(f\"\\nError parsing JSON {i}:\")\n",
        "                print(f\"Generated: {json_text[:200]}\")\n",
        "\n",
        "        total_count += 1\n",
        "\n",
        "    validity_rate = valid_count / total_count if total_count > 0 else 0\n",
        "    print(f\"\\nJSON Validity Rate: {validity_rate:.2%} ({valid_count}/{total_count})\")\n",
        "    return validity_rate\n",
        "\n",
        "# Ch·∫°y evaluation\n",
        "validity_rate = evaluate_json_validity(model, tokenizer, test_data, max_samples=50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ksGeSByQ0u4"
      },
      "source": [
        "## B∆∞·ªõc 8: Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNz8StLmQ0u4"
      },
      "outputs": [],
      "source": [
        "def generate_json_response(model, tokenizer, query: str, max_length=512):\n",
        "    \"\"\"Generate JSON response t·ª´ query\"\"\"\n",
        "    prompt = f\"Query: {query}\\n\\nOutput JSON:\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Decode\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract JSON\n",
        "    if \"Output JSON:\" in generated_text:\n",
        "        json_text = generated_text.split(\"Output JSON:\")[-1].strip()\n",
        "    else:\n",
        "        json_text = generated_text\n",
        "\n",
        "    # Parse JSON\n",
        "    try:\n",
        "        result = json.loads(json_text)\n",
        "        return result\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decode error: {e}\")\n",
        "        print(f\"Generated text: {json_text}\")\n",
        "        return None\n",
        "\n",
        "# Test inference\n",
        "test_queries = [\n",
        "    \"s·ªØa cho b√© 6 th√°ng\",\n",
        "    \"t√£ b·ªâm size M\",\n",
        "    \"ƒë·ªì ch∆°i cho tr·∫ª s∆° sinh\"\n",
        "]\n",
        "\n",
        "print(\"Testing inference:\\n\")\n",
        "for query in test_queries:\n",
        "    result = generate_json_response(model, tokenizer, query)\n",
        "    print(f\"Query: {query}\")\n",
        "    print(f\"Result: {json.dumps(result, ensure_ascii=False, indent=2)}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4513E8QYQ0u4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "499xUKxQQ0u4"
      },
      "outputs": [],
      "source": [
        "# L∆∞u adapter (ch·ªâ LoRA weights, nh·ªè)\n",
        "adapter_path = f\"{output_dir}/adapter\"\n",
        "model.save_pretrained(adapter_path)\n",
        "tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "print(\"\\nƒê·ªÉ load l·∫°i model sau n√†y:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load base model\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"{model_name}\",\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "# Load adapter\n",
        "model = PeftModel.from_pretrained(base_model, \"{adapter_path}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{adapter_path}\")\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhWJGA6_Q0u5"
      },
      "source": [
        "## L∆∞u √ù\n",
        "\n",
        "- ƒêi·ªÅu ch·ªânh `per_device_train_batch_size` v√† `gradient_accumulation_steps` d·ª±a tr√™n GPU memory\n",
        "- C√≥ th·ªÉ th·ª≠ `r=16` n·∫øu `r=8` kh√¥ng ƒë·ªß hi·ªáu su·∫•t\n",
        "- Monitor training loss v√† validation loss ƒë·ªÉ tr√°nh overfitting\n",
        "- S·ª≠ d·ª•ng TensorBoard ƒë·ªÉ theo d√µi: `tensorboard --logdir ./results/qwen2.5-7b-rephrase-lora/logs`\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
